[{"content":"Redis面试经典案例 缓存 缓存三兄弟（穿透，击穿，雪崩） ​\t1.缓存穿透：进行查询操作的时候，先查询redis，没有命中的话查询db，db如果查询到了数据，先写入redis进行缓存重构，然后返回数据，但是如果恶意攻击，大量请求不存在的数据，那么请求会一直打到数据库，数据库并发量小，会导致宕机，这就是缓存穿透 ​\t解决办法：1.使用布隆过滤器，布隆过滤器实际上是一个bitmap的数组，在启动redis时，先对布隆过滤器进行初始化，将key存储到布隆过滤器中，（原理是，会使用三个不同的hash算法，计算出三个不同的值，并将bitmap中的值改为1），查询请求到业务层的时候，会先进行判断，如果布隆过滤器中没有对应的key，那么直接返回null，但是布隆过滤器存在误判和内存占用率的问题，首先，如果计算出的hash值，正好与其他两个key计算出的值对应上了，那么也会判断为存在，进行查询，第二，想要误判率小，那么就要扩大bitmap这个数组的长度，增大了内存的占用，在使用布隆过滤器时，可以对数组长度和误判率进行初始化调整 ​\t2.将空值也缓存到redis中，即便数据库查询为空，也将对应的key和null存储到redis中，让下次请求不打到数据库，直接命中redis，但是会造成脏数据占用多的问题， ​\t个人觉得，小项目中使用方法二，大项目使用方法一\n​\t2.缓存击穿：在进行查询操作的时候，如果热点数据突然过期，第一个请求会进行缓存重建，但是在缓存重建的过程中，如果有大量请求并发，会有大量并发请求到数据库，此时数据库承受不住高并发，会导致宕机，这就是缓存击穿。\n​\t解决办法：1.使用互斥锁，在第一个请求到达业务层时，先进行判断，如果此时redis未命中，那么先获取锁，获取到锁之后查询数据库，并将数据库中的数据在redis中进行缓存重建，然后释放锁，返回数据。别的请求到达业务层后，redis未命中，同时获取锁失败，就会进行自旋，重新查询redis，直到获取到redis中的数据。优点是有强一致性，缺点是性能差。\n​\t2.使用逻辑过期，也就是在redis中构建数据时，加上一个expire的过期字段，设置上过期时间，在第一个请求redis中命中数据之后，先检查一下逻辑过期时间，如果过期，那么获取锁，同时，新开一个线程，并返回过期数据，在新线程中，完成缓存重建。其他请求命中redis后，尝试获取锁，获取锁失败，直接返回当前旧数据。优点是性能好，高可用，但是不能保证数据强一致性\n​\t个人觉得，使用逻辑过期更为合理，因为这样用户体验较好\n​\t3.缓存雪崩：是指在同一时段，redis中有大量的key过期或者redis宕机，那么大量请求同时打到数据库，会给数据库带来巨大压力，可能导致数据库直接宕机，这就是缓存雪崩\n​\t解决办法：1.给不同的key增加随机的TTL随机值\n​\t2.如果是redis宕机，那么解决方案就是搭建redis集群提高服务的可用性（哨兵模式，集群模式）\n​\t3.给缓存业务增加降级限流策略（nginx或者是spring cloud gateway）降级限流可以作为缓存所有问题的保底策略\n​\t4.给业务添加多级缓存（Guava或Caffeine）但是这两个我都没了解过，多级缓存的话，我知道jvm有一个缓存机制，还有数据库也有自己的缓存机制\n双写一致性 ​\tredis作为缓存，mysql中的数据如何与redis中数据进行同步，这就是双写一致性\n​\t首先，可以采用延迟双删的策略，就是先删除缓存，然后更改数据库，然后延时再删除一次缓存，这样后面查询的时候，拿到的就是数据库中最新的数据，但是延时多久拿，这个不太好确定\n​\t还有一个问题就是，先删除缓存还是先删除数据库，都会有脏数据的出现，所以其实先后不重要\n​\t1.允许延时一致的业务\n​\t可以使用MQ等中间件，更新数据之后，通知缓存删除\n​\t可以使用canal中间件，不需要修改业务代码，伪装为mysql的一个从节点，canal可以通过监听mysql的binlog文件，当所监听的表数据发生了修改操作时，会通知缓存进行删除\n​\t2.要求数据强一致性的业务\n​\t采用redisson提供的读写锁\n​\t共享锁：读锁readlock，加锁之后，别的线程也可以进行读操作，但是不能进行写操作\n​\t排他锁：writelock，也叫独占锁，加锁之后，别的线程无法进行读写操作，实际上底层也是 一个setnx，保证同一时间只有一个线程操作锁\n缓存的持久化 redis作为缓存，提供了两种数据持久化的方式，RDB和AOF\nRDB是一个二进制快照文件，将redis内存存储的数据写到磁盘上，当redis宕机需要新的实例时，从RDB快照文件中读取数据，但是这也会造成短时间的数据丢失，优点是文件小，数据恢复的速度快，但是占用的cpu和内存资源也较多\nAOF是一个追加文件，也就是在redis进行写操作的时候，会将redis的操作命令记录下来，存储到磁盘中，当redis宕机时，会从这个文件中再执行一次命令，优点是数据基本不会丢失，但是恢复的速度慢\n缓存数据过期策略 ​\t1.惰性删除： 在需要使用到key的时候，先判断下其是否过期，如果过期，那么就删除当前key，这样的好处是，不会浪费cpu的资源来查询没有过期的key，坏处是，如果过期的key过多但是没有使用到，会占用内存\n​\t2.定期删除：每隔一段时间，就对redis中的key进行检查，删除其中过期的key\n​\tSLOW模式：一个定时任务，执行的默认频率为10hz，每次不超过25ms，可以通过修改redis的配置文件的hz选项来进行调整\n​\tFAST模式：执行频率不固定，但是两次之间间隔不低于2ms，每次耗时不超过1ms\n​\t优点：可以通过限制删除操作的时长和频率来减少操作对cpu的影响，定期删除，也能减少过期key对内存的占用\n​\t缺点：难以确定删除操作的时长和频率\n​\tredis 的过期策略是，惰性删除＋定期删除配合使用\n缓存数据淘汰策略 ​\t当redis中的内存不够用时，往redis中放入新的key，这时redis会按照某一种规则将内存中的数据删除掉，这就是缓存数据淘汰策略\n​\tredis中支持8种不同的策略来选择要删除的key\n策略名称 淘汰范围 淘汰算法 适用场景 特点说明 noeviction 不淘汰 无淘汰 关键数据存储，数据安全优先 内存不足时拒绝写入，确保数据不丢失 allkeys-lru 所有键 LRU（最近最少使用） 通用缓存场景，全部数据可淘汰 近似LRU算法，采样淘汰最久未访问的键 volatile-lru 仅有过期时间的键 LRU（最近最少使用） 缓存与持久数据混合存储 只淘汰设置了过期时间的键中的LRU键 allkeys-random 所有键 随机选择 访问模式均匀的场景 从所有键中随机选择淘汰 volatile-random 仅有过期时间的键 随机选择 缓存数据随机访问模式 从有过期时间的键中随机选择淘汰 volatile-ttl 仅有过期时间的键 TTL（存活时间） 希望尽快释放过期键内存 淘汰剩余生存时间最短的键 allkeys-lfu 所有键 LFU（最不经常使用） 热点数据明显的场景 淘汰访问频率最低的键，保留热点数据 volatile-lfu 仅有过期时间的键 LFU（最不经常使用） 缓存数据中有明显热点 从有过期时间的键中淘汰访问频率最低的键 分布式锁 redisson实现分布式锁，其底层原理其实是setnx和lua脚本，可以保证原子性\n在redisson提供的分布式锁中，可以通过看门狗机制来有效延长锁的持有时间，一个线程获取锁成功后，watchdog会给持有锁的线程进行续期，默认是每十秒续一次，避免业务还没完成，但是锁已经释放的情况\nredisson的锁是可以实现重入的，因为redisson的锁在redis中使用的是hash结构，有一个大key，一个小key，大key可以自定义，但是小key就是当前线程的唯一id，如果是同一线程，那么就可以进行重入，一般需要使用到重入的情况，都是业务比较复杂\n红锁可以解决redis主从数据一致的问题，但是性能很差，在redis集群中，通常使用的是主从集群结构，主节点一般负责写数据，从节点一般负责读数据，当有一台redis宕机之后，从节点成为主节点，当有一个线程获取到锁之后，主节点宕机，主节点还没来得及将数据同步到从节点，此时从节点成为了主节点，又有一个线程来获取锁，那么就会发生一把锁，两个线程持有的情况，不满足互斥锁的特性，会导致有脏数据，如果是需要主从数据一致性强的业务，建议使用zookeeper，zookeeper能保证数据的一致性\n集群方案 单节点redis的并发能力是有限的，如果要提供redis的并发能力，那么就需要构建redis集群，实现读写分离，一般都是一主多从，主节点负责写数据，从节点负责写数据，这就是redis的主从同步\n​\t1.主从复制\n​\t全量同步：从节点执行replicaof命令，与主节点建立连接，从节点会向主节点发送一个replid和offset，replid是数据集id，id一致说明是同一个数据集，主节点先判断id是否与自己一致，如果不一致，那么说明是第一次同步，主节点返回自己的relid和当前的偏移量offset从节点，同时主节点执行bgsave，生成一个RDB文件，发送给从节点，在生成RDB的过程中，可能还会有命令执行，这时主节点会用一个repl_baklog的日志文件来记录RDB期间所有的命令，并发送给从节点，从节点将RDB文件加载后，再根据日志文件执行命令，offset就是日志文件中的偏移量，如果从节点的offset低于了主节点的offset，那么说明需要更新\n​\t增量同步：从节点请求同步数据，如果不是第一次请求，那么获取到从节点的offset值，然后将repl_baklog中获取到offset值后的数据，发送给从节点，从节点执行命令，进行数据同步\n​\t2.哨兵模式\n​\t哨兵的作用，来实现主从集群的自动故障恢复 （监控，自动故障恢复，通知）\n​\t哨兵模式的主要作用其实就是为了保证redis集群的高可用， 每一个sentinel都会去监听所有的实例，如果超过一般的哨兵都发现实例没有响应，那么哨兵就会认为这个实例已经宕机\n​\t集群脑裂问题：当某一个时间段，sentinel和主节点和从节点不在一个网络分区，此时哨兵没有监测到主节点master的心跳，那么哨兵就会将从节点升为主节点，但是此时客户端的服务还在之前的主节点写入数据，当网络恢复之后，之前的主节点会强制降为从节点，清除数据，然后跟新的主节点同步数据，此时，就会造成大量数据的丢失，因为在同一时间段，出现了两个master，就像大脑分裂了一样，这就是集群脑裂问题\n​\t解决办法：可以通过修改redis的配置，可以设置最少的从节点数量，也就是如果当前master没有从节点，那么就拒绝请求，以及缩短主从数据同步的延迟时间，也就是master与slave之间的数据同步时间要短，如果出现脑裂，那么是没有办法同步数据的，如果达不到要求，那么也拒绝请求\n​\t3.分片集群\n​\t解决海量存储问题，高并发写的问题\n​\t通过redis集群的分片，实现大量数据存储，集群中有多个master节点，每个master存储不同的数据，每个master还可以拥有自己的slave节点，可以形成主从集群的关系，因为有多个master，可以解决高并发写的问题，多个slave，也可以解决高并发读的问题，并且master之间还可以通过ping检测彼此健康状态，相当于自带了哨兵，客户端请求可以发送到任意节点，集群会自动路由，最终请求会转发到正确的节点\n​\t因为redis分片集群引入了hash槽的概念，redis集群有16384个哈希槽，每个key通过crc16校验后对16384取模，决定放置于哪个槽，集群的每个节点负责一部分哈希槽\nredis是单线程的，但是为什么还那么快 因为redis是纯内存操作，并且完成基于C语言完成，执行速度非常快，采用单线程，避免不必要的上下文切换可竞争条件，多线程还要考虑线程安全问题，使用了多路复用IO模型，非阻塞IO\n多路复用IO模型：使用单个线程同时监听多个socket，并在某个socket可读和可写时，得到通知，从而避免无效的等待，充分利用cpu资源，目前I/O多路复用普通采用的都是epoll模式，它会在通知用户进程socket准备就绪的同时，把已就绪的socket写入用户空间，不需要遍历socket来确定是否就绪，提升了性能\nredis的网络模型：redis的网络模型就是使用了多路复用IO和任务派发的机制来应对多个socket请求\n连接应答处理器\n命令恢复处理器\t在 redis 6.0之后，使用了多线程\n命令请求处理器\t在redis 6.0之后，将命令的转换使用了多线程，增加命令转换的速度，但是命令的执行还是单线程\nMySQL面试经典案例 mysql的优化\n定位慢查询 如聚合查询，多表查询，表数据量过大查询，深度分页查询（表象：查询时间过长，接口返回数据时间过慢）\n方案1：开源工具\n调试工具：Arthas\t运维工具：Prometheus，Skywalking\n方案2：mysql自带的慢日志（一般测试阶段使用，生产环境中会损失mysql的性能）\n分析慢sql MySQL EXPLAIN 工具字段详解表\n字段 说明 常见值/含义 优化建议 id 查询标识符 1. 数字：执行顺序（越大越先执行） 2. 相同id：从上到下执行 3. 不同id：id大的先执行 用于理解复杂查询的执行顺序 select_type 查询类型 1. SIMPLE：简单查询（无子查询/UNION） 2. PRIMARY：主查询 3. SUBQUERY：子查询 4. DERIVED：派生表（FROM子句中的子查询） 5. UNION：UNION中的第二个及以后查询 6. UNION RESULT：UNION结果 识别复杂查询结构，优化子查询和派生表 table 访问的表 表名或别名，表示派生表 确认查询涉及的具体表 partitions 匹配的分区 分区表使用的分区名称 分区裁剪优化 type 访问类型（关键指标） 性能从好到差： 1. system：系统表，仅一行 2. const：通过主键/唯一索引查找 3. eq_ref：关联查询，使用唯一索引 4. ref：使用非唯一索引查找 5. range：索引范围扫描 6. index：全索引扫描 7. ALL：全表扫描 尽量避免ALL和index，优化为range或ref possible_keys 可能使用的索引 查询可能使用的索引列表 检查是否有合适的索引未被使用 key 实际使用的索引 实际选择的索引，NULL表示未使用索引 对比possible_keys，确认索引选择是否合理 key_len 索引长度 使用的索引字节数，可判断索引使用情况 复合索引中查看是否充分利用索引 ref 索引引用 显示索引的哪一列被使用 检查关联查询的索引使用 rows 预估扫描行数 预估需要检查的行数 数值越大性能越差，考虑优化索引 filtered 过滤百分比 存储引擎返回数据在服务器层过滤的比例（0-100） 值越小表示过滤效果越好，但大量数据过滤可能需优化 Extra 额外信息（关键指标） 常见值： 1. Using index：覆盖索引 2. Using where：服务器层过滤 3. Using temporary：使用临时表 4. Using filesort：文件排序 5. Using join buffer：使用连接缓冲区 6. Impossible WHERE：WHERE条件不可能满足 关注Using filesort/temporary，这些通常需要优化 如果一条sql执行很慢的话，我们通常会使用mysql自动的执行计划explain来去查看这条sql的执行情况，比如在这里面可以通过key和key_len检查是否命中了索引，如果本身已经添加了索引，也可以判断索引是否有失效的情况。第二个，可以通过type字段查看sql是否有进一步的优化空间，是否存在全索引扫描或全盘扫描，第三个可以通过extra建议来判断，是否出现了回表的情况，如果出现了，可以尝试添加索引或修改返回字段来修复\n索引 什么是索引：索引 (index) 是帮助MysoL高效获取数据的数据结构 (有序) 。在数据之外，数据库系统还维护着满足特定查找算法的数据结构 (B+树) ，这些数据结构以某种方式引用 (指向) 数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。\n索引的底层数据结构是什么：\nMySQL的默认的存储引擎InnoDB采用的B+树的数据结构来存储索引，选择B+树的主要的原因是：第一阶数更多，路径更短，第二个磁盘读写代价B+树更低，非叶子节点只存储指针，叶子阶段存储数据，第三是B+树便于扫库和区间查询，叶子节点是一个双向链表\nB树和B+树的区别是什么呢?\n第一：在B树中，非叶子节点和叶子节点都会存放数据，而B+树的所有的数据都会出现在叶子节点，在查询的时候，B+树查找效率更加稳定\n第二：在进行范围查询的时候，B+树效率更高，因为B+树都在叶子节点存储，并且叶子节点是一个双向链表\n聚集索引和非聚集索引（二级索引）\n​\t什么是聚簇索引什么是非聚簇索引？\n聚簇索引（聚集索引）：数据与索引放到一块，B+树的叶子节点保存了整行数据，有且只有一个 非聚簇索引（二级索引）：数据与索引分开存储，B+树的叶子节点保存对应的主键，可以有多个 ​\t什么是回表查询？\n​\t通过二级索引找到对应的主键值，到聚集索引中查找整行数据，这个过程就是回表\n覆盖索引：覆盖索引是指查询使用了索引，返回的列，必须在索引中全部能够找到\n使用id查询，直接走聚集索引查询，一次索引扫描，直接返回数据，性能高。 如果返回的列中没有创建索引，有可能会触发回表查询，尽量避免使用select * mysql超大分页怎么处理：\n在数据量比较大时，limit分页查询，需要对数据进行排序，效率低，此时使用覆盖索引+子查询，先在子查询中查询id，因为id是覆盖索引，所以在索引中查询效率快，底层是B+树，所以其实范围查询效率是快的，然后将返回的id集合，再到原来的表中做关联查询，能提高很多效率\n索引的创建原则：\n针对于数据量较大，且查询比较频繁的表建立索引。 针对于常作为查询条件（where）、排序（order by）、分组（group by）操作的字段建立索引。 尽量选择区分度高的列作为索引，尽量建立唯一索引，区分度越高，使用索引的效率越高。 如果是字符串类型的字段，字段的长度较长，可以针对于字段的特点，建立前缀索引。 尽量使用联合索引，减少单列索引，查询时，联合索引很多时候可以覆盖索引，节省存储空间，避免回表，提高查询效率。 要控制索引的数量，索引并不是多多益善，索引越多，维护索引结构的代价也就越大，会影响增删改的效率。 如果索引列不能存储NULL值，请在创建表时使用NOT NULL约束它。当优化器知道每列是否包含NULL值时，它可以更好地确定哪个索引最有效地用于查询。 索引失效的场景：\n索引失效情况及其解释：\n索引失效情况 解释 违反最左前缀法则 在使用联合索引时，查询条件必须从索引的最左列开始，否则索引将无法生效。 范围查询右边的列，不能使用索引 当查询条件中使用了范围查询（如 \u0026gt;、\u0026lt;、BETWEEN）后，其右边的列将无法使用索引进行进一步筛选。 不要在索引列上进行运算操作，索引将失效 如果在索引列上进行函数运算、算术运算或表达式操作，数据库将无法直接使用该索引进行查询。 字符串不加单引号，造成索引失效（类型转换） 如果字符串类型的列在查询时未加引号，数据库可能会进行隐式类型转换，导致索引无法使用。 以 % 开头的 Like 模糊查询，索引失效 如果使用 LIKE '%xxx' 这种以通配符开头的模糊查询，索引将无法被有效利用，通常只支持 LIKE 'xxx%' 使用索引。 sql优化 优化方向 具体措施 1. 表的设计与数据类型选择 - 合理设计表结构，遵循范式与反范式平衡\n- 选择合适数据类型（如用 INT 代替 BIGINT）\n- 尽量使用 NOT NULL 约束\n- 合理使用分区表 2. 索引优化 - 在 WHERE、ORDER BY、GROUP BY 相关列创建索引\n- 优先使用联合索引\n- 为区分度高的列建索引\n- 控制索引数量，定期清理冗余索引 3. SQL 语句优化 - 避免 SELECT *，只取所需字段\n- 使用 EXPLAIN 分析执行计划\n- 避免 WHERE 中对字段进行函数运算\n- 用 INNER JOIN 替代子查询（合适时）\n- 优化分页查询，避免大数据量翻页 4. 主从复制与读写分离 - 配置主从复制，读操作分流至从库\n- 使用读写分离中间件或框架\n- 避免写操作阻塞读操作\n- 监控主从同步延迟 5. 分库分表 - 水平分表（按时间、ID 范围等）\n- 垂直分表（按业务模块拆分字段）\n- 分库（按业务模块分库）\n- 使用分库分表中间件（如 ShardingSphere）\n- 合理设计分片键，处理跨库查询 事务 什么是事务：事务是一组操作的集合，它是一个不可分割的工作单位，系统会将这些操作一起提交或者撤销，即同时成功或者同时失败\nACID:\n原子性（Atomicity）：事务是不可分割的最小操作单元，要么全部成功，要么全部失败。\n一致性（Consistency）：事务完成时，必须使所有的数据都保持一致状态。\n隔离性（Isolation）：数据库系统提供的隔离机制，保证事务在不受外部并发操作影响的独立环境下运行。\n持久性（Durability）：事务一旦提交或回滚，它对数据库中的数据的改变就是永久的。\n事务并发问题：\n问题 描述 脏读 一个事务读取到另一个事务尚未提交的数据。 不可重复读 一个事务先后读取同一条记录，但两次读取的数据不同。 幻读 一个事务按照条件查询数据时未找到对应行，但在插入数据时又发现该行已存在，仿佛出现“幻影”。 事务隔离级别:\n隔离级别 中文说明 可能存在的问题 READ UNCOMMITTED 未提交读 脏读、不可重复读、幻读 READ COMMITTED 读已提交 不可重复读、幻读 REPEATABLE READ 可重复读 幻读 SERIALIZABLE 串行化 无 实际上可重复读的隔离级别可以解决部分幻读问题\n在 MySQL InnoDB 引擎的“可重复读（REPEATABLE READ）”隔离级别下，幻读问题在实际应用中被有效解决了，但这属于数据库对标准隔离级别的增强实现，而非 SQL 标准的强制要求\n从 SQL 标准来看：可重复读级别本身允许幻读。 从 MySQL InnoDB 实现来看：它通过 MVCC（多版本并发控制） 和 间隙锁（Next-Key Lock） 两种机制的协同作用，分别解决了“快照读”和“当前读”场景下的幻读问题： MVCC：确保事务内普通的 SELECT 查询（快照读）基于一致性视图，看不到其他事务插入的数据。 间隙锁：对涉及范围查询的加锁操作（如 SELECT ... FOR UPDATE），会锁住记录之间的“间隙”，阻止其他事务插入新记录。 因此，虽然从严格意义上讲，标准并未强制要求可重复读解决幻读，但 InnoDB 的实际实现已经将幻读风险降至极低，可以认为在该级别下幻读被“有效防御”了。这也是 MySQL 选择 REPEATABLE READ 作为默认隔离级别的重要原因。\nredolog和undolog：\nredolog：在数据库存储层面，实际上存储分为两个层次，缓冲层和磁盘层，缓存层面是基于内存的，会将磁盘层中的数据页加载到缓冲层中便于数据的查询和修改，而当进行增删改查的操作时，首先会进入到缓冲层中查询数据页，然后进行修改，缓存层再将修改后的数据同步到磁盘层，在这中间，就有redolog bufffer来记录数据页的变化，然后同步到磁盘层，但是如果还未同步服务器就宕机了，那么就会造成数据的丢失，所以在事务提交之后，会将缓冲中的所有修改信息存到redolog file日志文件中，用于刷新脏页到磁盘时发生错误，进行数据恢复使用，这就是redolog\nundolog：回滚日志，用于记录数据被修改前的信息，作用包含两个：提供回滚 和 MVCC（多版本并发控制）。undo log和redo log记录物理日志不一样，它是逻辑日志。\n可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然。 当update一条记录时，它记录一条对应相反的update记录。当执行rollback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。 undo log和redo log的区别\nredo log：记录的是数据页的物理变化，服务宕机可用来同步数据 undo log：记录的是逻辑日志，当事务回滚时，通过逆操作恢复原来的数据 redo log保证了事务的持久性，undo log保证了事务的原子性和一致性 ","date":"2025-12-11T00:00:00Z","image":"https://nova-bryan.github.io/p/java%E9%9D%A2%E8%AF%95%E6%89%8B%E8%AE%B0/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/java%E9%9D%A2%E8%AF%95%E6%89%8B%E8%AE%B0/","title":"Java面试手记"},{"content":"在昨天的练习作业中，我们改造了余额支付功能，在支付成功后利用RabbitMQ通知交易服务，更新业务订单状态为已支付。 但是大家思考一下，如果这里MQ通知失败，支付服务中支付流水显示支付成功，而交易服务中的订单状态却显示未支付，数据出现了不一致。 此时前端发送请求查询支付状态时，肯定是查询交易服务状态，会发现业务订单未支付，而用户自己知道已经支付成功，这就导致用户体验不一致。\n因此，这里我们必须尽可能确保MQ消息的可靠性，即：消息应该至少被消费者处理1次 那么问题来了：\n我们该如何确保MQ消息的可靠性？ 如果真的发送失败，有没有其它的兜底方案？ 这些问题，在今天的学习中都会找到答案。\n1.发送者的可靠性 首先，我们一起分析一下消息丢失的可能性有哪些。 消息从发送者发送消息，到消费者处理消息，需要经过的流程是这样的： 消息从生产者到消费者的每一步都可能导致消息丢失：\n发送消息时丢失： 生产者发送消息时连接MQ失败 生产者发送消息到达MQ后未找到Exchange 生产者发送消息到达MQ的Exchange后，未找到合适的Queue 消息到达MQ后，处理消息的进程发生异常 MQ导致消息丢失： 消息到达MQ，保存到队列后，尚未消费就突然宕机 消费者处理消息时： 消息接收后尚未处理突然宕机 消息接收后处理过程中抛出异常 综上，我们要解决消息丢失问题，保证MQ的可靠性，就必须从3个方面入手：\n确保生产者一定把消息发送到MQ 确保MQ不会将消息弄丢 确保消费者一定要处理消息 这一章我们先来看如何确保生产者一定能把消息发送到MQ。\n1.1.生产者重试机制 首先第一种情况，就是生产者发送消息时，出现了网络故障，导致与MQ的连接中断。\n为了解决这个问题，SpringAMQP提供的消息发送时的重试机制。即：当RabbitTemplate与MQ连接超时后，多次重试。\n修改publisher模块的application.yaml文件，添加下面的内容：\n1 2 3 4 5 6 7 8 9 spring: rabbitmq: connection-timeout: 1s # 设置MQ的连接超时时间 template: retry: enabled: true # 开启超时重试机制 initial-interval: 1000ms # 失败后的初始等待时间 multiplier: 1 # 失败后下次的等待时长倍数，下次等待时长 = initial-interval * multiplier max-attempts: 3 # 最大重试次数 我们利用命令停掉RabbitMQ服务：\n1 docker stop mq 然后测试发送一条消息，会发现会每隔1秒重试1次，总共重试了3次。消息发送的超时重试机制配置成功了！\n:::warning 注意：当网络不稳定的时候，利用重试机制可以有效提高消息发送的成功率。不过SpringAMQP提供的重试机制是阻塞式的重试，也就是说多次重试等待的过程中，当前线程是被阻塞的。 如果对于业务性能有要求，建议禁用重试机制。如果一定要使用，请合理配置等待时长和重试次数，当然也可以考虑使用异步线程来执行发送消息的代码。 :::\n1.2.生产者确认机制 一般情况下，只要生产者与MQ之间的网路连接顺畅，基本不会出现发送消息丢失的情况，因此大多数情况下我们无需考虑这种问题。 不过，在少数情况下，也会出现消息发送到MQ之后丢失的现象，比如：\nMQ内部处理消息的进程发生了异常 生产者发送消息到达MQ后未找到Exchange 生产者发送消息到达MQ的Exchange后，未找到合适的Queue，因此无法路由 针对上述情况，RabbitMQ提供了生产者消息确认机制，包括Publisher Confirm和Publisher Return两种。在开启确认机制的情况下，当生产者发送消息给MQ后，MQ会根据消息处理的情况返回不同的回执。 具体如图所示： 总结如下：\n当消息投递到MQ，但是路由失败时，通过Publisher Return返回异常信息，同时返回ack的确认信息，代表投递成功 临时消息投递到了MQ，并且入队成功，返回ACK，告知投递成功 持久消息投递到了MQ，并且入队完成持久化，返回ACK ，告知投递成功 其它情况都会返回NACK，告知投递失败 其中ack和nack属于Publisher Confirm机制，ack是投递成功；nack是投递失败。而return则属于Publisher Return机制。 默认两种机制都是关闭状态，需要通过配置文件来开启。\n1.3.实现生产者确认 1.3.1.开启生产者确认 在publisher模块的application.yaml中添加配置：\n1 2 3 4 spring: rabbitmq: publisher-confirm-type: correlated # 开启publisher confirm机制，并设置confirm类型 publisher-returns: true # 开启publisher return机制 这里publisher-confirm-type有三种模式可选：\nnone：关闭confirm机制 simple：同步阻塞等待MQ的回执 correlated：MQ异步回调返回回执 一般我们推荐使用correlated，回调机制。\n1.3.2.定义ReturnCallback 每个RabbitTemplate只能配置一个ReturnCallback，因此我们可以在配置类中统一设置。我们在publisher模块定义一个配置类： 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package com.itheima.publisher.config; import lombok.AllArgsConstructor; import lombok.extern.slf4j.Slf4j; import org.springframework.amqp.core.ReturnedMessage; import org.springframework.amqp.rabbit.core.RabbitTemplate; import org.springframework.context.annotation.Configuration; import javax.annotation.PostConstruct; @Slf4j @AllArgsConstructor @Configuration public class MqConfig { private final RabbitTemplate rabbitTemplate; @PostConstruct public void init(){ rabbitTemplate.setReturnsCallback(new RabbitTemplate.ReturnsCallback() { @Override public void returnedMessage(ReturnedMessage returned) { log.error(\u0026#34;触发return callback,\u0026#34;); log.debug(\u0026#34;exchange: {}\u0026#34;, returned.getExchange()); log.debug(\u0026#34;routingKey: {}\u0026#34;, returned.getRoutingKey()); log.debug(\u0026#34;message: {}\u0026#34;, returned.getMessage()); log.debug(\u0026#34;replyCode: {}\u0026#34;, returned.getReplyCode()); log.debug(\u0026#34;replyText: {}\u0026#34;, returned.getReplyText()); } }); } } 1.3.3.定义ConfirmCallback 由于每个消息发送时的处理逻辑不一定相同，因此ConfirmCallback需要在每次发消息时定义。具体来说，是在调用RabbitTemplate中的convertAndSend方法时，多传递一个参数： 这里的CorrelationData中包含两个核心的东西：\nid：消息的唯一标示，MQ对不同的消息的回执以此做判断，避免混淆 SettableListenableFuture：回执结果的Future对象 将来MQ的回执就会通过这个Future来返回，我们可以提前给CorrelationData中的Future添加回调函数来处理消息回执： 我们新建一个测试，向系统自带的交换机发送消息，并且添加ConfirmCallback：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Test void testPublisherConfirm() { // 1.创建CorrelationData CorrelationData cd = new CorrelationData(); // 2.给Future添加ConfirmCallback cd.getFuture().addCallback(new ListenableFutureCallback\u0026lt;CorrelationData.Confirm\u0026gt;() { @Override public void onFailure(Throwable ex) { // 2.1.Future发生异常时的处理逻辑，基本不会触发 log.error(\u0026#34;send message fail\u0026#34;, ex); } @Override public void onSuccess(CorrelationData.Confirm result) { // 2.2.Future接收到回执的处理逻辑，参数中的result就是回执内容 if(result.isAck()){ // result.isAck()，boolean类型，true代表ack回执，false 代表 nack回执 log.debug(\u0026#34;发送消息成功，收到 ack!\u0026#34;); }else{ // result.getReason()，String类型，返回nack时的异常描述 log.error(\u0026#34;发送消息失败，收到 nack, reason : {}\u0026#34;, result.getReason()); } } }); // 3.发送消息 rabbitTemplate.convertAndSend(\u0026#34;hmall.direct\u0026#34;, \u0026#34;q\u0026#34;, \u0026#34;hello\u0026#34;, cd); } 执行结果如下： 可以看到，由于传递的RoutingKey是错误的，路由失败后，触发了return callback，同时也收到了ack。 当我们修改为正确的RoutingKey以后，就不会触发return callback了，只收到ack。 而如果连交换机都是错误的，则只会收到nack。\n:::warning 注意： 开启生产者确认比较消耗MQ性能，一般不建议开启。而且大家思考一下触发确认的几种情况：\n路由失败：一般是因为RoutingKey错误导致，往往是编程导致 交换机名称错误：同样是编程错误导致 MQ内部故障：这种需要处理，但概率往往较低。因此只有对消息可靠性要求非常高的业务才需要开启，而且仅仅需要开启ConfirmCallback处理nack就可以了。 ::: 2.MQ的可靠性 消息到达MQ以后，如果MQ不能及时保存，也会导致消息丢失，所以MQ的可靠性也非常重要。\n2.1.数据持久化 为了提升性能，默认情况下MQ的数据都是在内存存储的临时数据，重启后就会消失。为了保证数据的可靠性，必须配置数据持久化，包括：\n交换机持久化 队列持久化 消息持久化 我们以控制台界面为例来说明。\n2.1.1.交换机持久化 在控制台的Exchanges页面，添加交换机时可以配置交换机的Durability参数： 设置为Durable就是持久化模式，Transient就是临时模式。\n2.1.2.队列持久化 在控制台的Queues页面，添加队列时，同样可以配置队列的Durability参数： 除了持久化以外，你可以看到队列还有很多其它参数，有一些我们会在后期学习。\n2.1.3.消息持久化 在控制台发送消息的时候，可以添加很多参数，而消息的持久化是要配置一个properties： :::warning 说明：在开启持久化机制以后，如果同时还开启了生产者确认，那么MQ会在消息持久化以后才发送ACK回执，进一步确保消息的可靠性。 不过出于性能考虑，为了减少IO次数，发送到MQ的消息并不是逐条持久化到数据库的，而是每隔一段时间批量持久化。一般间隔在100毫秒左右，这就会导致ACK有一定的延迟，因此建议生产者确认全部采用异步方式。 :::\n2.2.LazyQueue 在默认情况下，RabbitMQ会将接收到的信息保存在内存中以降低消息收发的延迟。但在某些特殊情况下，这会导致消息积压，比如：\n消费者宕机或出现网络故障 消息发送量激增，超过了消费者处理速度 消费者处理业务发生阻塞 一旦出现消息堆积问题，RabbitMQ的内存占用就会越来越高，直到触发内存预警上限。此时RabbitMQ会将内存消息刷到磁盘上，这个行为成为PageOut. PageOut会耗费一段时间，并且会阻塞队列进程。因此在这个过程中RabbitMQ不会再处理新的消息，生产者的所有请求都会被阻塞。\n为了解决这个问题，从RabbitMQ的3.6.0版本开始，就增加了Lazy Queues的模式，也就是惰性队列。惰性队列的特征如下：\n接收到消息后直接存入磁盘而非内存 消费者要消费消息时才会从磁盘中读取并加载到内存（也就是懒加载） 支持数百万条的消息存储 而在3.12版本之后，LazyQueue已经成为所有队列的默认格式。因此官方推荐升级MQ为3.12版本或者所有队列都设置为LazyQueue模式。\n2.2.1.控制台配置Lazy模式 在添加队列的时候，添加x-queue-mod=lazy参数即可设置队列为Lazy模式： 2.2.2.代码配置Lazy模式 在利用SpringAMQP声明队列的时候，添加x-queue-mod=lazy参数也可设置队列为Lazy模式：\n1 2 3 4 5 6 7 @Bean public Queue lazyQueue(){ return QueueBuilder .durable(\u0026#34;lazy.queue\u0026#34;) .lazy() // 开启Lazy模式 .build(); } 这里是通过QueueBuilder的lazy()函数配置Lazy模式，底层源码如下： 当然，我们也可以基于注解来声明队列并设置为Lazy模式：\n1 2 3 4 5 6 7 8 @RabbitListener(queuesToDeclare = @Queue( name = \u0026#34;lazy.queue\u0026#34;, durable = \u0026#34;true\u0026#34;, arguments = @Argument(name = \u0026#34;x-queue-mode\u0026#34;, value = \u0026#34;lazy\u0026#34;) )) public void listenLazyQueue(String msg){ log.info(\u0026#34;接收到 lazy.queue的消息：{}\u0026#34;, msg); } 2.2.3.更新已有队列为lazy模式 对于已经存在的队列，也可以配置为lazy模式，但是要通过设置policy实现。 可以基于命令行设置policy：\n1 rabbitmqctl set_policy Lazy \u0026#34;^lazy-queue$\u0026#34; \u0026#39;{\u0026#34;queue-mode\u0026#34;:\u0026#34;lazy\u0026#34;}\u0026#39; --apply-to queues 命令解读：\nrabbitmqctl ：RabbitMQ的命令行工具 set_policy ：添加一个策略 Lazy ：策略名称，可以自定义 \u0026quot;^lazy-queue$\u0026quot; ：用正则表达式匹配队列的名字 '{\u0026quot;queue-mode\u0026quot;:\u0026quot;lazy\u0026quot;}' ：设置队列模式为lazy模式 --apply-to queues：策略的作用对象，是所有的队列 当然，也可以在控制台配置policy，进入在控制台的Admin页面，点击Policies，即可添加配置： 3.消费者的可靠性 当RabbitMQ向消费者投递消息以后，需要知道消费者的处理状态如何。因为消息投递给消费者并不代表就一定被正确消费了，可能出现的故障有很多，比如：\n消息投递的过程中出现了网络故障 消费者接收到消息后突然宕机 消费者接收到消息后，因处理不当导致异常 \u0026hellip; 一旦发生上述情况，消息也会丢失。因此，RabbitMQ必须知道消费者的处理状态，一旦消息处理失败才能重新投递消息。 但问题来了：RabbitMQ如何得知消费者的处理状态呢？\n本章我们就一起研究一下消费者处理消息时的可靠性解决方案。\n2.1.消费者确认机制 为了确认消费者是否成功处理消息，RabbitMQ提供了消费者确认机制（Consumer Acknowledgement）。即：当消费者处理消息结束后，应该向RabbitMQ发送一个回执，告知RabbitMQ自己消息处理状态。回执有三种可选值：\nack：成功处理消息，RabbitMQ从队列中删除该消息 nack：消息处理失败，RabbitMQ需要再次投递消息 reject：消息处理失败并拒绝该消息，RabbitMQ从队列中删除该消息 一般reject方式用的较少，除非是消息格式有问题，那就是开发问题了。因此大多数情况下我们需要将消息处理的代码通过try catch机制捕获，消息处理成功时返回ack，处理失败时返回nack.\n由于消息回执的处理代码比较统一，因此SpringAMQP帮我们实现了消息确认。并允许我们通过配置文件设置ACK处理方式，有三种模式：\n**none**：不处理。即消息投递给消费者后立刻ack，消息会立刻从MQ删除。非常不安全，不建议使用 **manual**：手动模式。需要自己在业务代码中调用api，发送ack或reject，存在业务入侵，但更灵活 **auto**：自动模式。SpringAMQP利用AOP对我们的消息处理逻辑做了环绕增强，当业务正常执行时则自动返回ack. 当业务出现异常时，根据异常判断返回不同结果： 如果是业务异常，会自动返回nack； 如果是消息处理或校验异常，自动返回reject; 返回Reject的常见异常有：\nStarting with version 1.3.2, the default ErrorHandler is now a ConditionalRejectingErrorHandler that rejects (and does not requeue) messages that fail with an irrecoverable error. Specifically, it rejects messages that fail with the following errors:\no.s.amqp…MessageConversionException: Can be thrown when converting the incoming message payload using a MessageConverter. o.s.messaging…MessageConversionException: Can be thrown by the conversion service if additional conversion is required when mapping to a @RabbitListener method. o.s.messaging…MethodArgumentNotValidException: Can be thrown if validation (for example, @Valid) is used in the listener and the validation fails. o.s.messaging…MethodArgumentTypeMismatchException: Can be thrown if the inbound message was converted to a type that is not correct for the target method. For example, the parameter is declared as Message but Message is received. java.lang.NoSuchMethodException: Added in version 1.6.3. java.lang.ClassCastException: Added in version 1.6.3. 通过下面的配置可以修改SpringAMQP的ACK处理方式：\n1 2 3 4 5 spring: rabbitmq: listener: simple: acknowledge-mode: none # 不做处理 修改consumer服务的SpringRabbitListener类中的方法，模拟一个消息处理的异常：\n1 2 3 4 5 6 7 8 @RabbitListener(queues = \u0026#34;simple.queue\u0026#34;) public void listenSimpleQueueMessage(String msg) throws InterruptedException { log.info(\u0026#34;spring 消费者接收到消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); if (true) { throw new MessageConversionException(\u0026#34;故意的\u0026#34;); } log.info(\u0026#34;消息处理完成\u0026#34;); } 测试可以发现：当消息处理发生异常时，消息依然被RabbitMQ删除了。\n我们再次把确认机制修改为auto：\n1 2 3 4 5 spring: rabbitmq: listener: simple: acknowledge-mode: auto # 自动ack 在异常位置打断点，再次发送消息，程序卡在断点时，可以发现此时消息状态为unacked（未确定状态）： 放行以后，由于抛出的是消息转换异常，因此Spring会自动返回reject，所以消息依然会被删除： 我们将异常改为RuntimeException类型：\n1 2 3 4 5 6 7 8 @RabbitListener(queues = \u0026#34;simple.queue\u0026#34;) public void listenSimpleQueueMessage(String msg) throws InterruptedException { log.info(\u0026#34;spring 消费者接收到消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); if (true) { throw new RuntimeException(\u0026#34;故意的\u0026#34;); } log.info(\u0026#34;消息处理完成\u0026#34;); } 在异常位置打断点，然后再次发送消息测试，程序卡在断点时，可以发现此时消息状态为unacked（未确定状态）： 放行以后，由于抛出的是业务异常，所以Spring返回ack，最终消息恢复至Ready状态，并且没有被RabbitMQ删除： 当我们把配置改为auto时，消息处理失败后，会回到RabbitMQ，并重新投递到消费者。\n2.2.失败重试机制 当消费者出现异常后，消息会不断requeue（重入队）到队列，再重新发送给消费者。如果消费者再次执行依然出错，消息会再次requeue到队列，再次投递，直到消息处理成功为止。 极端情况就是消费者一直无法执行成功，那么消息requeue就会无限循环，导致mq的消息处理飙升，带来不必要的压力： 当然，上述极端情况发生的概率还是非常低的，不过不怕一万就怕万一。为了应对上述情况Spring又提供了消费者失败重试机制：在消费者出现异常时利用本地重试，而不是无限制的requeue到mq队列。\n修改consumer服务的application.yml文件，添加内容：\n1 2 3 4 5 6 7 8 9 10 spring: rabbitmq: listener: simple: retry: enabled: true # 开启消费者失败重试 initial-interval: 1000ms # 初识的失败等待时长为1秒 multiplier: 1 # 失败的等待时长倍数，下次等待时长 = multiplier * last-interval max-attempts: 3 # 最大重试次数 stateless: true # true无状态；false有状态。如果业务中包含事务，这里改为false 重启consumer服务，重复之前的测试。可以发现：\n消费者在失败后消息没有重新回到MQ无限重新投递，而是在本地重试了3次 本地重试3次以后，抛出了AmqpRejectAndDontRequeueException异常。查看RabbitMQ控制台，发现消息被删除了，说明最后SpringAMQP返回的是reject 结论：\n开启本地重试时，消息处理过程中抛出异常，不会requeue到队列，而是在消费者本地重试 重试达到最大次数后，Spring会返回reject，消息会被丢弃 2.3.失败处理策略 在之前的测试中，本地测试达到最大重试次数后，消息会被丢弃。这在某些对于消息可靠性要求较高的业务场景下，显然不太合适了。 因此Spring允许我们自定义重试次数耗尽后的消息处理策略，这个策略是由MessageRecovery接口来定义的，它有3个不同实现：\nRejectAndDontRequeueRecoverer：重试耗尽后，直接reject，丢弃消息。默认就是这种方式 ImmediateRequeueMessageRecoverer：重试耗尽后，返回nack，消息重新入队 RepublishMessageRecoverer：重试耗尽后，将失败消息投递到指定的交换机 比较优雅的一种处理方案是RepublishMessageRecoverer，失败后将消息投递到一个指定的，专门存放异常消息的队列，后续由人工集中处理。\n1）在consumer服务中定义处理失败消息的交换机和队列\n1 2 3 4 5 6 7 8 9 10 11 12 @Bean public DirectExchange errorMessageExchange(){ return new DirectExchange(\u0026#34;error.direct\u0026#34;); } @Bean public Queue errorQueue(){ return new Queue(\u0026#34;error.queue\u0026#34;, true); } @Bean public Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){ return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with(\u0026#34;error\u0026#34;); } 2）定义一个RepublishMessageRecoverer，关联队列和交换机\n1 2 3 4 @Bean public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){ return new RepublishMessageRecoverer(rabbitTemplate, \u0026#34;error.direct\u0026#34;, \u0026#34;error\u0026#34;); } 完整代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package com.itheima.consumer.config; import org.springframework.amqp.core.Binding; import org.springframework.amqp.core.BindingBuilder; import org.springframework.amqp.core.DirectExchange; import org.springframework.amqp.core.Queue; import org.springframework.amqp.rabbit.core.RabbitTemplate; import org.springframework.amqp.rabbit.retry.MessageRecoverer; import org.springframework.amqp.rabbit.retry.RepublishMessageRecoverer; import org.springframework.context.annotation.Bean; @Configuration @ConditionalOnProperty(name = \u0026#34;spring.rabbitmq.listener.simple.retry.enabled\u0026#34;, havingValue = \u0026#34;true\u0026#34;) public class ErrorMessageConfig { @Bean public DirectExchange errorMessageExchange(){ return new DirectExchange(\u0026#34;error.direct\u0026#34;); } @Bean public Queue errorQueue(){ return new Queue(\u0026#34;error.queue\u0026#34;, true); } @Bean public Binding errorBinding(Queue errorQueue, DirectExchange errorMessageExchange){ return BindingBuilder.bind(errorQueue).to(errorMessageExchange).with(\u0026#34;error\u0026#34;); } @Bean public MessageRecoverer republishMessageRecoverer(RabbitTemplate rabbitTemplate){ return new RepublishMessageRecoverer(rabbitTemplate, \u0026#34;error.direct\u0026#34;, \u0026#34;error\u0026#34;); } } 2.4.业务幂等性 何为幂等性？ 幂等是一个数学概念，用函数表达式来描述是这样的：f(x) = f(f(x))，例如求绝对值函数。 在程序开发中，则是指同一个业务，执行一次或多次对业务状态的影响是一致的。例如：\n根据id删除数据 查询数据 新增数据 但数据的更新往往不是幂等的，如果重复执行可能造成不一样的后果。比如：\n取消订单，恢复库存的业务。如果多次恢复就会出现库存重复增加的情况 退款业务。重复退款对商家而言会有经济损失。 所以，我们要尽可能避免业务被重复执行。 然而在实际业务场景中，由于意外经常会出现业务被重复执行的情况，例如：\n页面卡顿时频繁刷新导致表单重复提交 服务间调用的重试 MQ消息的重复投递 我们在用户支付成功后会发送MQ消息到交易服务，修改订单状态为已支付，就可能出现消息重复投递的情况。如果消费者不做判断，很有可能导致消息被消费多次，出现业务故障。 举例：\n假如用户刚刚支付完成，并且投递消息到交易服务，交易服务更改订单为已支付状态。 由于某种原因，例如网络故障导致生产者没有得到确认，隔了一段时间后重新投递给交易服务。 但是，在新投递的消息被消费之前，用户选择了退款，将订单状态改为了已退款状态。 退款完成后，新投递的消息才被消费，那么订单状态会被再次改为已支付。业务异常。 因此，我们必须想办法保证消息处理的幂等性。这里给出两种方案：\n唯一消息ID 业务状态判断 2.4.1.唯一消息ID 这个思路非常简单：\n每一条消息都生成一个唯一的id，与消息一起投递给消费者。 消费者接收到消息后处理自己的业务，业务处理成功后将消息ID保存到数据库 如果下次又收到相同消息，去数据库查询判断是否存在，存在则为重复消息放弃处理。 我们该如何给消息添加唯一ID呢？ 其实很简单，SpringAMQP的MessageConverter自带了MessageID的功能，我们只要开启这个功能即可。 以Jackson的消息转换器为例：\n1 2 3 4 5 6 7 8 @Bean public MessageConverter messageConverter(){ // 1.定义消息转换器 Jackson2JsonMessageConverter jjmc = new Jackson2JsonMessageConverter(); // 2.配置自动创建消息id，用于识别不同消息，也可以在业务中基于ID判断是否是重复消息 jjmc.setCreateMessageIds(true); return jjmc; } 2.4.2.业务判断 业务判断就是基于业务本身的逻辑或状态来判断是否是重复的请求或消息，不同的业务场景判断的思路也不一样。 例如我们当前案例中，处理消息的业务逻辑是把订单状态从未支付修改为已支付。因此我们就可以在执行业务时判断订单状态是否是未支付，如果不是则证明订单已经被处理过，无需重复处理。\n相比较而言，消息ID的方案需要改造原有的数据库，所以我更推荐使用业务判断的方案。\n以支付修改订单的业务为例，我们需要修改OrderServiceImpl中的markOrderPaySuccess方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Override public void markOrderPaySuccess(Long orderId) { // 1.查询订单 Order old = getById(orderId); // 2.判断订单状态 if (old == null || old.getStatus() != 1) { // 订单不存在或者订单状态不是1，放弃处理 return; } // 3.尝试更新订单 Order order = new Order(); order.setId(orderId); order.setStatus(2); order.setPayTime(LocalDateTime.now()); updateById(order); } 上述代码逻辑上符合了幂等判断的需求，但是由于判断和更新是两步动作，因此在极小概率下可能存在线程安全问题。\n我们可以合并上述操作为这样：\n1 2 3 4 5 6 7 8 9 10 @Override public void markOrderPaySuccess(Long orderId) { // UPDATE `order` SET status = ? , pay_time = ? WHERE id = ? AND status = 1 lambdaUpdate() .set(Order::getStatus, 2) .set(Order::getPayTime, LocalDateTime.now()) .eq(Order::getId, orderId) .eq(Order::getStatus, 1) .update(); } 注意看，上述代码等同于这样的SQL语句：\n1 UPDATE `order` SET status = ? , pay_time = ? WHERE id = ? AND status = 1 我们在where条件中除了判断id以外，还加上了status必须为1的条件。如果条件不符（说明订单已支付），则SQL匹配不到数据，根本不会执行。\n2.5.兜底方案 虽然我们利用各种机制尽可能增加了消息的可靠性，但也不好说能保证消息100%的可靠。万一真的MQ通知失败该怎么办呢？ 有没有其它兜底方案，能够确保订单的支付状态一致呢？\n其实思想很简单：既然MQ通知不一定发送到交易服务，那么交易服务就必须自己主动去查询支付状态。这样即便支付服务的MQ通知失败，我们依然能通过主动查询来保证订单状态的一致。 流程如下： 图中黄色线圈起来的部分就是MQ通知失败后的兜底处理方案，由交易服务自己主动去查询支付状态。\n不过需要注意的是，交易服务并不知道用户会在什么时候支付，如果查询的时机不正确（比如查询的时候用户正在支付中），可能查询到的支付状态也不正确。 那么问题来了，我们到底该在什么时间主动查询支付状态呢？\n这个时间是无法确定的，因此，通常我们采取的措施就是利用定时任务定期查询，例如每隔20秒就查询一次，并判断支付状态。如果发现订单已经支付，则立刻更新订单状态为已支付即可。 定时任务大家之前学习过，具体的实现这里就不再赘述了。\n至此，消息可靠性的问题已经解决了。\n综上，支付服务与交易服务之间的订单状态一致性是如何保证的？\n首先，支付服务会正在用户支付成功以后利用MQ消息通知交易服务，完成订单状态同步。 其次，为了保证MQ消息的可靠性，我们采用了生产者确认机制、消费者确认、消费者失败重试等策略，确保消息投递的可靠性 最后，我们还在交易服务设置了定时任务，定期查询订单支付状态。这样即便MQ通知失败，还可以利用定时任务作为兜底方案，确保订单支付状态的最终一致性。 4.延迟消息 在电商的支付业务中，对于一些库存有限的商品，为了更好的用户体验，通常都会在用户下单时立刻扣减商品库存。例如电影院购票、高铁购票，下单后就会锁定座位资源，其他人无法重复购买。\n但是这样就存在一个问题，假如用户下单后一直不付款，就会一直占有库存资源，导致其他客户无法正常交易，最终导致商户利益受损！\n因此，电商中通常的做法就是：对于超过一定时间未支付的订单，应该立刻取消订单并释放占用的库存。\n例如，订单支付超时时间为30分钟，则我们应该在用户下单后的第30分钟检查订单支付状态，如果发现未支付，应该立刻取消订单，释放库存。\n但问题来了：如何才能准确的实现在下单后第30分钟去检查支付状态呢？\n像这种在一段时间以后才执行的任务，我们称之为延迟任务，而要实现延迟任务，最简单的方案就是利用MQ的延迟消息了。\n在RabbitMQ中实现延迟消息也有两种方案：\n死信交换机+TTL 延迟消息插件 这一章我们就一起研究下这两种方案的实现方式，以及优缺点。\n4.1.死信交换机和延迟消息 首先我们来学习一下基于死信交换机的延迟消息方案。\n4.1.1.死信交换机 什么是死信？\n当一个队列中的消息满足下列情况之一时，可以成为死信（dead letter）：\n消费者使用basic.reject或 basic.nack声明消费失败，并且消息的requeue参数设置为false 消息是一个过期消息，超时无人消费 要投递的队列消息满了，无法投递 如果一个队列中的消息已经成为死信，并且这个队列通过**dead-letter-exchange**属性指定了一个交换机，那么队列中的死信就会投递到这个交换机中，而这个交换机就称为死信交换机（Dead Letter Exchange）。而此时加入有队列与死信交换机绑定，则最终死信就会被投递到这个队列中。\n死信交换机有什么作用呢？\n收集那些因处理失败而被拒绝的消息 收集那些因队列满了而被拒绝的消息 收集因TTL（有效期）到期的消息 4.1.2.延迟消息 前面两种作用场景可以看做是把死信交换机当做一种消息处理的最终兜底方案，与消费者重试时讲的RepublishMessageRecoverer作用类似。\n而最后一种场景，大家设想一下这样的场景： 如图，有一组绑定的交换机（ttl.fanout）和队列（ttl.queue）。但是ttl.queue没有消费者监听，而是设定了死信交换机hmall.direct，而队列direct.queue1则与死信交换机绑定，RoutingKey是blue： 假如我们现在发送一条消息到ttl.fanout，RoutingKey为blue，并设置消息的有效期为5000毫秒： :::warning 注意：尽管这里的ttl.fanout不需要RoutingKey，但是当消息变为死信并投递到死信交换机时，会沿用之前的RoutingKey，这样hmall.direct才能正确路由消息。 :::\n消息肯定会被投递到ttl.queue之后，由于没有消费者，因此消息无人消费。5秒之后，消息的有效期到期，成为死信： 死信被再次投递到死信交换机hmall.direct，并沿用之前的RoutingKey，也就是blue： 由于direct.queue1与hmall.direct绑定的key是blue，因此最终消息被成功路由到direct.queue1，如果此时有消费者与direct.queue1绑定， 也就能成功消费消息了。但此时已经是5秒钟以后了： 也就是说，publisher发送了一条消息，但最终consumer在5秒后才收到消息。我们成功实现了延迟消息。\n4.1.3.总结 :::warning 注意： RabbitMQ的消息过期是基于追溯方式来实现的，也就是说当一个消息的TTL到期以后不一定会被移除或投递到死信交换机，而是在消息恰好处于队首时才会被处理。 当队列中消息堆积很多的时候，过期消息可能不会被按时处理，因此你设置的TTL时间不一定准确。 :::\n4.2.DelayExchange插件 基于死信队列虽然可以实现延迟消息，但是太麻烦了。因此RabbitMQ社区提供了一个延迟消息插件来实现相同的效果。 官方文档说明： Scheduling Messages with RabbitMQ | RabbitMQ - Blog\n4.2.1.下载 插件下载地址： GitHub - rabbitmq/rabbitmq-delayed-message-exchange: Delayed Messaging for RabbitMQ 由于我们安装的MQ是3.8版本，因此这里下载3.8.17版本： 当然，也可以直接使用课前资料提供好的插件： 4.2.2.安装 因为我们是基于Docker安装，所以需要先查看RabbitMQ的插件目录对应的数据卷。\n1 docker volume inspect mq-plugins 结果如下：\n1 2 3 4 5 6 7 8 9 10 11 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2024-06-19T09:22:59+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/mq-plugins/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;mq-plugins\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] 插件目录被挂载到了/var/lib/docker/volumes/mq-plugins/_data这个目录，我们上传插件到该目录下。\n接下来执行命令，安装插件：\n1 docker exec -it mq rabbitmq-plugins enable rabbitmq_delayed_message_exchange 运行结果如下： 4.2.3.声明延迟交换机 基于注解方式：\n1 2 3 4 5 6 7 8 @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;delay.queue\u0026#34;, durable = \u0026#34;true\u0026#34;), exchange = @Exchange(name = \u0026#34;delay.direct\u0026#34;, delayed = \u0026#34;true\u0026#34;), key = \u0026#34;delay\u0026#34; )) public void listenDelayMessage(String msg){ log.info(\u0026#34;接收到delay.queue的延迟消息：{}\u0026#34;, msg); } 基于@Bean的方式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package com.itheima.consumer.config; import lombok.extern.slf4j.Slf4j; import org.springframework.amqp.core.*; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Slf4j @Configuration public class DelayExchangeConfig { @Bean public DirectExchange delayExchange(){ return ExchangeBuilder .directExchange(\u0026#34;delay.direct\u0026#34;) // 指定交换机类型和名称 .delayed() // 设置delay的属性为true .durable(true) // 持久化 .build(); } @Bean public Queue delayedQueue(){ return new Queue(\u0026#34;delay.queue\u0026#34;); } @Bean public Binding delayQueueBinding(){ return BindingBuilder.bind(delayedQueue()).to(delayExchange()).with(\u0026#34;delay\u0026#34;); } } 4.2.4.发送延迟消息 发送消息时，必须通过x-delay属性设定延迟时间：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Test void testPublisherDelayMessage() { // 1.创建消息 String message = \u0026#34;hello, delayed message\u0026#34;; // 2.发送消息，利用消息后置处理器添加消息头 rabbitTemplate.convertAndSend(\u0026#34;delay.direct\u0026#34;, \u0026#34;delay\u0026#34;, message, new MessagePostProcessor() { @Override public Message postProcessMessage(Message message) throws AmqpException { // 添加延迟消息属性 message.getMessageProperties().setDelay(5000); return message; } }); } :::warning 注意： 延迟消息插件内部会维护一个本地数据库表，同时使用Elang Timers功能实现计时。如果消息的延迟时间设置较长，可能会导致堆积的延迟消息非常多，会带来较大的CPU开销，同时延迟消息的时间会存在误差。 因此，不建议设置延迟时间过长的延迟消息。 :::\n4.5.订单状态同步问题 接下来，我们就在交易服务中利用延迟消息实现订单支付状态的同步。其大概思路如下： 假如订单超时支付时间为30分钟，理论上说我们应该在下单时发送一条延迟消息，延迟时间为30分钟。这样就可以在接收到消息时检验订单支付状态，关闭未支付订单。 但是大多数情况下用户支付都会在1分钟内完成，我们发送的消息却要在MQ中停留30分钟，额外消耗了MQ的资源。因此，我们最好多检测几次订单支付状态，而不是在最后第30分钟才检测。 例如：我们在用户下单后的第10秒、20秒、30秒、45秒、60秒、1分30秒、2分、\u0026hellip;30分分别设置延迟消息，如果提前发现订单已经支付，则后续的检测取消即可。 这样就可以有效避免对MQ资源的浪费了。\n优化后的实现思路如下： 由于我们要多次发送延迟消息，因此需要先定义一个记录消息延迟时间的消息体，处于通用性考虑，我们将其定义到hm-common模块下： 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package com.hmall.common.domain; import com.hmall.common.utils.CollUtils; import lombok.Data; import java.util.List; @Data public class MultiDelayMessage\u0026lt;T\u0026gt; { /** * 消息体 */ private T data; /** * 记录延迟时间的集合 */ private List\u0026lt;Long\u0026gt; delayMillis; public MultiDelayMessage(T data, List\u0026lt;Long\u0026gt; delayMillis) { this.data = data; this.delayMillis = delayMillis; } public static \u0026lt;T\u0026gt; MultiDelayMessage\u0026lt;T\u0026gt; of(T data, Long ... delayMillis){ return new MultiDelayMessage\u0026lt;\u0026gt;(data, CollUtils.newArrayList(delayMillis)); } /** * 获取并移除下一个延迟时间 * @return 队列中的第一个延迟时间 */ public Long removeNextDelay(){ return delayMillis.remove(0); } /** * 是否还有下一个延迟时间 */ public boolean hasNextDelay(){ return !delayMillis.isEmpty(); } } 4.5.1.定义常量 无论是消息发送还是接收都是在交易服务完成，因此我们在trade-service中定义一个常量类，用于记录交换机、队列、RoutingKey等常量： 内容如下：\n1 2 3 4 5 6 7 package com.hmall.trade.constants; public interface MqConstants { String DELAY_EXCHANGE = \u0026#34;trade.delay.topic\u0026#34;; String DELAY_ORDER_QUEUE = \u0026#34;trade.order.delay.queue\u0026#34;; String DELAY_ORDER_ROUTING_KEY = \u0026#34;order.query\u0026#34;; } 4.5.2.抽取共享mq配置 我们将mq的配置抽取到nacos中，方便各个微服务共享配置。 在nacos中定义一个名为shared-mq.xml的配置文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 spring: rabbitmq: host: ${hm.mq.host:192.168.150.101} # 主机名 port: ${hm.mq.port:5672} # 端口 virtual-host: ${hm.mq.vhost:/hmall} # 虚拟主机 username: ${hm.mq.un:hmall} # 用户名 password: ${hm.mq.pw:123} # 密码 listener: simple: prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息 这里只添加一些基础配置，至于生产者确认，消费者确认配置则由微服务根据业务自己决定。\n在trade-service模块添加共享配置： 4.5.3.改造下单业务 接下来，我们改造下单业务，在下单完成后，发送延迟消息，查询支付状态。\n1）引入依赖 在trade-service模块的pom.xml中引入amqp的依赖：\n1 2 3 4 5 \u0026lt;!--amqp--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2）改造下单业务 修改trade-service模块的com.hmall.trade.service.impl.OrderServiceImpl类的createOrder方法，添加消息发送的代码： 4.5.4.编写查询支付状态接口 由于MQ消息处理时需要查询支付状态，因此我们要在pay-service模块定义一个这样的接口，并提供对应的FeignClient. 首先，在hm-api模块定义三个类： 说明：\nPayOrderDTO：支付单的数据传输实体 PayClient：支付系统的Feign客户端 PayClientFallback：支付系统的fallback逻辑 PayOrderDTO代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package com.hmall.api.dto; import io.swagger.annotations.ApiModel; import io.swagger.annotations.ApiModelProperty; import lombok.Data; import java.time.LocalDateTime; /** * \u0026lt;p\u0026gt; * 支付订单 * \u0026lt;/p\u0026gt; */ @Data @ApiModel(description = \u0026#34;支付单数据传输实体\u0026#34;) public class PayOrderDTO { @ApiModelProperty(\u0026#34;id\u0026#34;) private Long id; @ApiModelProperty(\u0026#34;业务订单号\u0026#34;) private Long bizOrderNo; @ApiModelProperty(\u0026#34;支付单号\u0026#34;) private Long payOrderNo; @ApiModelProperty(\u0026#34;支付用户id\u0026#34;) private Long bizUserId; @ApiModelProperty(\u0026#34;支付渠道编码\u0026#34;) private String payChannelCode; @ApiModelProperty(\u0026#34;支付金额，单位分\u0026#34;) private Integer amount; @ApiModelProperty(\u0026#34;付类型，1：h5,2:小程序，3：公众号，4：扫码，5：余额支付\u0026#34;) private Integer payType; @ApiModelProperty(\u0026#34;付状态，0：待提交，1:待支付，2：支付超时或取消，3：支付成功\u0026#34;) private Integer status; @ApiModelProperty(\u0026#34;拓展字段，用于传递不同渠道单独处理的字段\u0026#34;) private String expandJson; @ApiModelProperty(\u0026#34;第三方返回业务码\u0026#34;) private String resultCode; @ApiModelProperty(\u0026#34;第三方返回提示信息\u0026#34;) private String resultMsg; @ApiModelProperty(\u0026#34;支付成功时间\u0026#34;) private LocalDateTime paySuccessTime; @ApiModelProperty(\u0026#34;支付超时时间\u0026#34;) private LocalDateTime payOverTime; @ApiModelProperty(\u0026#34;支付二维码链接\u0026#34;) private String qrCodeUrl; @ApiModelProperty(\u0026#34;创建时间\u0026#34;) private LocalDateTime createTime; @ApiModelProperty(\u0026#34;更新时间\u0026#34;) private LocalDateTime updateTime; } PayClient代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package com.hmall.api.client; import com.hmall.api.client.fallback.PayClientFallback; import com.hmall.api.dto.PayOrderDTO; import org.springframework.cloud.openfeign.FeignClient; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable; @FeignClient(value = \u0026#34;pay-service\u0026#34;, fallbackFactory = PayClientFallback.class) public interface PayClient { /** * 根据交易订单id查询支付单 * @param id 业务订单id * @return 支付单信息 */ @GetMapping(\u0026#34;/pay-orders/biz/{id}\u0026#34;) PayOrderDTO queryPayOrderByBizOrderNo(@PathVariable(\u0026#34;id\u0026#34;) Long id); } PayClientFallback代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.hmall.api.client.fallback; import com.hmall.api.client.PayClient; import com.hmall.api.dto.PayOrderDTO; import lombok.extern.slf4j.Slf4j; import org.springframework.cloud.openfeign.FallbackFactory; @Slf4j public class PayClientFallback implements FallbackFactory\u0026lt;PayClient\u0026gt; { @Override public PayClient create(Throwable cause) { return new PayClient() { @Override public PayOrderDTO queryPayOrderByBizOrderNo(Long id) { return null; } }; } } 最后，在pay-service模块的PayController中实现该接口：\n1 2 3 4 5 6 @ApiOperation(\u0026#34;根据id查询支付单\u0026#34;) @GetMapping(\u0026#34;/biz/{id}\u0026#34;) public PayOrderDTO queryPayOrderByBizOrderNo(@PathVariable(\u0026#34;id\u0026#34;) Long id){ PayOrder payOrder = payOrderService.lambdaQuery().eq(PayOrder::getBizOrderNo, id).one(); return BeanUtils.copyBean(payOrder, PayOrderDTO.class); } 4.5.5.消息监听 接下来，我们在trader-service编写一个监听器，监听延迟消息，查询订单支付状态： 代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 package com.hmall.trade.listener; import com.hmall.api.client.PayClient; import com.hmall.api.dto.PayOrderDTO; import com.hmall.common.domain.MultiDelayMessage; import com.hmall.trade.constants.MqConstants; import com.hmall.trade.domain.po.Order; import com.hmall.trade.service.IOrderService; import lombok.RequiredArgsConstructor; import lombok.extern.slf4j.Slf4j; import org.springframework.amqp.core.ExchangeTypes; import org.springframework.amqp.rabbit.annotation.Exchange; import org.springframework.amqp.rabbit.annotation.Queue; import org.springframework.amqp.rabbit.annotation.QueueBinding; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.amqp.rabbit.core.RabbitTemplate; import org.springframework.stereotype.Component; @Slf4j @Component @RequiredArgsConstructor public class OrderStatusListener { private final IOrderService orderService; private final PayClient payClient; private final RabbitTemplate rabbitTemplate; @RabbitListener(bindings = @QueueBinding( value = @Queue(name = MqConstants.DELAY_ORDER_QUEUE, durable = \u0026#34;true\u0026#34;), exchange = @Exchange(name = MqConstants.DELAY_EXCHANGE, type = ExchangeTypes.TOPIC), key = MqConstants.DELAY_ORDER_ROUTING_KEY )) public void listenOrderCheckDelayMessage(MultiDelayMessage\u0026lt;Long\u0026gt; msg) { // 1.获取消息中的订单id Long orderId = msg.getData(); // 2.查询订单，判断状态：1是未支付，大于1则是已支付或已关闭 Order order = orderService.getById(orderId); if (order == null || order.getStatus() \u0026gt; 1) { // 订单不存在或交易已经结束，放弃处理 return; } // 3.可能是未支付，查询支付服务 PayOrderDTO payOrder = payClient.queryPayOrderByBizOrderNo(orderId); if (payOrder != null \u0026amp;\u0026amp; payOrder.getStatus() == 3) { // 支付成功，更新订单状态 orderService.markOrderPaySuccess(orderId); return; } // 4.确定未支付，判断是否还有剩余延迟时间 if (msg.hasNextDelay()) { // 4.1.有延迟时间，需要重发延迟消息，先获取延迟时间的int值 int delayVal = msg.removeNextDelay().intValue(); // 4.2.发送延迟消息 rabbitTemplate.convertAndSend(MqConstants.DELAY_EXCHANGE, MqConstants.DELAY_ORDER_ROUTING_KEY, msg, message -\u0026gt; { message.getMessageProperties().setDelay(delayVal); return message; }); return; } // 5.没有剩余延迟时间了，说明订单超时未支付，需要取消订单 orderService.cancelOrder(orderId); } } 注意，这里要在OrderServiceImpl中实现cancelOrder方法，留作作业大家自行实现。\n5.作业 5.1.取消订单 在处理超时未支付订单时，如果发现订单确实超时未支付，最终需要关闭该订单。 关闭订单需要完成两件事情：\n将订单状态修改为已关闭 恢复订单中已经扣除的库存 这部分功能尚未实现。 大家要在IOrderService接口中定义cancelOrder方法：\n1 void cancelOrder(Long orderId); 并且在OrderServiceImpl中实现该方法。实现过程中要注意业务幂等性判断。\n5.2.抽取MQ工具 MQ在企业开发中的常见应用我们就学习完毕了，除了收发消息以外，消息可靠性的处理、生产者确认、消费者确认、延迟消息等等编码还是相对比较复杂的。 因此，我们需要将这些常用的操作封装为工具，方便在项目中使用。要求如下：\n在hm-commom模块下编写发送消息的工具类RabbitMqHelper 定义一个自动配置类MqConsumeErrorAutoConfiguration，内容包括： 声明一个交换机，名为error.direct，类型为direct 声明一个队列，名为：微服务名 + error.queue，也就是说要动态获取 将队列与交换机绑定，绑定时的RoutingKey就是微服务名 声明RepublishMessageRecoverer，消费失败消息投递到上述交换机 给配置类添加条件，当spring.rabbitmq.listener.simple.retry.enabled为true时触发 RabbitMqHelper的结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public class RabbitMqHelper { private final RabbitTemplate rabbitTemplate; public void sendMessage(String exchange, String routingKey, Object msg){ } public void sendDelayMessage(String exchange, String routingKey, Object msg, int delay){ } public void sendMessageWithConfirm(String exchange, String routingKey, Object msg, int maxRetries){ } } 5.3.改造业务 利用你编写的工具，改造支付服务、购物车服务、交易服务中消息发送功能，并且添加消息确认或消费者重试机制，确保消息的可靠性。\n","date":"2025-07-22T00:00:00Z","image":"https://nova-bryan.github.io/p/rabbitmq%E9%AB%98%E7%BA%A7/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/rabbitmq%E9%AB%98%E7%BA%A7/","title":"RabbitMQ高级"},{"content":"微服务一旦拆分，必然涉及到服务之间的相互调用，目前我们服务之间调用采用的都是基于OpenFeign的调用。这种调用中，调用者发起请求后需要等待服务提供者执行业务返回结果后，才能继续执行后面的业务。也就是说调用者在调用过程中处于阻塞状态，因此我们成这种调用方式为同步调用，也可以叫同步通讯。但在很多场景下，我们可能需要采用异步通讯的方式，为什么呢？\n我们先来看看什么是同步通讯和异步通讯。如图： 解读：\n同步通讯：就如同打视频电话，双方的交互都是实时的。因此同一时刻你只能跟一个人打视频电话。 异步通讯：就如同发微信聊天，双方的交互不是实时的，你不需要立刻给对方回应。因此你可以多线操作，同时跟多人聊天。 两种方式各有优劣，打电话可以立即得到响应，但是你却不能跟多个人同时通话。发微信可以同时与多个人收发微信，但是往往响应会有延迟。\n所以，如果我们的业务需要实时得到服务提供方的响应，则应该选择同步通讯（同步调用）。而如果我们追求更高的效率，并且不需要实时响应，则应该选择异步通讯（异步调用）。\n同步调用的方式我们已经学过了，之前的OpenFeign调用就是。但是：\n异步调用又该如何实现？ 哪些业务适合用异步调用来实现呢？ 通过今天的学习你就能明白这些问题了。\n1.初识MQ 1.1.同步调用 之前说过，我们现在基于OpenFeign的调用都属于是同步调用，那么这种方式存在哪些问题呢？ 举个例子，我们以昨天留给大家作为作业的余额支付功能为例来分析，首先看下整个流程： 目前我们采用的是基于OpenFeign的同步调用，也就是说业务执行流程是这样的\n支付服务需要先调用用户服务完成余额扣减 然后支付服务自己要更新支付流水单的状态 然后支付服务调用交易服务，更新业务订单状态为已支付 三个步骤依次执行。 这其中就存在3个问题： 第一，拓展性差 我们目前的业务相对简单，但是随着业务规模扩大，产品的功能也在不断完善。 在大多数电商业务中，用户支付成功后都会以短信或者其它方式通知用户，告知支付成功。假如后期产品经理提出这样新的需求，你怎么办？是不是要在上述业务中再加入通知用户的业务？ 某些电商项目中，还会有积分或金币的概念。假如产品经理提出需求，用户支付成功后，给用户以积分奖励或者返还金币，你怎么办？是不是要在上述业务中再加入积分业务、返还金币业务？ 。。。 最终你的支付业务会越来越臃肿： 也就是说每次有新的需求，现有支付逻辑都要跟着变化，代码经常变动，不符合开闭原则，拓展性不好。\n第二，性能下降 由于我们采用了同步调用，调用者需要等待服务提供者执行完返回结果后，才能继续向下执行，也就是说每次远程调用，调用者都是阻塞等待状态。最终整个业务的响应时长就是每次远程调用的执行时长之和： 假如每个微服务的执行时长都是50ms，则最终整个业务的耗时可能高达300ms，性能太差了。\n第三，级联失败 由于我们是基于OpenFeign调用交易服务、通知服务。当交易服务、通知服务出现故障时，整个事务都会回滚，交易失败。 这其实就是同步调用的级联失败问题。\n但是大家思考一下，我们假设用户余额充足，扣款已经成功，此时我们应该确保支付流水单更新为已支付，确保交易成功。毕竟收到手里的钱没道理再退回去吧。\n因此，这里不能因为短信通知、更新订单状态失败而回滚整个事务。\n综上，同步调用的方式存在下列问题：\n拓展性差 性能下降 级联失败 而要解决这些问题，我们就必须用异步调用的方式来代替同步调用。\n1.2.异步调用 异步调用方式其实就是基于消息通知的方式，一般包含三个角色：\n消息发送者：投递消息的人，就是原来的调用方 消息Broker：管理、暂存、转发消息，你可以把它理解成微信服务器 消息接收者：接收和处理消息的人，就是原来的服务提供方 在异步调用中，发送者不再直接同步调用接收者的业务接口，而是发送一条消息投递给消息Broker。然后接收者根据自己的需求从消息Broker那里订阅消息。每当发送方发送消息后，接受者都能获取消息并处理。 这样，发送消息的人和接收消息的人就完全解耦了。\n还是以余额支付业务为例： 除了扣减余额、更新支付流水单状态以外，其它调用逻辑全部取消。而是改为发送一条消息到Broker。而相关的微服务都可以订阅消息通知，一旦消息到达Broker，则会分发给每一个订阅了的微服务，处理各自的业务。\n假如产品经理提出了新的需求，比如要在支付成功后更新用户积分。支付代码完全不用变更，而仅仅是让积分服务也订阅消息即可： 不管后期增加了多少消息订阅者，作为支付服务来讲，执行问扣减余额、更新支付流水状态后，发送消息即可。业务耗时仅仅是这三部分业务耗时，仅仅100ms，大大提高了业务性能。\n另外，不管是交易服务、通知服务，还是积分服务，他们的业务与支付关联度低。现在采用了异步调用，解除了耦合，他们即便执行过程中出现了故障，也不会影响到支付服务。\n综上，异步调用的优势包括：\n耦合度更低 性能更好 业务拓展性强 故障隔离，避免级联失败 当然，异步通信也并非完美无缺，它存在下列缺点：\n完全依赖于Broker的可靠性、安全性和性能 架构复杂，后期维护和调试麻烦 1.3.技术选型 消息Broker，目前常见的实现方案就是消息队列（MessageQueue），简称为MQ. 目比较常见的MQ实现：\nActiveMQ RabbitMQ RocketMQ Kafka 几种常见MQ的对比：\nRabbitMQ ActiveMQ RocketMQ Kafka 公司/社区 Rabbit Apache 阿里 Apache 开发语言 Erlang Java Java Scala\u0026amp;Java 协议支持 AMQP，XMPP，SMTP，STOMP OpenWire,STOMP，REST,XMPP,AMQP 自定义协议 自定义协议 可用性 高 一般 高 高 单机吞吐量 一般 差 高 非常高 消息延迟 微秒级 毫秒级 毫秒级 毫秒以内 消息可靠性 高 一般 高 一般 追求可用性：Kafka、 RocketMQ 、RabbitMQ 追求可靠性：RabbitMQ、RocketMQ 追求吞吐能力：RocketMQ、Kafka 追求消息低延迟：RabbitMQ、Kafka\n据统计，目前国内消息队列使用最多的还是RabbitMQ，再加上其各方面都比较均衡，稳定性也好，因此我们课堂上选择RabbitMQ来学习。\n2.RabbitMQ RabbitMQ是基于Erlang语言开发的开源消息通信中间件，官网地址： Messaging that just works — RabbitMQ 接下来，我们就学习它的基本概念和基础用法。\n2.1.安装 我们同样基于Docker来安装RabbitMQ，使用下面的命令即可：\n1 2 3 4 5 6 7 8 9 10 11 docker run \\ -e RABBITMQ_DEFAULT_USER=itheima \\ -e RABBITMQ_DEFAULT_PASS=123321 \\ -v mq-plugins:/plugins \\ --name mq \\ --hostname mq \\ -p 15672:15672 \\ -p 5672:5672 \\ --network hmall \\ -d \\ rabbitmq:3.8-management 如果拉取镜像困难的话，可以使用课前资料给大家准备的镜像，利用docker load命令加载： 可以看到在安装命令中有两个映射的端口：\n15672：RabbitMQ提供的管理控制台的端口 5672：RabbitMQ的消息发送处理接口 安装完成后，我们访问 http://192.168.150.101:15672即可看到管理控制台。首次访问需要登录，默认的用户名和密码在配置文件中已经指定了。 登录后即可看到管理控制台总览页面： RabbitMQ对应的架构如图： 其中包含几个概念：\n**publisher**：生产者，也就是发送消息的一方 **consumer**：消费者，也就是消费消息的一方 **queue**：队列，存储消息。生产者投递的消息会暂存在消息队列中，等待消费者处理 **exchange**：交换机，负责消息路由。生产者发送的消息由交换机决定投递到哪个队列。 **virtual host**：虚拟主机，起到数据隔离的作用。每个虚拟主机相互独立，有各自的exchange、queue 上述这些东西都可以在RabbitMQ的管理控制台来管理，下一节我们就一起来学习控制台的使用。\n2.2.收发消息 2.2.1.交换机 我们打开Exchanges选项卡，可以看到已经存在很多交换机： 我们点击任意交换机，即可进入交换机详情页面。仍然会利用控制台中的publish message 发送一条消息： 这里是由控制台模拟了生产者发送的消息。由于没有消费者存在，最终消息丢失了，这样说明交换机没有存储消息的能力。\n2.2.2.队列 我们打开Queues选项卡，新建一个队列： 命名为hello.queue1： 再以相同的方式，创建一个队列，密码为hello.queue2，最终队列列表如下： 此时，我们再次向amq.fanout交换机发送一条消息。会发现消息依然没有到达队列！！ 怎么回事呢？ 发送到交换机的消息，只会路由到与其绑定的队列，因此仅仅创建队列是不够的，我们还需要将其与交换机绑定。\n2.2.3.绑定关系 点击Exchanges选项卡，点击amq.fanout交换机，进入交换机详情页，然后点击Bindings菜单，在表单中填写要绑定的队列名称： 相同的方式，将hello.queue2也绑定到改交换机。 最终，绑定结果如下： 2.2.4.发送消息 再次回到exchange页面，找到刚刚绑定的amq.fanout，点击进入详情页，再次发送一条消息： 回到Queues页面，可以发现hello.queue中已经有一条消息了： 点击队列名称，进入详情页，查看队列详情，这次我们点击get message： 可以看到消息到达队列了： 这个时候如果有消费者监听了MQ的hello.queue1或hello.queue2队列，自然就能接收到消息了。\n2.3.数据隔离 2.3.1.用户管理 点击Admin选项卡，首先会看到RabbitMQ控制台的用户管理界面： 这里的用户都是RabbitMQ的管理或运维人员。目前只有安装RabbitMQ时添加的itheima这个用户。仔细观察用户表格中的字段，如下：\nName：itheima，也就是用户名 Tags：administrator，说明itheima用户是超级管理员，拥有所有权限 Can access virtual host： /，可以访问的virtual host，这里的/是默认的virtual host 对于小型企业而言，出于成本考虑，我们通常只会搭建一套MQ集群，公司内的多个不同项目同时使用。这个时候为了避免互相干扰， 我们会利用virtual host的隔离特性，将不同项目隔离。一般会做两件事情：\n给每个项目创建独立的运维账号，将管理权限分离。 给每个项目创建不同的virtual host，将每个项目的数据隔离。 比如，我们给黑马商城创建一个新的用户，命名为hmall： 你会发现此时hmall用户没有任何virtual host的访问权限： 别急，接下来我们就来授权。\n2.3.2.virtual host 我们先退出登录： 切换到刚刚创建的hmall用户登录，然后点击Virtual Hosts菜单，进入virtual host管理页： 可以看到目前只有一个默认的virtual host，名字为 /。 我们可以给黑马商城项目创建一个单独的virtual host，而不是使用默认的/。 创建完成后如图： 由于我们是登录hmall账户后创建的virtual host，因此回到users菜单，你会发现当前用户已经具备了对/hmall这个virtual host的访问权限了： 此时，点击页面右上角的virtual host下拉菜单，切换virtual host为 /hmall： 然后再次查看queues选项卡，会发现之前的队列已经看不到了： 这就是基于virtual host 的隔离效果。\n3.SpringAMQP 将来我们开发业务功能的时候，肯定不会在控制台收发消息，而是应该基于编程的方式。由于RabbitMQ采用了AMQP协议，因此它具备跨语言的特性。任何语言只要遵循AMQP协议收发消息，都可以与RabbitMQ交互。并且RabbitMQ官方也提供了各种不同语言的客户端。 但是，RabbitMQ官方提供的Java客户端编码相对复杂，一般生产环境下我们更多会结合Spring来使用。而Spring的官方刚好基于RabbitMQ提供了这样一套消息收发的模板工具：SpringAMQP。并且还基于SpringBoot对其实现了自动装配，使用起来非常方便。\nSpringAmqp的官方地址： Spring AMQP SpringAMQP提供了三个功能：\n自动声明队列、交换机及其绑定关系 基于注解的监听器模式，异步接收消息 封装了RabbitTemplate工具，用于发送消息 这一章我们就一起学习一下，如何利用SpringAMQP实现对RabbitMQ的消息收发。\n3.1.导入Demo工程 在课前资料给大家提供了一个Demo工程，方便我们学习SpringAMQP的使用： 将其复制到你的工作空间，然后用Idea打开，项目结构如图： 包括三部分：\nmq-demo：父工程，管理项目依赖 publisher：消息的发送者 consumer：消息的消费者 在mq-demo这个父工程中，已经配置好了SpringAMQP相关的依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;cn.itcast.demo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mq-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;publisher\u0026lt;/module\u0026gt; \u0026lt;module\u0026gt;consumer\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.12\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--AMQP依赖，包含RabbitMQ--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--单元测试--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 因此，子工程中就可以直接使用SpringAMQP了。\n3.2.快速入门 在之前的案例中，我们都是经过交换机发送消息到队列，不过有时候为了测试方便，我们也可以直接向队列发送消息，跳过交换机。 在入门案例中，我们就演示这样的简单模型，如图： 也就是：\npublisher直接发送消息到队列 消费者监听并处理队列中的消息 :::warning 注意：这种模式一般测试使用，很少在生产中使用。 :::\n为了方便测试，我们现在控制台新建一个队列：simple.queue 添加成功： 接下来，我们就可以利用Java代码收发消息了。\n3.1.1.消息发送 首先配置MQ地址，在publisher服务的application.yml中添加配置：\n1 2 3 4 5 6 7 spring: rabbitmq: host: 192.168.150.101 # 你的虚拟机IP port: 5672 # 端口 virtual-host: /hmall # 虚拟主机 username: hmall # 用户名 password: 123 # 密码 然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package com.itheima.publisher.amqp; import org.junit.jupiter.api.Test; import org.springframework.amqp.rabbit.core.RabbitTemplate; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest public class SpringAmqpTest { @Autowired private RabbitTemplate rabbitTemplate; @Test public void testSimpleQueue() { // 队列名称 String queueName = \u0026#34;simple.queue\u0026#34;; // 消息 String message = \u0026#34;hello, spring amqp!\u0026#34;; // 发送消息 rabbitTemplate.convertAndSend(queueName, message); } } 打开控制台，可以看到消息已经发送到队列中： 接下来，我们再来实现消息接收。\n3.1.2.消息接收 首先配置MQ地址，在consumer服务的application.yml中添加配置：\n1 2 3 4 5 6 7 spring: rabbitmq: host: 192.168.150.101 # 你的虚拟机IP port: 5672 # 端口 virtual-host: /hmall # 虚拟主机 username: hmall # 用户名 password: 123 # 密码 然后在consumer服务的com.itheima.consumer.listener包中新建一个类SpringRabbitListener，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package com.itheima.consumer.listener; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.stereotype.Component; @Component public class SpringRabbitListener { // 利用RabbitListener来声明要监听的队列信息 // 将来一旦监听的队列中有了消息，就会推送给当前服务，调用当前方法，处理消息。 // 可以看到方法体中接收的就是消息体的内容 @RabbitListener(queues = \u0026#34;simple.queue\u0026#34;) public void listenSimpleQueueMessage(String msg) throws InterruptedException { System.out.println(\u0026#34;spring 消费者接收到消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } } 3.1.3.测试 启动consumer服务，然后在publisher服务中运行测试代码，发送MQ消息。最终consumer收到消息： 3.3.WorkQueues模型 Work queues，任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。 当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。 此时就可以使用work 模型，多个消费者共同处理消息处理，消息处理的速度就能大大提高了。\n接下来，我们就来模拟这样的场景。 首先，我们在控制台创建一个新的队列，命名为work.queue： 3.3.1.消息发送 这次我们循环发送，模拟大量消息堆积现象。 在publisher服务中的SpringAmqpTest类中添加一个测试方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * workQueue * 向队列中不停发送消息，模拟消息堆积。 */ @Test public void testWorkQueue() throws InterruptedException { // 队列名称 String queueName = \u0026#34;simple.queue\u0026#34;; // 消息 String message = \u0026#34;hello, message_\u0026#34;; for (int i = 0; i \u0026lt; 50; i++) { // 发送消息，每20毫秒发送一次，相当于每秒发送50条消息 rabbitTemplate.convertAndSend(queueName, message + i); Thread.sleep(20); } } 3.3.2.消息接收 要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法：\n1 2 3 4 5 6 7 8 9 10 11 @RabbitListener(queues = \u0026#34;work.queue\u0026#34;) public void listenWorkQueue1(String msg) throws InterruptedException { System.out.println(\u0026#34;消费者1接收到消息：【\u0026#34; + msg + \u0026#34;】\u0026#34; + LocalTime.now()); Thread.sleep(20); } @RabbitListener(queues = \u0026#34;work.queue\u0026#34;) public void listenWorkQueue2(String msg) throws InterruptedException { System.err.println(\u0026#34;消费者2........接收到消息：【\u0026#34; + msg + \u0026#34;】\u0026#34; + LocalTime.now()); Thread.sleep(200); } 注意到这两消费者，都设置了Thead.sleep，模拟任务耗时：\n消费者1 sleep了20毫秒，相当于每秒钟处理50个消息 消费者2 sleep了200毫秒，相当于每秒处理5个消息 3.3.3.测试 启动ConsumerApplication后，在执行publisher服务中刚刚编写的发送测试方法testWorkQueue。 最终结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 消费者1接收到消息：【hello, message_0】21:06:00.869555300 消费者2........接收到消息：【hello, message_1】21:06:00.884518 消费者1接收到消息：【hello, message_2】21:06:00.907454400 消费者1接收到消息：【hello, message_4】21:06:00.953332100 消费者1接收到消息：【hello, message_6】21:06:00.997867300 消费者1接收到消息：【hello, message_8】21:06:01.042178700 消费者2........接收到消息：【hello, message_3】21:06:01.086478800 消费者1接收到消息：【hello, message_10】21:06:01.087476600 消费者1接收到消息：【hello, message_12】21:06:01.132578300 消费者1接收到消息：【hello, message_14】21:06:01.175851200 消费者1接收到消息：【hello, message_16】21:06:01.218533400 消费者1接收到消息：【hello, message_18】21:06:01.261322900 消费者2........接收到消息：【hello, message_5】21:06:01.287003700 消费者1接收到消息：【hello, message_20】21:06:01.304412400 消费者1接收到消息：【hello, message_22】21:06:01.349950100 消费者1接收到消息：【hello, message_24】21:06:01.394533900 消费者1接收到消息：【hello, message_26】21:06:01.439876500 消费者1接收到消息：【hello, message_28】21:06:01.482937800 消费者2........接收到消息：【hello, message_7】21:06:01.488977100 消费者1接收到消息：【hello, message_30】21:06:01.526409300 消费者1接收到消息：【hello, message_32】21:06:01.572148 消费者1接收到消息：【hello, message_34】21:06:01.618264800 消费者1接收到消息：【hello, message_36】21:06:01.660780600 消费者2........接收到消息：【hello, message_9】21:06:01.689189300 消费者1接收到消息：【hello, message_38】21:06:01.705261 消费者1接收到消息：【hello, message_40】21:06:01.746927300 消费者1接收到消息：【hello, message_42】21:06:01.789835 消费者1接收到消息：【hello, message_44】21:06:01.834393100 消费者1接收到消息：【hello, message_46】21:06:01.875312100 消费者2........接收到消息：【hello, message_11】21:06:01.889969500 消费者1接收到消息：【hello, message_48】21:06:01.920702500 消费者2........接收到消息：【hello, message_13】21:06:02.090725900 消费者2........接收到消息：【hello, message_15】21:06:02.293060600 消费者2........接收到消息：【hello, message_17】21:06:02.493748 消费者2........接收到消息：【hello, message_19】21:06:02.696635100 消费者2........接收到消息：【hello, message_21】21:06:02.896809700 消费者2........接收到消息：【hello, message_23】21:06:03.099533400 消费者2........接收到消息：【hello, message_25】21:06:03.301446400 消费者2........接收到消息：【hello, message_27】21:06:03.504999100 消费者2........接收到消息：【hello, message_29】21:06:03.705702500 消费者2........接收到消息：【hello, message_31】21:06:03.906601200 消费者2........接收到消息：【hello, message_33】21:06:04.108118500 消费者2........接收到消息：【hello, message_35】21:06:04.308945400 消费者2........接收到消息：【hello, message_37】21:06:04.511547700 消费者2........接收到消息：【hello, message_39】21:06:04.714038400 消费者2........接收到消息：【hello, message_41】21:06:04.916192700 消费者2........接收到消息：【hello, message_43】21:06:05.116286400 消费者2........接收到消息：【hello, message_45】21:06:05.318055100 消费者2........接收到消息：【hello, message_47】21:06:05.520656400 消费者2........接收到消息：【hello, message_49】21:06:05.723106700 可以看到消费者1和消费者2竟然每人消费了25条消息：\n消费者1很快完成了自己的25条消息 消费者2却在缓慢的处理自己的25条消息。 也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。导致1个消费者空闲，另一个消费者忙的不可开交。没有充分利用每一个消费者的能力，最终消息处理的耗时远远超过了1秒。这样显然是有问题的。\n3.3.4.能者多劳 在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置：\n1 2 3 4 5 spring: rabbitmq: listener: simple: prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息 再次测试，发现结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 消费者1接收到消息：【hello, message_0】21:12:51.659664200 消费者2........接收到消息：【hello, message_1】21:12:51.680610 消费者1接收到消息：【hello, message_2】21:12:51.703625 消费者1接收到消息：【hello, message_3】21:12:51.724330100 消费者1接收到消息：【hello, message_4】21:12:51.746651100 消费者1接收到消息：【hello, message_5】21:12:51.768401400 消费者1接收到消息：【hello, message_6】21:12:51.790511400 消费者1接收到消息：【hello, message_7】21:12:51.812559800 消费者1接收到消息：【hello, message_8】21:12:51.834500600 消费者1接收到消息：【hello, message_9】21:12:51.857438800 消费者1接收到消息：【hello, message_10】21:12:51.880379600 消费者2........接收到消息：【hello, message_11】21:12:51.899327100 消费者1接收到消息：【hello, message_12】21:12:51.922828400 消费者1接收到消息：【hello, message_13】21:12:51.945617400 消费者1接收到消息：【hello, message_14】21:12:51.968942500 消费者1接收到消息：【hello, message_15】21:12:51.992215400 消费者1接收到消息：【hello, message_16】21:12:52.013325600 消费者1接收到消息：【hello, message_17】21:12:52.035687100 消费者1接收到消息：【hello, message_18】21:12:52.058188 消费者1接收到消息：【hello, message_19】21:12:52.081208400 消费者2........接收到消息：【hello, message_20】21:12:52.103406200 消费者1接收到消息：【hello, message_21】21:12:52.123827300 消费者1接收到消息：【hello, message_22】21:12:52.146165100 消费者1接收到消息：【hello, message_23】21:12:52.168828300 消费者1接收到消息：【hello, message_24】21:12:52.191769500 消费者1接收到消息：【hello, message_25】21:12:52.214839100 消费者1接收到消息：【hello, message_26】21:12:52.238998700 消费者1接收到消息：【hello, message_27】21:12:52.259772600 消费者1接收到消息：【hello, message_28】21:12:52.284131800 消费者2........接收到消息：【hello, message_29】21:12:52.306190600 消费者1接收到消息：【hello, message_30】21:12:52.325315800 消费者1接收到消息：【hello, message_31】21:12:52.347012500 消费者1接收到消息：【hello, message_32】21:12:52.368508600 消费者1接收到消息：【hello, message_33】21:12:52.391785100 消费者1接收到消息：【hello, message_34】21:12:52.416383800 消费者1接收到消息：【hello, message_35】21:12:52.439019 消费者1接收到消息：【hello, message_36】21:12:52.461733900 消费者1接收到消息：【hello, message_37】21:12:52.485990 消费者1接收到消息：【hello, message_38】21:12:52.509219900 消费者2........接收到消息：【hello, message_39】21:12:52.523683400 消费者1接收到消息：【hello, message_40】21:12:52.547412100 消费者1接收到消息：【hello, message_41】21:12:52.571191800 消费者1接收到消息：【hello, message_42】21:12:52.593024600 消费者1接收到消息：【hello, message_43】21:12:52.616731800 消费者1接收到消息：【hello, message_44】21:12:52.640317 消费者1接收到消息：【hello, message_45】21:12:52.663111100 消费者1接收到消息：【hello, message_46】21:12:52.686727 消费者1接收到消息：【hello, message_47】21:12:52.709266500 消费者2........接收到消息：【hello, message_48】21:12:52.725884900 消费者1接收到消息：【hello, message_49】21:12:52.746299900 可以发现，由于消费者1处理速度较快，所以处理了更多的消息；消费者2处理速度较慢，只处理了6条消息。而最终总的执行耗时也在1秒左右，大大提升。 正所谓能者多劳，这样充分利用了每一个消费者的处理能力，可以有效避免消息积压问题。\n3.3.5.总结 Work模型的使用：\n多个消费者绑定到一个队列，同一条消息只会被一个消费者处理 通过设置prefetch来控制消费者预取的消息数量 3.4.交换机类型 在之前的两个测试案例中，都没有交换机，生产者直接发送消息到队列。而一旦引入交换机，消息发送的模式会有很大变化： 可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化：\nPublisher：生产者，不再发送消息到队列中，而是发给交换机 Exchange：交换机，一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。 Queue：消息队列也与以前一样，接收消息、缓存消息。不过队列一定要与交换机绑定。 Consumer：消费者，与以前一样，订阅队列，没有变化 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！\n交换机的类型有四种：\nFanout：广播，将消息交给所有绑定到交换机的队列。我们最早在控制台使用的正是Fanout交换机 Direct：订阅，基于RoutingKey（路由key）发送给订阅了消息的队列 Topic：通配符订阅，与Direct类似，只不过RoutingKey可以使用通配符 Headers：头匹配，基于MQ的消息头匹配，用的较少。 课堂中，我们讲解前面的三种交换机模式。\n3.5.Fanout交换机 Fanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。 在广播模式下，消息发送流程是这样的： 1） 可以有多个队列 2） 每个队列都要绑定到Exchange（交换机） 3） 生产者发送的消息，只能发送到交换机 4） 交换机把消息发送给绑定过的所有队列 5） 订阅队列的消费者都能拿到消息 我们的计划是这样的： 创建一个名为 hmall.fanout的交换机，类型是Fanout 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机hmall.fanout 3.5.1.声明队列和交换机 在控制台创建队列fanout.queue1: 在创建一个队列fanout.queue2： 然后再创建一个交换机： 然后绑定两个队列到交换机： 3.5.2.消息发送 在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 @Test public void testFanoutExchange() { // 交换机名称 String exchangeName = \u0026#34;hmall.fanout\u0026#34;; // 消息 String message = \u0026#34;hello, everyone!\u0026#34;; rabbitTemplate.convertAndSend(exchangeName, \u0026#34;\u0026#34;, message); } 3.5.3.消息接收 在consumer服务的SpringRabbitListener中添加两个方法，作为消费者：\n1 2 3 4 5 6 7 8 9 @RabbitListener(queues = \u0026#34;fanout.queue1\u0026#34;) public void listenFanoutQueue1(String msg) { System.out.println(\u0026#34;消费者1接收到Fanout消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } @RabbitListener(queues = \u0026#34;fanout.queue2\u0026#34;) public void listenFanoutQueue2(String msg) { System.out.println(\u0026#34;消费者2接收到Fanout消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 3.5.4.总结 交换机的作用是什么？\n接收publisher发送的消息 将消息按照规则路由到与之绑定的队列 不能缓存消息，路由失败，消息丢失 FanoutExchange的会将消息路由到每个绑定的队列 3.6.Direct交换机 在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。 在Direct模型下：\n队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。 Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息 案例需求如图： 声明一个名为hmall.direct的交换机 声明队列direct.queue1，绑定hmall.direct，bindingKey为blud和red 声明队列direct.queue2，绑定hmall.direct，bindingKey为yellow和red 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2 在publisher中编写测试方法，向hmall.direct发送消息 3.6.1.声明队列和交换机 首先在控制台声明两个队列direct.queue1和direct.queue2，这里不再展示过程： 然后声明一个direct类型的交换机，命名为hmall.direct: 然后使用red和blue作为key，绑定direct.queue1到hmall.direct： 同理，使用red和yellow作为key，绑定direct.queue2到hmall.direct，步骤略，最终结果： 3.6.2.消息接收 在consumer服务的SpringRabbitListener中添加方法：\n1 2 3 4 5 6 7 8 9 @RabbitListener(queues = \u0026#34;direct.queue1\u0026#34;) public void listenDirectQueue1(String msg) { System.out.println(\u0026#34;消费者1接收到direct.queue1的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } @RabbitListener(queues = \u0026#34;direct.queue2\u0026#34;) public void listenDirectQueue2(String msg) { System.out.println(\u0026#34;消费者2接收到direct.queue2的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 3.6.3.消息发送 在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 9 @Test public void testSendDirectExchange() { // 交换机名称 String exchangeName = \u0026#34;hmall.direct\u0026#34;; // 消息 String message = \u0026#34;红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！\u0026#34;; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, \u0026#34;red\u0026#34;, message); } 由于使用的red这个key，所以两个消费者都收到了消息： 我们再切换为blue这个key：\n1 2 3 4 5 6 7 8 9 @Test public void testSendDirectExchange() { // 交换机名称 String exchangeName = \u0026#34;hmall.direct\u0026#34;; // 消息 String message = \u0026#34;最新报道，哥斯拉是居民自治巨型气球，虚惊一场！\u0026#34;; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, \u0026#34;blue\u0026#34;, message); } 你会发现，只有消费者1收到了消息： 3.6.4.总结 描述下Direct交换机与Fanout交换机的差异？\nFanout交换机将消息路由给每一个与之绑定的队列 Direct交换机根据RoutingKey判断路由给哪个队列 如果多个队列具有相同的RoutingKey，则与Fanout功能类似 3.7.Topic交换机 3.7.1.说明 Topic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。 只不过Topic类型Exchange可以让队列在绑定BindingKey 的时候使用通配符！\nBindingKey 一般都是有一个或多个单词组成，多个单词之间以.分割，例如： item.insert\n通配符规则：\n#：匹配一个或多个词 *：匹配不多不少恰好1个词 举例：\nitem.#：能够匹配item.spu.insert 或者 item.spu item.*：只能匹配item.spu 图示： 假如此时publisher发送的消息使用的RoutingKey共有四种：\nchina.news 代表有中国的新闻消息； china.weather 代表中国的天气消息； japan.news 则代表日本新闻 japan.weather 代表日本的天气消息； 解释：\ntopic.queue1：绑定的是china.# ，凡是以 china.开头的routing key 都会被匹配到，包括： china.news china.weather topic.queue2：绑定的是#.news ，凡是以 .news结尾的 routing key 都会被匹配。包括: china.news japan.news 接下来，我们就按照上图所示，来演示一下Topic交换机的用法。 首先，在控制台按照图示例子创建队列、交换机，并利用通配符绑定队列和交换机。此处步骤略。最终结果如下： 3.7.2.消息发送 在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 9 10 11 12 /** * topicExchange */ @Test public void testSendTopicExchange() { // 交换机名称 String exchangeName = \u0026#34;hmall.topic\u0026#34;; // 消息 String message = \u0026#34;喜报！孙悟空大战哥斯拉，胜!\u0026#34;; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, \u0026#34;china.news\u0026#34;, message); } 3.7.3.消息接收 在consumer服务的SpringRabbitListener中添加方法：\n1 2 3 4 5 6 7 8 9 @RabbitListener(queues = \u0026#34;topic.queue1\u0026#34;) public void listenTopicQueue1(String msg){ System.out.println(\u0026#34;消费者1接收到topic.queue1的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } @RabbitListener(queues = \u0026#34;topic.queue2\u0026#34;) public void listenTopicQueue2(String msg){ System.out.println(\u0026#34;消费者2接收到topic.queue2的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 3.7.4.总结 描述下Direct交换机与Topic交换机的差异？\nTopic交换机接收的消息RoutingKey必须是多个单词，以 **.** 分割 Topic交换机与队列绑定时的bindingKey可以指定通配符 #：代表0个或多个词 *：代表1个词 3.8.声明队列和交换机 在之前我们都是基于RabbitMQ控制台来创建队列、交换机。但是在实际开发时，队列和交换机是程序员定义的，将来项目上线，又要交给运维去创建。那么程序员就需要把程序中运行的所有队列和交换机都写下来，交给运维。在这个过程中是很容易出现错误的。 因此推荐的做法是由程序启动时检查队列和交换机是否存在，如果不存在自动创建。\n3.8.1.基本API SpringAMQP提供了一个Queue类，用来创建队列： SpringAMQP还提供了一个Exchange接口，来表示所有不同类型的交换机： 我们可以自己创建队列和交换机，不过SpringAMQP还提供了ExchangeBuilder来简化这个过程： 而在绑定队列和交换机时，则需要使用BindingBuilder来创建Binding对象： 3.8.2.fanout示例 在consumer中创建一个类，声明队列和交换机：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 package com.itheima.consumer.config; import org.springframework.amqp.core.Binding; import org.springframework.amqp.core.BindingBuilder; import org.springframework.amqp.core.FanoutExchange; import org.springframework.amqp.core.Queue; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class FanoutConfig { /** * 声明交换机 * @return Fanout类型交换机 */ @Bean public FanoutExchange fanoutExchange(){ return new FanoutExchange(\u0026#34;hmall.fanout\u0026#34;); } /** * 第1个队列 */ @Bean public Queue fanoutQueue1(){ return new Queue(\u0026#34;fanout.queue1\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange){ return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); } /** * 第2个队列 */ @Bean public Queue fanoutQueue2(){ return new Queue(\u0026#34;fanout.queue2\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange){ return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); } } 3.8.2.direct示例 direct模式由于要绑定多个KEY，会非常麻烦，每一个Key都要编写一个binding：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 package com.itheima.consumer.config; import org.springframework.amqp.core.*; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class DirectConfig { /** * 声明交换机 * @return Direct类型交换机 */ @Bean public DirectExchange directExchange(){ return ExchangeBuilder.directExchange(\u0026#34;hmall.direct\u0026#34;).build(); } /** * 第1个队列 */ @Bean public Queue directQueue1(){ return new Queue(\u0026#34;direct.queue1\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue1WithRed(Queue directQueue1, DirectExchange directExchange){ return BindingBuilder.bind(directQueue1).to(directExchange).with(\u0026#34;red\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue1WithBlue(Queue directQueue1, DirectExchange directExchange){ return BindingBuilder.bind(directQueue1).to(directExchange).with(\u0026#34;blue\u0026#34;); } /** * 第2个队列 */ @Bean public Queue directQueue2(){ return new Queue(\u0026#34;direct.queue2\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue2WithRed(Queue directQueue2, DirectExchange directExchange){ return BindingBuilder.bind(directQueue2).to(directExchange).with(\u0026#34;red\u0026#34;); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue2WithYellow(Queue directQueue2, DirectExchange directExchange){ return BindingBuilder.bind(directQueue2).to(directExchange).with(\u0026#34;yellow\u0026#34;); } } 3.8.4.基于注解声明 基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。\n例如，我们同样声明Direct模式的交换机和队列：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;direct.queue1\u0026#34;), exchange = @Exchange(name = \u0026#34;hmall.direct\u0026#34;, type = ExchangeTypes.DIRECT), key = {\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;} )) public void listenDirectQueue1(String msg){ System.out.println(\u0026#34;消费者1接收到direct.queue1的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;direct.queue2\u0026#34;), exchange = @Exchange(name = \u0026#34;hmall.direct\u0026#34;, type = ExchangeTypes.DIRECT), key = {\u0026#34;red\u0026#34;, \u0026#34;yellow\u0026#34;} )) public void listenDirectQueue2(String msg){ System.out.println(\u0026#34;消费者2接收到direct.queue2的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 是不是简单多了。 再试试Topic模式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;topic.queue1\u0026#34;), exchange = @Exchange(name = \u0026#34;hmall.topic\u0026#34;, type = ExchangeTypes.TOPIC), key = \u0026#34;china.#\u0026#34; )) public void listenTopicQueue1(String msg){ System.out.println(\u0026#34;消费者1接收到topic.queue1的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;topic.queue2\u0026#34;), exchange = @Exchange(name = \u0026#34;hmall.topic\u0026#34;, type = ExchangeTypes.TOPIC), key = \u0026#34;#.news\u0026#34; )) public void listenTopicQueue2(String msg){ System.out.println(\u0026#34;消费者2接收到topic.queue2的消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 3.9.消息转换器 Spring的消息发送代码接收的消息体是一个Object： 而在数据传输时，它会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。 只不过，默认情况下Spring采用的序列化方式是JDK序列化。众所周知，JDK序列化存在下列问题：\n数据体积过大 有安全漏洞 可读性差 我们来测试一下。\n3.9.1.测试默认转换器 1）创建测试队列 首先，我们在consumer服务中声明一个新的配置类： 利用@Bean的方式创建一个队列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package com.itheima.consumer.config; import org.springframework.amqp.core.Queue; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class MessageConfig { @Bean public Queue objectQueue() { return new Queue(\u0026#34;object.queue\u0026#34;); } } 注意，这里我们先不要给这个队列添加消费者，我们要查看消息体的格式。\n重启consumer服务以后，该队列就会被自动创建出来了： 2）发送消息 我们在publisher模块的SpringAmqpTest中新增一个消息发送的代码，发送一个Map对象：\n1 2 3 4 5 6 7 8 9 @Test public void testSendMap() throws InterruptedException { // 准备消息 Map\u0026lt;String,Object\u0026gt; msg = new HashMap\u0026lt;\u0026gt;(); msg.put(\u0026#34;name\u0026#34;, \u0026#34;柳岩\u0026#34;); msg.put(\u0026#34;age\u0026#34;, 21); // 发送消息 rabbitTemplate.convertAndSend(\u0026#34;object.queue\u0026#34;, msg); } 发送消息后查看控制台： 可以看到消息格式非常不友好。\n3.9.2.配置JSON转换器 显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。\n在publisher和consumer两个服务中都引入依赖：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.dataformat\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-dataformat-xml\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 注意，如果项目中引入了spring-boot-starter-web依赖，则无需再次引入Jackson依赖。\n配置消息转换器，在publisher和consumer两个服务的启动类中添加一个Bean即可：\n1 2 3 4 5 6 7 8 @Bean public MessageConverter messageConverter(){ // 1.定义消息转换器 Jackson2JsonMessageConverter jackson2JsonMessageConverter = new Jackson2JsonMessageConverter(); // 2.配置自动创建消息id，用于识别不同消息，也可以在业务中基于ID判断是否是重复消息 jackson2JsonMessageConverter.setCreateMessageIds(true); return jackson2JsonMessageConverter; } 消息转换器中添加的messageId可以便于我们将来做幂等性判断。\n此时，我们到MQ控制台删除object.queue中的旧的消息。然后再次执行刚才的消息发送的代码，到MQ的控制台查看消息结构： 3.9.3.消费者接收Object 我们在consumer服务中定义一个新的消费者，publisher是用Map发送，那么消费者也一定要用Map接收，格式如下：\n1 2 3 4 @RabbitListener(queues = \u0026#34;object.queue\u0026#34;) public void listenSimpleQueueMessage(Map\u0026lt;String, Object\u0026gt; msg) throws InterruptedException { System.out.println(\u0026#34;消费者接收到object.queue消息：【\u0026#34; + msg + \u0026#34;】\u0026#34;); } 4.业务改造 案例需求：改造余额支付功能，将支付成功后基于OpenFeign的交易服务的更新订单状态接口的同步调用，改为基于RabbitMQ的异步通知。 如图： 说明，我们只关注交易服务，步骤如下：\n定义topic类型交换机，命名为pay.topic 定义消息队列，命名为mark.order.pay.queue 将mark.order.pay.queue与pay.topic绑定，BindingKey为pay.success 支付成功时不再调用交易服务更新订单状态的接口，而是发送一条消息到pay.topic，发送消息的RoutingKey 为pay.success，消息内容是订单id 交易服务监听mark.order.pay.queue队列，接收到消息后更新订单状态为已支付 4.1.配置MQ 不管是生产者还是消费者，都需要配置MQ的基本信息。分为两步： 1）添加依赖：\n1 2 3 4 5 \u0026lt;!--消息发送--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2）配置MQ地址：\n1 2 3 4 5 6 7 spring: rabbitmq: host: 192.168.150.101 # 你的虚拟机IP port: 5672 # 端口 virtual-host: /hmall # 虚拟主机 username: hmall # 用户名 password: 123 # 密码 4.1.接收消息 在trade-service服务中定义一个消息监听类： 其代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package com.hmall.trade.listener; import com.hmall.trade.service.IOrderService; import lombok.RequiredArgsConstructor; import org.springframework.amqp.core.ExchangeTypes; import org.springframework.amqp.rabbit.annotation.Exchange; import org.springframework.amqp.rabbit.annotation.Queue; import org.springframework.amqp.rabbit.annotation.QueueBinding; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.stereotype.Component; @Component @RequiredArgsConstructor public class PayStatusListener { private final IOrderService orderService; @RabbitListener(bindings = @QueueBinding( value = @Queue(name = \u0026#34;mark.order.pay.queue\u0026#34;, durable = \u0026#34;true\u0026#34;), exchange = @Exchange(name = \u0026#34;pay.topic\u0026#34;, type = ExchangeTypes.TOPIC), key = \u0026#34;pay.success\u0026#34; )) public void listenPaySuccess(Long orderId){ orderService.markOrderPaySuccess(orderId); } } 4.2.发送消息 修改pay-service服务下的com.hmall.pay.service.impl.PayOrderServiceImpl类中的tryPayOrderByBalance方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 private final RabbitTemplate rabbitTemplate; @Override @Transactional public void tryPayOrderByBalance(PayOrderDTO payOrderDTO) { // 1.查询支付单 PayOrder po = getById(payOrderDTO.getId()); // 2.判断状态 if(!PayStatus.WAIT_BUYER_PAY.equalsValue(po.getStatus())){ // 订单不是未支付，状态异常 throw new BizIllegalException(\u0026#34;交易已支付或关闭！\u0026#34;); } // 3.尝试扣减余额 userClient.deductMoney(payOrderDTO.getPw(), po.getAmount()); // 4.修改支付单状态 boolean success = markPayOrderSuccess(payOrderDTO.getId(), LocalDateTime.now()); if (!success) { throw new BizIllegalException(\u0026#34;交易已支付或关闭！\u0026#34;); } // 5.修改订单状态 // tradeClient.markOrderPaySuccess(po.getBizOrderNo()); try { rabbitTemplate.convertAndSend(\u0026#34;pay.topic\u0026#34;, \u0026#34;pay.success\u0026#34;, po.getBizOrderNo()); } catch (Exception e) { log.error(\u0026#34;支付成功的消息发送失败，支付单id：{}， 交易单id：{}\u0026#34;, po.getId(), po.getBizOrderNo(), e); } } 5.练习 5.1.抽取共享的MQ配置 将MQ配置抽取到Nacos中管理，微服务中直接使用共享配置。\n5.2.改造下单功能 改造下单功能，将基于OpenFeign的清理购物车同步调用，改为基于RabbitMQ的异步通知：\n定义topic类型交换机，命名为trade.topic 定义消息队列，命名为cart.clear.queue 将cart.clear.queue与trade.topic绑定，BindingKey为order.create 下单成功时不再调用清理购物车接口，而是发送一条消息到trade.topic，发送消息的RoutingKey 为order.create，消息内容是下单的具体商品、当前登录用户信息 购物车服务监听cart.clear.queue队列，接收到消息后清理指定用户的购物车中的指定商品 5.3.登录信息传递优化 某些业务中，需要根据登录用户信息处理业务，而基于MQ的异步调用并不会传递登录用户信息。前面我们的做法比较麻烦，至少要做两件事：\n消息发送者在消息体中传递登录用户 消费者获取消息体中的登录用户，处理业务 这样做不仅麻烦，而且编程体验也不统一，毕竟我们之前都是使用UserContext来获取用户。\n大家思考一下：有没有更优雅的办法传输登录用户信息，让使用MQ的人无感知，依然采用UserContext来随时获取用户。\n参考资料： Spring AMQP\n5.4.改造项目一 思考一下，项目一中的哪些业务可以由同步方式改为异步方式调用？试着改造一下。 举例：短信发送\n","date":"2025-07-22T00:00:00Z","image":"https://nova-bryan.github.io/p/rabbitmq%E5%9F%BA%E7%A1%80/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/rabbitmq%E5%9F%BA%E7%A1%80/","title":"RabbitMQ基础"},{"content":"基础篇Redis 开篇导读 理想课程\n小伙伴们理想的课程一定是能够通过讲解的方式，得到如下这些启发，我们的课程会从基础到精通，从redis小白，到redis大牛，还在等什么，这套课程一定就是你最适合你的课程~\n1.Redis简单介绍 Redis是一种键值型的NoSql数据库，这里有两个关键字：\n键值型 NoSql 其中键值型，是指Redis中存储的数据都是以key.value对的形式存储，而value的形式多种多样，可以是字符串.数值.甚至json：\n而NoSql则是相对于传统关系型数据库而言，有很大差异的一种数据库。\n对于存储的数据，没有类似Mysql那么严格的约束，比如唯一性，是否可以为null等等，所以我们把这种松散结构的数据库，称之为NoSQL数据库。\n2.课程目录 初始Redis 认识NoSQL 认识Redis 安装Redis Redis常见命令 5种常见数据结构 通用命令 不同数据结构的操作命令 Redis的Java客户端 Jedis客户端 SpringDataRedis客户端 3.初始Redis 3.1.认识NoSQL NoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n3.1.1.结构化与非结构化 传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名.字段数据类型.字段约束等等信息，插入的数据必须遵守这些约束：\n而NoSql则对数据库格式没有严格约束，往往形式松散，自由。\n可以是键值型：\n也可以是文档型：\n甚至可以是图格式：\n3.1.2.关联和非关联 传统数据库的表与表之间往往存在关联，例如外键：\n而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { id: 1, name: \u0026#34;张三\u0026#34;, orders: [ { id: 1, item: { id: 10, title: \u0026#34;荣耀6\u0026#34;, price: 4999 } }, { id: 2, item: { id: 20, title: \u0026#34;小米11\u0026#34;, price: 3999 } } ] } 此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。\n3.1.3.查询方式 传统关系型数据库会基于Sql语句做查询，语法有统一标准；\n而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n3.1.4.事务 传统关系型数据库能满足事务ACID的原则。\n而非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。\n3.1.5.总结 除了上述四点以外，在存储方式.扩展性.查询性能上关系型与非关系型也都有着显著差异，总结如下：\n存储方式 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些 扩展性 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦 3.2.认识Redis Redis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。\n特征：\n键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存.IO多路复用.良好的编码）。 支持数据持久化 支持主从集群.分片集群 支持多语言客户端 作者：Antirez\nRedis的官方网站地址：https://redis.io/\n3.3.安装Redis 大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包。因此课程中我们会基于Linux系统来安装Redis.\n此处选择的Linux版本为CentOS 7.\n3.3.1.依赖库 Redis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：\n1 yum install -y gcc tcl 3.3.2.上传安装包并解压 然后将课前资料提供的Redis安装包上传到虚拟机的任意目录：\n例如，我放到了/usr/local/src 目录：\n解压缩：\n1 tar -xzf redis-6.2.6.tar.gz 解压后：\n进入redis目录：\n1 cd redis-6.2.6 运行编译命令：\n1 make \u0026amp;\u0026amp; make install 如果没有出错，应该就安装成功了。\n默认的安装路径是在 /usr/local/bin目录下：\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\nredis-cli：是redis提供的命令行客户端 redis-server：是redis的服务端启动脚本 redis-sentinel：是redis的哨兵启动脚本 3.3.3.启动 redis的启动方式有很多种，例如：\n默认启动 指定配置启动 开机自启 3.3.4.默认启动 安装完成后，在任意目录输入redis-server命令即可启动Redis：\n1 redis-server 如图：\n这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。\n3.3.5.指定配置启动 如果要让Redis以后台方式启动，则必须修改Redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf：\n我们先将这个配置文件备份一份：\n1 cp redis.conf redis.conf.bck 然后修改redis.conf文件中的一些配置：\n1 2 3 4 5 6 # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 Redis的其它常见配置：\n1 2 3 4 5 6 7 8 9 10 # 监听的端口 port 6379 # 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志.持久化等文件会保存在这个目录 dir . # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb # 日志文件，默认为空，不记录日志，可以指定日志文件名 logfile \u0026#34;redis.log\u0026#34; 启动Redis：\n1 2 3 4 # 进入redis安装目录 cd /usr/local/src/redis-6.2.6 # 启动 redis-server redis.conf 停止服务：\n1 2 3 # 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务， # 因为之前配置了密码，因此需要通过 -u 来指定密码 redis-cli -u 123321 shutdown 3.3.6.开机自启 我们也可以通过配置来实现开机自启。\n首先，新建一个系统服务文件：\n1 vi /etc/systemd/system/redis.service 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 [Unit] Description=redis-server After=network.target [Service] Type=forking ExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target 然后重载系统服务：\n1 systemctl daemon-reload 现在，我们可以用下面这组命令来操作redis了：\n1 2 3 4 5 6 7 8 # 启动 systemctl start redis # 停止 systemctl stop redis # 重启 systemctl restart redis # 查看状态 systemctl status redis 执行下面的命令，可以让redis开机自启：\n1 systemctl enable redis 3.4.Redis桌面客户端 安装完成Redis，我们就可以操作Redis，实现数据的CRUD了。这需要用到Redis客户端，包括：\n命令行客户端 图形化桌面客户端 编程客户端 3.4.1.Redis命令行客户端 Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：\n1 redis-cli [options] [commonds] 其中常见的options有：\n-h 127.0.0.1：指定要连接的redis节点的IP地址，默认是127.0.0.1 -p 6379：指定要连接的redis节点的端口，默认是6379 -a 123321：指定redis的访问密码 其中的commonds就是Redis的操作命令，例如：\nping：与redis服务端做心跳测试，服务端正常会返回pong 不指定commond时，会进入redis-cli的交互控制台：\n3.4.2.图形化桌面客户端 GitHub上的大神编写了Redis的图形化桌面客户端，地址：https://github.com/uglide/RedisDesktopManager\n不过该仓库提供的是RedisDesktopManager的源码，并未提供windows安装包。\n在下面这个仓库可以找到安装包：https://github.com/lework/RedisDesktopManager-Windows/releases\n3.4.3.安装 在课前资料中可以找到Redis的图形化桌面客户端：\n解压缩后，运行安装程序即可安装：\n安装完成后，在安装目录下找到rdm.exe文件：\n双击即可运行：\n3.4.4.建立连接 点击左上角的连接到Redis服务器按钮：\n在弹出的窗口中填写Redis服务信息：\n点击确定后，在左侧菜单会出现这个链接：\n点击即可建立连接了。\nRedis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。\n如果是基于redis-cli连接Redis服务，可以通过select命令来选择数据库：\n1 2 # 选择 0号库 select 0 4.Redis常见命令 4.1 Redis数据结构介绍 Redis是一个key-value的数据库，key一般是String类型，不过value的类型多种多样：\n贴心小建议：命令不要死记，学会查询就好啦\nRedis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令：\n当然我们也可以通过Help命令来帮助我们去查看命令\n4.2 Redis 通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有：\nKEYS：查看符合模板的所有key DEL：删除一个指定的key EXISTS：判断key是否存在 EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除 TTL：查看一个KEY的剩余有效期 通过help [command] 可以查看一个命令的具体用法，例如：\n课堂代码如下\nKEYS 1 2 3 4 5 6 7 8 9 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;name\u0026#34; 2) \u0026#34;age\u0026#34; 127.0.0.1:6379\u0026gt; # 查询以a开头的key 127.0.0.1:6379\u0026gt; keys a* 1) \u0026#34;age\u0026#34; 127.0.0.1:6379\u0026gt; 贴心小提示：在生产环境下，不推荐使用keys 命令，因为这个命令在key过多的情况下，效率不高\nDEL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 127.0.0.1:6379\u0026gt; help del DEL key [key ...] summary: Delete a key since: 1.0.0 group: generic 127.0.0.1:6379\u0026gt; del name #删除单个 (integer) 1 #成功删除1个 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;age\u0026#34; 127.0.0.1:6379\u0026gt; MSET k1 v1 k2 v2 k3 v3 #批量添加数据 OK 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;k3\u0026#34; 2) \u0026#34;k2\u0026#34; 3) \u0026#34;k1\u0026#34; 4) \u0026#34;age\u0026#34; 127.0.0.1:6379\u0026gt; del k1 k2 k3 k4 (integer) 3 #此处返回的是成功删除的key，由于redis中只有k1,k2,k3 所以只成功删除3个，最终返回 127.0.0.1:6379\u0026gt; 127.0.0.1:6379\u0026gt; keys * #再查询全部的key 1) \u0026#34;age\u0026#34;\t#只剩下一个了 127.0.0.1:6379\u0026gt; 贴心小提示：同学们在拷贝代码的时候，只需要拷贝对应的命令哦~\nEXISTS 1 2 3 4 5 6 7 8 9 10 11 12 127.0.0.1:6379\u0026gt; help EXISTS EXISTS key [key ...] summary: Determine if a key exists since: 1.0.0 group: generic 127.0.0.1:6379\u0026gt; exists age (integer) 1 127.0.0.1:6379\u0026gt; exists name (integer) 0 EXPIRE 贴心小提示：内存非常宝贵，对于一些数据，我们应当给他一些过期时间，当过期时间到了之后，他就会自动被删除~\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 127.0.0.1:6379\u0026gt; expire age 10 (integer) 1 127.0.0.1:6379\u0026gt; ttl age (integer) 8 127.0.0.1:6379\u0026gt; ttl age (integer) 6 127.0.0.1:6379\u0026gt; ttl age (integer) -2 127.0.0.1:6379\u0026gt; ttl age (integer) -2 #当这个key过期了，那么此时查询出来就是-2 127.0.0.1:6379\u0026gt; keys * (empty list or set) 127.0.0.1:6379\u0026gt; set age 10 #如果没有设置过期时间 OK 127.0.0.1:6379\u0026gt; ttl age (integer) -1 # ttl的返回值就是-1 4.3 Redis命令-String命令 String类型，也就是字符串类型，是Redis中最简单的存储类型。\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\nstring：普通字符串 int：整数类型，可以做自增.自减操作 float：浮点类型，可以做自增.自减操作 String的常见命令有：\nSET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期 贴心小提示：以上命令除了INCRBYFLOAT 都是常用命令\nSET 和GET: 如果key不存在则是新增，如果存在则是修改 1 2 3 4 5 6 7 8 9 10 11 127.0.0.1:6379\u0026gt; set name Rose //原来不存在 OK 127.0.0.1:6379\u0026gt; get name \u0026#34;Rose\u0026#34; 127.0.0.1:6379\u0026gt; set name Jack //原来存在，就是修改 OK 127.0.0.1:6379\u0026gt; get name \u0026#34;Jack\u0026#34; MSET和MGET 1 2 3 4 5 6 7 8 9 127.0.0.1:6379\u0026gt; MSET k1 v1 k2 v2 k3 v3 OK 127.0.0.1:6379\u0026gt; MGET name age k1 k2 k3 1) \u0026#34;Jack\u0026#34; //之前存在的name 2) \u0026#34;10\u0026#34; //之前存在的age 3) \u0026#34;v1\u0026#34; 4) \u0026#34;v2\u0026#34; 5) \u0026#34;v3\u0026#34; INCR和INCRBY和DECY 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 127.0.0.1:6379\u0026gt; get age \u0026#34;10\u0026#34; 127.0.0.1:6379\u0026gt; incr age //增加1 (integer) 11 127.0.0.1:6379\u0026gt; get age //获得age \u0026#34;11\u0026#34; 127.0.0.1:6379\u0026gt; incrby age 2 //一次增加2 (integer) 13 //返回目前的age的值 127.0.0.1:6379\u0026gt; incrby age 2 (integer) 15 127.0.0.1:6379\u0026gt; incrby age -1 //也可以增加负数，相当于减 (integer) 14 127.0.0.1:6379\u0026gt; incrby age -2 //一次减少2个 (integer) 12 127.0.0.1:6379\u0026gt; DECR age //相当于 incr 负数，减少正常用法 (integer) 11 127.0.0.1:6379\u0026gt; get age \u0026#34;11\u0026#34; SETNX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 127.0.0.1:6379\u0026gt; help setnx SETNX key value summary: Set the value of a key, only if the key does not exist since: 1.0.0 group: string 127.0.0.1:6379\u0026gt; set name Jack //设置名称 OK 127.0.0.1:6379\u0026gt; setnx name lisi //如果key不存在，则添加成功 (integer) 0 127.0.0.1:6379\u0026gt; get name //由于name已经存在，所以lisi的操作失败 \u0026#34;Jack\u0026#34; 127.0.0.1:6379\u0026gt; setnx name2 lisi //name2 不存在，所以操作成功 (integer) 1 127.0.0.1:6379\u0026gt; get name2 \u0026#34;lisi\u0026#34; SETEX 1 2 3 4 5 6 7 8 9 10 11 127.0.0.1:6379\u0026gt; setex name 10 jack OK 127.0.0.1:6379\u0026gt; ttl name (integer) 8 127.0.0.1:6379\u0026gt; ttl name (integer) 7 127.0.0.1:6379\u0026gt; ttl name (integer) 5 4.4 Redis命令-Key的层级结构 Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？\n例如，需要存储用户.商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？\n我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\nRedis的key允许有多个单词形成层级结构，多个单词之间用\u0026rsquo;:\u0026lsquo;隔开，格式如下：\n这个格式并非固定，也可以根据自己的需求来删除或添加词条。\n例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key：\nuser相关的key：heima:user:1\nproduct相关的key：heima:product:1\n如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储：\nKEY VALUE heima:user:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;Jack\u0026rdquo;, \u0026ldquo;age\u0026rdquo;: 21} heima:product:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;小米11\u0026rdquo;, \u0026ldquo;price\u0026rdquo;: 4999} 一旦我们向redis采用这样的方式存储，那么在可视化界面中，redis会以层级结构来进行存储，形成类似于这样的结构，更加方便Redis获取数据\n4.5 Redis命令-Hash命令 Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。\nString结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便：\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD：\nHash类型的常见命令\nHSET key field value：添加或者修改hash类型key的field的值\nHGET key field：获取一个hash类型key的field的值\nHMSET：批量添加多个hash类型key的field的值\nHMGET：批量获取多个hash类型key的field的值\nHGETALL：获取一个hash类型的key中的所有的field和value\nHKEYS：获取一个hash类型的key中的所有的field\nHINCRBY:让一个hash类型key的字段值自增并指定步长\nHSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行\n贴心小提示：哈希结构也是我们以后实际开发中常用的命令哟\nHSET和HGET 1 2 3 4 5 6 7 8 9 10 127.0.0.1:6379\u0026gt; HSET heima:user:3 name Lucy//大key是 heima:user:3 小key是name，小value是Lucy (integer) 1 127.0.0.1:6379\u0026gt; HSET heima:user:3 age 21// 如果操作不存在的数据，则是新增 (integer) 1 127.0.0.1:6379\u0026gt; HSET heima:user:3 age 17 //如果操作存在的数据，则是修改 (integer) 0 127.0.0.1:6379\u0026gt; HGET heima:user:3 name \u0026#34;Lucy\u0026#34; 127.0.0.1:6379\u0026gt; HGET heima:user:3 age \u0026#34;17\u0026#34; HMSET和HMGET 1 2 3 4 5 6 7 8 127.0.0.1:6379\u0026gt; HMSET heima:user:4 name HanMeiMei OK 127.0.0.1:6379\u0026gt; HMSET heima:user:4 name LiLei age 20 sex man OK 127.0.0.1:6379\u0026gt; HMGET heima:user:4 name age sex 1) \u0026#34;LiLei\u0026#34; 2) \u0026#34;20\u0026#34; 3) \u0026#34;man\u0026#34; HGETALL 1 2 3 4 5 6 7 127.0.0.1:6379\u0026gt; HGETALL heima:user:4 1) \u0026#34;name\u0026#34; 2) \u0026#34;LiLei\u0026#34; 3) \u0026#34;age\u0026#34; 4) \u0026#34;20\u0026#34; 5) \u0026#34;sex\u0026#34; 6) \u0026#34;man\u0026#34; HKEYS和HVALS 1 2 3 4 5 6 7 8 127.0.0.1:6379\u0026gt; HKEYS heima:user:4 1) \u0026#34;name\u0026#34; 2) \u0026#34;age\u0026#34; 3) \u0026#34;sex\u0026#34; 127.0.0.1:6379\u0026gt; HVALS heima:user:4 1) \u0026#34;LiLei\u0026#34; 2) \u0026#34;20\u0026#34; 3) \u0026#34;man\u0026#34; HINCRBY 1 2 3 4 5 6 7 8 127.0.0.1:6379\u0026gt; HINCRBY heima:user:4 age 2 (integer) 22 127.0.0.1:6379\u0026gt; HVALS heima:user:4 1) \u0026#34;LiLei\u0026#34; 2) \u0026#34;22\u0026#34; 3) \u0026#34;man\u0026#34; 127.0.0.1:6379\u0026gt; HINCRBY heima:user:4 age -2 (integer) 20 HSETNX 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 127.0.0.1:6379\u0026gt; HSETNX heima:user4 sex woman (integer) 1 127.0.0.1:6379\u0026gt; HGETALL heima:user:3 1) \u0026#34;name\u0026#34; 2) \u0026#34;Lucy\u0026#34; 3) \u0026#34;age\u0026#34; 4) \u0026#34;17\u0026#34; 127.0.0.1:6379\u0026gt; HSETNX heima:user:3 sex woman (integer) 1 127.0.0.1:6379\u0026gt; HGETALL heima:user:3 1) \u0026#34;name\u0026#34; 2) \u0026#34;Lucy\u0026#34; 3) \u0026#34;age\u0026#34; 4) \u0026#34;17\u0026#34; 5) \u0026#34;sex\u0026#34; 6) \u0026#34;woman\u0026#34; 4.6 Redis命令-List命令 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n特征也与LinkedList类似：\n有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\nList的常见命令有：\nLPUSH key element \u0026hellip; ：向列表左侧插入一个或多个元素 LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element \u0026hellip; ：向列表右侧插入一个或多个元素 RPOP key：移除并返回列表右侧的第一个元素 LRANGE key star end：返回一段角标范围内的所有元素 BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil LPUSH和RPUSH 1 2 3 4 127.0.0.1:6379\u0026gt; LPUSH users 1 2 3 (integer) 3 127.0.0.1:6379\u0026gt; RPUSH users 4 5 6 (integer) 6 LPOP和RPOP 1 2 3 4 127.0.0.1:6379\u0026gt; LPOP users \u0026#34;3\u0026#34; 127.0.0.1:6379\u0026gt; RPOP users \u0026#34;6\u0026#34; LRANGE 1 2 3 127.0.0.1:6379\u0026gt; LRANGE users 1 2 1) \u0026#34;1\u0026#34; 2) \u0026#34;4\u0026#34; 4.7 Redis命令-Set命令 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征：\n无序 元素不可重复 查找快 支持交集.并集.差集等功能 Set类型的常见命令\nSADD key member \u0026hellip; ：向set中添加一个或多个元素 SREM key member \u0026hellip; : 移除set中的指定元素 SCARD key： 返回set中元素的个数 SISMEMBER key member：判断一个元素是否存在于set中 SMEMBERS：获取set中的所有元素 SINTER key1 key2 \u0026hellip; ：求key1与key2的交集 SDIFF key1 key2 \u0026hellip; ：求key1与key2的差集 SUNION key1 key2 ..：求key1和key2的并集 例如两个集合：s1和s2:\n求交集：SINTER s1 s2\n求s1与s2的不同：SDIFF s1 s2\n具体命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 127.0.0.1:6379\u0026gt; sadd s1 a b c (integer) 3 127.0.0.1:6379\u0026gt; smembers s1 1) \u0026#34;c\u0026#34; 2) \u0026#34;b\u0026#34; 3) \u0026#34;a\u0026#34; 127.0.0.1:6379\u0026gt; srem s1 a (integer) 1 127.0.0.1:6379\u0026gt; SISMEMBER s1 a (integer) 0 127.0.0.1:6379\u0026gt; SISMEMBER s1 b (integer) 1 127.0.0.1:6379\u0026gt; SCARD s1 (integer) 2 案例\n将下列数据用Redis的Set集合来存储： 张三的好友有：李四.王五.赵六 李四的好友有：王五.麻子.二狗 利用Set的命令实现下列功能： 计算张三的好友有几人 计算张三和李四有哪些共同好友 查询哪些人是张三的好友却不是李四的好友 查询张三和李四的好友总共有哪些人 判断李四是否是张三的好友 判断张三是否是李四的好友 将李四从张三的好友列表中移除 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 127.0.0.1:6379\u0026gt; SADD zs lisi wangwu zhaoliu (integer) 3 127.0.0.1:6379\u0026gt; SADD ls wangwu mazi ergou (integer) 3 127.0.0.1:6379\u0026gt; SCARD zs (integer) 3 127.0.0.1:6379\u0026gt; SINTER zs ls 1) \u0026#34;wangwu\u0026#34; 127.0.0.1:6379\u0026gt; SDIFF zs ls 1) \u0026#34;zhaoliu\u0026#34; 2) \u0026#34;lisi\u0026#34; 127.0.0.1:6379\u0026gt; SUNION zs ls 1) \u0026#34;wangwu\u0026#34; 2) \u0026#34;zhaoliu\u0026#34; 3) \u0026#34;lisi\u0026#34; 4) \u0026#34;mazi\u0026#34; 5) \u0026#34;ergou\u0026#34; 127.0.0.1:6379\u0026gt; SISMEMBER zs lisi (integer) 1 127.0.0.1:6379\u0026gt; SISMEMBER ls zhangsan (integer) 0 127.0.0.1:6379\u0026gt; SREM zs lisi (integer) 1 127.0.0.1:6379\u0026gt; SMEMBERS zs 1) \u0026#34;zhaoliu\u0026#34; 2) \u0026#34;wangwu\u0026#34; 4.8 Redis命令-SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。\nSortedSet具备下列特性：\n可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。\nSortedSet的常见命令有：\nZADD key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZREM key member：删除sorted set中的一个指定元素 ZSCORE key member : 获取sorted set中的指定元素的score值 ZRANK key member：获取sorted set 中的指定元素的排名 ZCARD key：获取sorted set中的元素个数 ZCOUNT key min max：统计score值在给定范围内的所有元素的个数 ZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max：按照score排序后，获取指定排名范围内的元素 ZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素 ZDIFF.ZINTER.ZUNION：求差集.交集.并集 注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如：\n升序获取sorted set 中的指定元素的排名：ZRANK key member 降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber 5.Redis的Java客户端-Jedis 在Redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/\n其中Java客户端也包含很多：\n标记为❤的就是推荐使用的java客户端，包括：\nJedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此我们后期会直接以SpringDataRedis来学习。 Redisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map.Queue等，而且支持跨进程的同步机制：Lock.Semaphore等待，比较适合用来实现特殊的功能需求。 5.1 Jedis快速入门 入门案例详细步骤\n案例分析：\n0）创建工程：\n1）引入依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!--jedis--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--单元测试--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.7.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 2）建立连接\n新建一个单元测试类，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\u0026#34;192.168.150.101\u0026#34;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\u0026#34;123321\u0026#34;); // 3.选择库 jedis.select(0); } 3）测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Test void testString() { // 存入数据 String result = jedis.set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); System.out.println(\u0026#34;result = \u0026#34; + result); // 获取数据 String name = jedis.get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } @Test void testHash() { // 插入hash数据 jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;Jack\u0026#34;); jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); // 获取 Map\u0026lt;String, String\u0026gt; map = jedis.hgetAll(\u0026#34;user:1\u0026#34;); System.out.println(map); } 4）释放资源\n1 2 3 4 5 6 @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } 5.2 Jedis连接池 Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式\n有关池化思想，并不仅仅是这里会使用，很多地方都有，比如说我们的数据库连接池，比如我们tomcat中的线程池，这些都是池化思想的体现。\n5.2.1.创建Jedis的连接池 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class JedisConnectionFacotry { private static final JedisPool jedisPool; static { //配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); //创建连接池对象 jedisPool = new JedisPool(poolConfig, \u0026#34;192.168.150.101\u0026#34;,6379,1000,\u0026#34;123321\u0026#34;); } public static Jedis getJedis(){ return jedisPool.getResource(); } } 代码说明：\n1） JedisConnectionFacotry：工厂设计模式是实际开发中非常常用的一种设计模式，我们可以使用工厂，去降低代的耦合，比如Spring中的Bean的创建，就用到了工厂设计模式\n2）静态代码块：随着类的加载而加载，确保只能执行一次，我们在加载当前工厂类的时候，就可以执行static的操作完成对 连接池的初始化\n3）最后提供返回连接池中连接的方法.\n5.2.2.改造原始代码 代码说明:\n1.在我们完成了使用工厂设计模式来完成代码的编写之后，我们在获得连接时，就可以通过工厂来获得。\n，而不用直接去new对象，降低耦合，并且使用的还是连接池对象。\n2.当我们使用了连接池后，当我们关闭连接其实并不是关闭，而是将Jedis还回连接池的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @BeforeEach void setUp(){ //建立连接 /*jedis = new Jedis(\u0026#34;127.0.0.1\u0026#34;,6379);*/ jedis = JedisConnectionFacotry.getJedis(); //选择库 jedis.select(0); } @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } 6.Redis的Java客户端-SpringDataRedis SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis\n提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK.JSON.字符串.Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 SpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中：\n6.1.快速入门 SpringBoot已经提供了对SpringDataRedis的支持，使用非常简单：\n6.1.1.导入pom坐标 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.7\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.heima\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redis-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;redis-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--redis依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--common-pool--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--Jackson依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 6.1.2 .配置文件 1 2 3 4 5 6 7 8 9 10 11 spring: redis: host: 192.168.150.101 port: 6379 password: 123321 lettuce: pool: max-active: 8 #最大连接 max-idle: 8 #最大空闲连接 min-idle: 0 #最小空闲连接 max-wait: 100ms #连接等待时间 6.1.3.测试代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @SpringBootTest class RedisDemoApplicationTests { @Autowired private RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate; @Test void testString() { // 写入一条String数据 redisTemplate.opsForValue().set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); // 获取string数据 Object name = redisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } } 贴心小提示：SpringDataJpa使用起来非常简单，记住如下几个步骤即可\nSpringDataRedis的使用步骤：\n引入spring-boot-starter-data-redis依赖 在application.yml配置Redis信息 注入RedisTemplate 6.2 .数据序列化器 RedisTemplate可以接收任意Object作为值写入Redis：\n只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的：\n缺点：\n可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory connectionFactory){ // 创建RedisTemplate对象 RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } 这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图：\n整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。\n6.3 StringRedisTemplate 尽管JSON的序列化方式可以满足我们的需求，但依然存在一些问题，如图：\n为了在反序列化时知道对象的类型，JSON序列化器会将类的class类型写入json结果中，存入Redis，会带来额外的内存开销。\n为了减少内存的消耗，我们可以采用手动序列化的方式，换句话说，就是不借助默认的序列化器，而是我们自己来控制序列化的动作，同时，我们只采用String的序列化器，这样，在存储value时，我们就不需要在内存中就不用多存储数据，从而节约我们的内存空间\n这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。\n省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @SpringBootTest class RedisStringTests { @Autowired private StringRedisTemplate stringRedisTemplate; @Test void testString() { // 写入一条String数据 stringRedisTemplate.opsForValue().set(\u0026#34;verify:phone:13600527634\u0026#34;, \u0026#34;124143\u0026#34;); // 获取string数据 Object name = stringRedisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException { // 创建对象 User user = new User(\u0026#34;虎哥\u0026#34;, 21); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(\u0026#34;user:200\u0026#34;, json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(\u0026#34;user:200\u0026#34;); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(\u0026#34;user1 = \u0026#34; + user1); } } 此时我们再来看一看存储的数据，小伙伴们就会发现那个class数据已经不在了，节约了我们的空间~\n最后小总结：\nRedisTemplate的两种序列化实践方案：\n方案一：\n自定义RedisTemplate 修改RedisTemplate的序列化器为GenericJackson2JsonRedisSerializer 方案二：\n使用StringRedisTemplate 写入Redis时，手动把对象序列化为JSON 读取Redis时，手动把读取到的JSON反序列化为对象 6.4 Hash结构操作 在基础篇的最后，咱们对Hash结构操作一下，收一个小尾巴，这个代码咱们就不再解释啦\n马上就开始新的篇章~~~进入到我们的Redis实战篇\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @SpringBootTest class RedisStringTests { @Autowired private StringRedisTemplate stringRedisTemplate; @Test void testHash() { stringRedisTemplate.opsForHash().put(\u0026#34;user:400\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); stringRedisTemplate.opsForHash().put(\u0026#34;user:400\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); Map\u0026lt;Object, Object\u0026gt; entries = stringRedisTemplate.opsForHash().entries(\u0026#34;user:400\u0026#34;); System.out.println(\u0026#34;entries = \u0026#34; + entries); } } ","date":"2025-07-19T00:00:00Z","image":"https://nova-bryan.github.io/p/redis%E5%88%9D%E7%BA%A7/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/redis%E5%88%9D%E7%BA%A7/","title":"Redis初级"},{"content":"Redis快速入门 Redis的常见命令和客户端使用\n1.初识Redis Redis是一种键值型的NoSql数据库，这里有两个关键字：\n键值型\nNoSql\n其中键值型，是指Redis中存储的数据都是以key、value对的形式存储，而value的形式多种多样，可以是字符串、数值、甚至json：\n而NoSql则是相对于传统关系型数据库而言，有很大差异的一种数据库。\n1.1.认识NoSQL NoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。\n1.1.1.结构化与非结构化 传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名、字段数据类型、字段约束等等信息，插入的数据必须遵守这些约束：\n而NoSql则对数据库格式没有严格约束，往往形式松散，自由。\n可以是键值型：\n也可以是文档型：\n甚至可以是图格式：\n1.1.2.关联和非关联 传统数据库的表与表之间往往存在关联，例如外键：\n而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { id: 1, name: \u0026#34;张三\u0026#34;, orders: [ { id: 1, item: { id: 10, title: \u0026#34;荣耀6\u0026#34;, price: 4999 } }, { id: 2, item: { id: 20, title: \u0026#34;小米11\u0026#34;, price: 3999 } } ] } 此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。\n1.1.3.查询方式 传统关系型数据库会基于Sql语句做查询，语法有统一标准；\n而不同的非关系数据库查询语法差异极大，五花八门各种各样。\n1.1.4.事务 传统关系型数据库能满足事务ACID的原则。\n而非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。\n1.1.5.总结 除了上述四点以外，在存储方式、扩展性、查询性能上关系型与非关系型也都有着显著差异，总结如下：\n存储方式 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些 扩展性 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦 1.2.认识Redis Redis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。\n特征：\n键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存、IO多路复用、良好的编码）。 支持数据持久化 支持主从集群、分片集群 支持多语言客户端 作者：Antirez\nRedis的官方网站地址：https://redis.io/\n1.3.安装Redis 大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包。因此课程中我们会基于Linux系统来安装Redis.\n此处选择的Linux版本为CentOS 7.\n1.3.1.依赖库 Redis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖：\n1 yum install -y gcc tcl 1.3.2.上传安装包并解压 然后将课前资料提供的Redis安装包上传到虚拟机的任意目录：\n例如，我放到了/usr/local/src 目录：\n解压缩：\n1 tar -xzf redis-6.2.6.tar.gz 解压后：\n进入redis目录：\n1 cd redis-6.2.6 运行编译命令：\n1 make \u0026amp;\u0026amp; make install 如果没有出错，应该就安装成功了。\n默认的安装路径是在 /usr/local/bin目录下：\n该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中：\nredis-cli：是redis提供的命令行客户端 redis-server：是redis的服务端启动脚本 redis-sentinel：是redis的哨兵启动脚本 1.3.3.启动 redis的启动方式有很多种，例如：\n默认启动 指定配置启动 开机自启 1.3.4.默认启动 安装完成后，在任意目录输入redis-server命令即可启动Redis：\n1 redis-server 如图：\n这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。\n1.3.5.指定配置启动 如果要让Redis以后台方式启动，则必须修改Redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf：\n我们先将这个配置文件备份一份：\n1 cp redis.conf redis.conf.bck 然后修改redis.conf文件中的一些配置：\n1 2 3 4 5 6 # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 Redis的其它常见配置：\n1 2 3 4 5 6 7 8 9 10 # 监听的端口 port 6379 # 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录 dir . # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb # 日志文件，默认为空，不记录日志，可以指定日志文件名 logfile \u0026#34;redis.log\u0026#34; 启动Redis：\n1 2 3 4 # 进入redis安装目录 cd /usr/local/src/redis-6.2.6 # 启动 redis-server redis.conf 停止服务：\n1 2 3 # 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务， # 因为之前配置了密码，因此需要通过 -u 来指定密码 redis-cli -u 123321 shutdown 1.3.6.开机自启 我们也可以通过配置来实现开机自启。\n首先，新建一个系统服务文件：\n1 vi /etc/systemd/system/redis.service 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 [Unit] Description=redis-server After=network.target [Service] Type=forking ExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target 然后重载系统服务：\n1 systemctl daemon-reload 现在，我们可以用下面这组命令来操作redis了：\n1 2 3 4 5 6 7 8 # 启动 systemctl start redis # 停止 systemctl stop redis # 重启 systemctl restart redis # 查看状态 systemctl status redis 执行下面的命令，可以让redis开机自启：\n1 systemctl enable redis 1.4.Redis桌面客户端 安装完成Redis，我们就可以操作Redis，实现数据的CRUD了。这需要用到Redis客户端，包括：\n命令行客户端 图形化桌面客户端 编程客户端 1.4.1.Redis命令行客户端 Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下：\n1 redis-cli [options] [commonds] 其中常见的options有：\n-h 127.0.0.1：指定要连接的redis节点的IP地址，默认是127.0.0.1 -p 6379：指定要连接的redis节点的端口，默认是6379 -a 123321：指定redis的访问密码 其中的commonds就是Redis的操作命令，例如：\nping：与redis服务端做心跳测试，服务端正常会返回pong 不指定commond时，会进入redis-cli的交互控制台：\n1.4.2.图形化桌面客户端 GitHub上的大神编写了Redis的图形化桌面客户端，地址：https://github.com/uglide/RedisDesktopManager\n不过该仓库提供的是RedisDesktopManager的源码，并未提供windows安装包。\n在下面这个仓库可以找到安装包：https://github.com/lework/RedisDesktopManager-Windows/releases\n1.4.3.安装 在课前资料中可以找到Redis的图形化桌面客户端：\n解压缩后，运行安装程序即可安装：\n安装完成后，在安装目录下找到rdm.exe文件：\n双击即可运行：\n1.4.4.建立连接 点击左上角的连接到Redis服务器按钮：\n在弹出的窗口中填写Redis服务信息：\n点击确定后，在左侧菜单会出现这个链接：\n点击即可建立连接了。\nRedis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。\n如果是基于redis-cli连接Redis服务，可以通过select命令来选择数据库：\n1 2 # 选择 0号库 select 0 2.Redis常见命令 Redis是典型的key-value数据库，key一般是字符串，而value包含很多不同的数据类型：\nRedis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令：\n不同类型的命令称为一个group，我们也可以通过help命令来查看各种不同group的命令：\n接下来，我们就学习常见的五种基本数据类型的相关命令。\n2.1.Redis通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有：\nKEYS：查看符合模板的所有key DEL：删除一个指定的key EXISTS：判断key是否存在 EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除 TTL：查看一个KEY的剩余有效期 通过help [command] 可以查看一个命令的具体用法，例如：\n1 2 3 4 5 6 7 # 查看keys命令的帮助信息： 127.0.0.1:6379\u0026gt; help keys KEYS pattern summary: Find all keys matching the given pattern since: 1.0.0 group: generic 2.2.String类型 String类型，也就是字符串类型，是Redis中最简单的存储类型。\n其value是字符串，不过根据字符串的格式不同，又可以分为3类：\nstring：普通字符串 int：整数类型，可以做自增、自减操作 float：浮点类型，可以做自增、自减操作 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同。字符串类型的最大空间不能超过512m.\n2.2.1.String的常见命令 String的常见命令有：\nSET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期 2.2.2.Key结构 Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？\n例如，需要存储用户、商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？\n我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范：\nRedis的key允许有多个单词形成层级结构，多个单词之间用\u0026rsquo;:\u0026lsquo;隔开，格式如下：\n1 项目名:业务名:类型:id 这个格式并非固定，也可以根据自己的需求来删除或添加词条。这样以来，我们就可以把不同类型的数据区分开了。从而避免了key的冲突问题。\n例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key：\nuser相关的key：heima:user:1\nproduct相关的key：heima:product:1\n如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储：\nKEY VALUE heima:user:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;Jack\u0026rdquo;, \u0026ldquo;age\u0026rdquo;: 21} heima:product:1 {\u0026ldquo;id\u0026rdquo;:1, \u0026ldquo;name\u0026rdquo;: \u0026ldquo;小米11\u0026rdquo;, \u0026ldquo;price\u0026rdquo;: 4999} 并且，在Redis的桌面客户端中，还会以相同前缀作为层级结构，让数据看起来层次分明，关系清晰：\n2.3.Hash类型 Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。\nString结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便：\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD：\nHash的常见命令有：\nHSET key field value：添加或者修改hash类型key的field的值\nHGET key field：获取一个hash类型key的field的值\nHMSET：批量添加多个hash类型key的field的值\nHMGET：批量获取多个hash类型key的field的值\nHGETALL：获取一个hash类型的key中的所有的field和value\nHKEYS：获取一个hash类型的key中的所有的field\nHINCRBY:让一个hash类型key的字段值自增并指定步长\nHSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行\n2.4.List类型 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。\n特征也与LinkedList类似：\n有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。\nList的常见命令有：\nLPUSH key element \u0026hellip; ：向列表左侧插入一个或多个元素 LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element \u0026hellip; ：向列表右侧插入一个或多个元素 RPOP key：移除并返回列表右侧的第一个元素 LRANGE key star end：返回一段角标范围内的所有元素 BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil 2.5.Set类型 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征：\n无序\n元素不可重复\n查找快\n支持交集、并集、差集等功能\nSet的常见命令有：\nSADD key member \u0026hellip; ：向set中添加一个或多个元素 SREM key member \u0026hellip; : 移除set中的指定元素 SCARD key： 返回set中元素的个数 SISMEMBER key member：判断一个元素是否存在于set中 SMEMBERS：获取set中的所有元素 SINTER key1 key2 \u0026hellip; ：求key1与key2的交集 例如两个集合：s1和s2:\n求交集：SINTER s1 s2\n求s1与s2的不同：SDIFF s1 s2\n练习：\n将下列数据用Redis的Set集合来存储： 张三的好友有：李四、王五、赵六 李四的好友有：王五、麻子、二狗 利用Set的命令实现下列功能： 计算张三的好友有几人 计算张三和李四有哪些共同好友 查询哪些人是张三的好友却不是李四的好友 查询张三和李四的好友总共有哪些人 判断李四是否是张三的好友 判断张三是否是李四的好友 将李四从张三的好友列表中移除 2.6.SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。\nSortedSet具备下列特性：\n可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。\nSortedSet的常见命令有：\nZADD key score member：添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZREM key member：删除sorted set中的一个指定元素 ZSCORE key member : 获取sorted set中的指定元素的score值 ZRANK key member：获取sorted set 中的指定元素的排名 ZCARD key：获取sorted set中的元素个数 ZCOUNT key min max：统计score值在给定范围内的所有元素的个数 ZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max：按照score排序后，获取指定排名范围内的元素 ZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素 ZDIFF、ZINTER、ZUNION：求差集、交集、并集 注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如：\n升序获取sorted set 中的指定元素的排名：ZRANK key member\n降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber\n练习题：\n将班级的下列学生得分存入Redis的SortedSet中：\nJack 85, Lucy 89, Rose 82, Tom 95, Jerry 78, Amy 92, Miles 76\n并实现下列功能：\n删除Tom同学 获取Amy同学的分数 获取Rose同学的排名 查询80分以下有几个学生 给Amy同学加2分 查出成绩前3名的同学 查出成绩80分以下的所有同学 3.Redis的Java客户端 在Redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/\n其中Java客户端也包含很多：\n标记为*的就是推荐使用的java客户端，包括：\nJedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此我们后期会直接以SpringDataRedis来学习。 Redisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map、Queue等，而且支持跨进程的同步机制：Lock、Semaphore等待，比较适合用来实现特殊的功能需求。 3.1.Jedis客户端 Jedis的官网地址： https://github.com/redis/jedis\n3.1.1.快速入门 我们先来个快速入门：\n1）引入依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!--jedis--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--单元测试--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.7.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 2）建立连接\n新建一个单元测试类，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\u0026#34;192.168.150.101\u0026#34;, 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\u0026#34;123321\u0026#34;); // 3.选择库 jedis.select(0); } 3）测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Test void testString() { // 存入数据 String result = jedis.set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); System.out.println(\u0026#34;result = \u0026#34; + result); // 获取数据 String name = jedis.get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } @Test void testHash() { // 插入hash数据 jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;Jack\u0026#34;); jedis.hset(\u0026#34;user:1\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;21\u0026#34;); // 获取 Map\u0026lt;String, String\u0026gt; map = jedis.hgetAll(\u0026#34;user:1\u0026#34;); System.out.println(map); } 4）释放资源\n1 2 3 4 5 6 @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } 3.1.2.连接池 Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package com.heima.jedis.util; import redis.clients.jedis.*; public class JedisConnectionFactory { private static JedisPool jedisPool; static { // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); // 创建连接池对象，参数：连接池配置、服务端ip、服务端端口、超时时间、密码 jedisPool = new JedisPool(poolConfig, \u0026#34;192.168.150.101\u0026#34;, 6379, 1000, \u0026#34;123321\u0026#34;); } public static Jedis getJedis(){ return jedisPool.getResource(); } } 3.2.SpringDataRedis客户端 SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis\n提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK、JSON、字符串、Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 SpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中：\n3.2.1.快速入门 SpringBoot已经提供了对SpringDataRedis的支持，使用非常简单。\n首先，新建一个maven项目，然后按照下面步骤执行：\n1）引入依赖 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.7\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.heima\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redis-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;redis-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--redis依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--common-pool--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--Jackson依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 2）配置Redis 1 2 3 4 5 6 7 8 9 10 11 spring: redis: host: 192.168.150.101 port: 6379 password: 123321 lettuce: pool: max-active: 8 max-idle: 8 min-idle: 0 max-wait: 100ms 3）注入RedisTemplate 因为有了SpringBoot的自动装配，我们可以拿来就用：\n1 2 3 4 5 6 @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate redisTemplate; } 4）编写测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate edisTemplate; @Test void testString() { // 写入一条String数据 redisTemplate.opsForValue().set(\u0026#34;name\u0026#34;, \u0026#34;虎哥\u0026#34;); // 获取string数据 Object name = stringRedisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } } 3.2.2.自定义序列化 RedisTemplate可以接收任意Object作为值写入Redis：\n只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的：\n缺点：\n可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory connectionFactory){ // 创建RedisTemplate对象 RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } 这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图：\n整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。\n3.2.3.StringRedisTemplate 为了节省内存空间，我们可以不使用JSON序列化器来处理value，而是统一使用String序列化器，要求只能存储String类型的key和value。当需要存储Java对象时，手动完成对象的序列化和反序列化。\n因为存入和读取时的序列化及反序列化都是我们自己实现的，SpringDataRedis就不会将class信息写入Redis了。\n这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。\n省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Autowired private StringRedisTemplate stringRedisTemplate; // JSON序列化工具 private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException { // 创建对象 User user = new User(\u0026#34;虎哥\u0026#34;, 21); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(\u0026#34;user:200\u0026#34;, json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(\u0026#34;user:200\u0026#34;); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(\u0026#34;user1 = \u0026#34; + user1); } ","date":"2025-07-19T00:00:00Z","image":"https://nova-bryan.github.io/p/redis%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/redis%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","title":"Redis快速入门"},{"content":"实战篇Redis 开篇导读 亲爱的小伙伴们大家好，马上咱们就开始实战篇的内容了，相信通过本章的学习，小伙伴们就能理解各种redis的使用啦，接下来咱们来一起看看实战篇我们要学习一些什么样的内容\n短信登录 这一块我们会使用redis共享session来实现\n商户查询缓存 通过本章节，我们会理解缓存击穿，缓存穿透，缓存雪崩等问题，让小伙伴的对于这些概念的理解不仅仅是停留在概念上，更是能在代码中看到对应的内容\n优惠卷秒杀 通过本章节，我们可以学会Redis的计数器功能， 结合Lua完成高性能的redis操作，同时学会Redis分布式锁的原理，包括Redis的三种消息队列\n附近的商户 我们利用Redis的GEOHash来完成对于地理坐标的操作\nUV统计 主要是使用Redis来完成统计功能\n用户签到 使用Redis的BitMap数据统计功能\n好友关注 基于Set集合的关注、取消关注，共同关注等等功能，这一块知识咱们之前就讲过，这次我们在项目中来使用一下\n打人探店 基于List来完成点赞列表的操作，同时基于SortedSet来完成点赞的排行榜功能\n以上这些内容咱们统统都会给小伙伴们讲解清楚，让大家充分理解如何使用Redis\n1、短信登录 1.1、导入黑马点评项目 1.1.1 、导入SQL 1.1.2、有关当前模型 手机或者app端发起请求，请求我们的nginx服务器，nginx基于七层模型走的事HTTP协议，可以实现基于Lua直接绕开tomcat访问redis，也可以作为静态资源服务器，轻松扛下上万并发， 负载均衡到下游tomcat服务器，打散流量，我们都知道一台4核8G的tomcat，在优化和处理简单业务的加持下，大不了就处理1000左右的并发， 经过nginx的负载均衡分流后，利用集群支撑起整个项目，同时nginx在部署了前端项目后，更是可以做到动静分离，进一步降低tomcat服务的压力，这些功能都得靠nginx起作用，所以nginx是整个项目中重要的一环。\n在tomcat支撑起并发流量后，我们如果让tomcat直接去访问Mysql，根据经验Mysql企业级服务器只要上点并发，一般是16或32 核心cpu，32 或64G内存，像企业级mysql加上固态硬盘能够支撑的并发，大概就是4000起~7000左右，上万并发， 瞬间就会让Mysql服务器的cpu，硬盘全部打满，容易崩溃，所以我们在高并发场景下，会选择使用mysql集群，同时为了进一步降低Mysql的压力，同时增加访问的性能，我们也会加入Redis，同时使用Redis集群使得Redis对外提供更好的服务。\n1.1.3、导入后端项目 在资料中提供了一个项目源码：\n1.1.4、导入前端工程 1.1.5 运行前端项目 1.2 、基于Session实现登录流程 发送验证码：\n用户在提交手机号后，会校验手机号是否合法，如果不合法，则要求用户重新输入手机号\n如果手机号合法，后台此时生成对应的验证码，同时将验证码进行保存，然后再通过短信的方式将验证码发送给用户\n短信验证码登录、注册：\n用户将验证码和手机号进行输入，后台从session中拿到当前验证码，然后和用户输入的验证码进行校验，如果不一致，则无法通过校验，如果一致，则后台根据手机号查询用户，如果用户不存在，则为用户创建账号信息，保存到数据库，无论是否存在，都会将用户信息保存到session中，方便后续获得当前登录信息\n校验登录状态:\n用户在请求时候，会从cookie中携带者JsessionId到后台，后台通过JsessionId从session中拿到用户信息，如果没有session信息，则进行拦截，如果有session信息，则将用户信息保存到threadLocal中，并且放行\n1.3 、实现发送短信验证码功能 页面流程\n具体代码如下\n贴心小提示：\n具体逻辑上文已经分析，我们仅仅只需要按照提示的逻辑写出代码即可。\n发送验证码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Override public Result sendCode(String phone, HttpSession session) { // 1.校验手机号 if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.符合，生成验证码 String code = RandomUtil.randomNumbers(6); // 4.保存验证码到 session session.setAttribute(\u0026#34;code\u0026#34;,code); // 5.发送验证码 log.debug(\u0026#34;发送短信验证码成功，验证码：{}\u0026#34;, code); // 返回ok return Result.ok(); } 登录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.校验验证码 Object cacheCode = session.getAttribute(\u0026#34;code\u0026#34;); String code = loginForm.getCode(); if(cacheCode == null || !cacheCode.toString().equals(code)){ //3.不一致，报错 return Result.fail(\u0026#34;验证码错误\u0026#34;); } //一致，根据手机号查询用户 User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); //5.判断用户是否存在 if(user == null){ //不存在，则创建 user = createUserWithPhone(phone); } //7.保存用户信息到session中 session.setAttribute(\u0026#34;user\u0026#34;,user); return Result.ok(); } 1.4、实现登录拦截功能 温馨小贴士：tomcat的运行原理\n当用户发起请求时，会访问我们像tomcat注册的端口，任何程序想要运行，都需要有一个线程对当前端口号进行监听，tomcat也不例外，当监听线程知道用户想要和tomcat连接连接时，那会由监听线程创建socket连接，socket都是成对出现的，用户通过socket像互相传递数据，当tomcat端的socket接受到数据后，此时监听线程会从tomcat的线程池中取出一个线程执行用户请求，在我们的服务部署到tomcat后，线程会找到用户想要访问的工程，然后用这个线程转发到工程中的controller，service，dao中，并且访问对应的DB，在用户执行完请求后，再统一返回，再找到tomcat端的socket，再将数据写回到用户端的socket，完成请求和响应\n通过以上讲解，我们可以得知 每个用户其实对应都是去找tomcat线程池中的一个线程来完成工作的， 使用完成后再进行回收，既然每个请求都是独立的，所以在每个用户去访问我们的工程时，我们可以使用threadlocal来做到线程隔离，每个线程操作自己的一份数据\n温馨小贴士：关于threadlocal\n如果小伙伴们看过threadLocal的源码，你会发现在threadLocal中，无论是他的put方法和他的get方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量map，只要线程不一样，map就不一样，所以可以通过这种方式来做到线程隔离\n拦截器代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //1.获取session HttpSession session = request.getSession(); //2.获取session中的用户 Object user = session.getAttribute(\u0026#34;user\u0026#34;); //3.判断用户是否存在 if(user == null){ //4.不存在，拦截，返回401状态码 response.setStatus(401); return false; } //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((User)user); //6.放行 return true; } } 让拦截器生效\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Configuration public class MvcConfig implements WebMvcConfigurer { @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) { // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \u0026#34;/shop/**\u0026#34;, \u0026#34;/voucher/**\u0026#34;, \u0026#34;/shop-type/**\u0026#34;, \u0026#34;/upload/**\u0026#34;, \u0026#34;/blog/hot\u0026#34;, \u0026#34;/user/code\u0026#34;, \u0026#34;/user/login\u0026#34; ).order(1); // token刷新的拦截器 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(\u0026#34;/**\u0026#34;).order(0); } } 1.5、隐藏用户敏感信息 我们通过浏览器观察到此时用户的全部信息都在，这样极为不靠谱，所以我们应当在返回用户信息之前，将用户的敏感信息进行隐藏，采用的核心思路就是书写一个UserDto对象，这个UserDto对象就没有敏感信息了，我们在返回前，将有用户敏感信息的User对象转化成没有敏感信息的UserDto对象，那么就能够避免这个尴尬的问题了\n在登录方法处修改\n1 2 //7.保存用户信息到session中 session.setAttribute(\u0026#34;user\u0026#34;, BeanUtils.copyProperties(user,UserDTO.class)); 在拦截器处：\n1 2 //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((UserDTO) user); 在UserHolder处：将user对象换成UserDTO\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class UserHolder { private static final ThreadLocal\u0026lt;UserDTO\u0026gt; tl = new ThreadLocal\u0026lt;\u0026gt;(); public static void saveUser(UserDTO user){ tl.set(user); } public static UserDTO getUser(){ return tl.get(); } public static void removeUser(){ tl.remove(); } } 1.6、session共享问题 核心思路分析：\n每个tomcat中都有一份属于自己的session,假设用户第一次访问第一台tomcat，并且把自己的信息存放到第一台服务器的session中，但是第二次这个用户访问到了第二台tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的session，所以此时 整个登录拦截功能就会出现问题，我们能如何解决这个问题呢？早期的方案是session拷贝，就是说虽然每个tomcat上都有不同的session，但是每当任意一台服务器的session修改时，都会同步给其他的Tomcat服务器的session，这样的话，就可以实现session的共享了\n但是这种方案具有两个大问题\n1、每台服务器中都有完整的一份session数据，服务器压力过大。\n2、session拷贝数据时，可能会出现延迟\n所以咱们后来采用的方案都是基于redis来完成，我们把session换成redis，redis数据本身就是共享的，就可以避免session共享的问题了\n1.7 Redis代替session的业务流程 1.7.1、设计key的结构 首先我们要思考一下利用redis来存储数据，那么到底使用哪种结构呢？由于存入的数据比较简单，我们可以考虑使用String，或者是使用哈希，如下图，如果使用String，同学们注意他的value，用多占用一点空间，如果使用哈希，则他的value中只会存储他数据本身，如果不是特别在意内存，其实使用String就可以啦。\n1.7.2、设计key的具体细节 所以我们可以使用String结构，就是一个简单的key，value键值对的方式，但是关于key的处理，session他是每个用户都有自己的session，但是redis的key是共享的，咱们就不能使用code了\n在设计这个key的时候，我们之前讲过需要满足两点\n1、key要具有唯一性\n2、key要方便携带\n如果我们采用phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到redis中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串token，然后让前端带来这个token就能完成我们的整体逻辑了\n1.7.3、整体访问流程 当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到redis，并且生成token作为redis的key，当我们校验用户是否登录时，会去携带着token进行访问，从redis中取出token对应的value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到threadLocal中，并且放行。\n1.8 基于Redis实现短信登录 这里具体逻辑就不分析了，之前咱们已经重点分析过这个逻辑啦。\nUserServiceImpl代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\u0026#34;手机号格式错误！\u0026#34;); } // 3.从redis获取验证码并校验 String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + phone); String code = loginForm.getCode(); if (cacheCode == null || !cacheCode.equals(code)) { // 不一致，报错 return Result.fail(\u0026#34;验证码错误\u0026#34;); } // 4.一致，根据手机号查询用户 select * from tb_user where phone = ? User user = query().eq(\u0026#34;phone\u0026#34;, phone).one(); // 5.判断用户是否存在 if (user == null) { // 6.不存在，创建新用户并保存 user = createUserWithPhone(phone); } // 7.保存用户信息到 redis中 // 7.1.随机生成token，作为登录令牌 String token = UUID.randomUUID().toString(true); // 7.2.将User对象转为HashMap存储 UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); Map\u0026lt;String, Object\u0026gt; userMap = BeanUtil.beanToMap(userDTO, new HashMap\u0026lt;\u0026gt;(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((fieldName, fieldValue) -\u0026gt; fieldValue.toString())); // 7.3.存储 String tokenKey = LOGIN_USER_KEY + token; stringRedisTemplate.opsForHash().putAll(tokenKey, userMap); // 7.4.设置token有效期 stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.返回token return Result.ok(token); } 1.9 解决状态登录刷新问题 1.9.1 初始方案思路总结： 在这个方案中，他确实可以使用对应路径的拦截，同时刷新登录token令牌的存活时间，但是现在这个拦截器他只是拦截需要被拦截的路径，假设当前用户访问了一些不需要拦截的路径，那么这个拦截器就不会生效，所以此时令牌刷新的动作实际上就不会执行，所以这个方案他是存在问题的\n1.9.2 优化方案 既然之前的拦截器无法对不需要拦截的路径生效，那么我们可以添加一个拦截器，在第一个拦截器中拦截所有的路径，把第二个拦截器做的事情放入到第一个拦截器中，同时刷新令牌，因为第一个拦截器有了threadLocal的数据，所以此时第二个拦截器只需要判断拦截器中的user对象是否存在即可，完成整体刷新功能。\n1.9.3 代码 RefreshTokenInterceptor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.获取请求头中的token String token = request.getHeader(\u0026#34;authorization\u0026#34;); if (StrUtil.isBlank(token)) { return true; } // 2.基于TOKEN获取redis中的用户 String key = LOGIN_USER_KEY + token; Map\u0026lt;Object, Object\u0026gt; userMap = stringRedisTemplate.opsForHash().entries(key); // 3.判断用户是否存在 if (userMap.isEmpty()) { return true; } // 5.将查询到的hash数据转为UserDTO UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.存在，保存用户信息到 ThreadLocal UserHolder.saveUser(userDTO); // 7.刷新token有效期 stringRedisTemplate.expire(key, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { // 移除用户 UserHolder.removeUser(); } } LoginInterceptor\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.判断是否需要拦截（ThreadLocal中是否有用户） if (UserHolder.getUser() == null) { // 没有，需要拦截，设置状态码 response.setStatus(401); // 拦截 return false; } // 有用户，则放行 return true; } } 2、商户查询缓存 2.1 什么是缓存? 前言:什么是缓存?\n就像自行车,越野车的避震器\n举个例子:越野车,山地自行车,都拥有\u0026quot;避震器\u0026quot;,防止车体加速后因惯性,在酷似\u0026quot;U\u0026quot;字母的地形上飞跃,硬着陆导致的损害,像个弹簧一样;\n同样,实际开发中,系统也需要\u0026quot;避震器\u0026quot;,防止过高的数据访问猛冲系统,导致其操作线程无法及时处理信息而瘫痪;\n这在实际开发中对企业讲,对产品口碑,用户评价都是致命的;所以企业非常重视缓存技术;\n缓存(Cache),就是数据交换的缓冲区,俗称的缓存就是缓冲区内的数据,一般从数据库中获取,存储于本地代码(例如:\n1 2 3 4 5 例1:Static final ConcurrentHashMap\u0026lt;K,V\u0026gt; map = new ConcurrentHashMap\u0026lt;\u0026gt;(); 本地用于高并发 例2:static final Cache\u0026lt;K,V\u0026gt; USER_CACHE = CacheBuilder.newBuilder().build(); 用于redis等缓存 例3:Static final Map\u0026lt;K,V\u0026gt; map = new HashMap(); 本地缓存 由于其被Static修饰,所以随着类的加载而被加载到内存之中,作为本地缓存,由于其又被final修饰,所以其引用(例3:map)和对象(例3:new HashMap())之间的关系是固定的,不能改变,因此不用担心赋值(=)导致缓存失效;\n2.1.1 为什么要使用缓存 一句话:因为速度快,好用\n缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力\n实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\u0026quot;避震器\u0026quot;,系统是几乎撑不住的,所以企业会大量运用到缓存技术;\n但是缓存也会增加代码复杂度和运营的成本:\n2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用\n浏览器缓存：主要是存在于浏览器端的缓存\n**应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存\n**数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中\n**CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存\n2.2 添加商户缓存 在我们查询商户信息时，我们是直接操作从数据库中去进行查询的，大致逻辑是这样，直接查询数据库那肯定慢咯，所以我们需要增加缓存\n1 2 3 4 5 @GetMapping(\u0026#34;/{id}\u0026#34;) public Result queryShopById(@PathVariable(\u0026#34;id\u0026#34;) Long id) { //这里是直接查询数据库 return shopService.queryById(id); } 2.2.1 、缓存模型和思路 标准的操作方式就是查询数据库之前先查询缓存，如果缓存数据存在，则直接从缓存中返回，如果缓存数据不存在，再查询数据库，然后将数据存入redis。\n2.1.2、代码如下 代码思路：如果缓存有，则直接返回，如果缓存不存在，则查询数据库，然后存入redis。\n2.3 缓存更新策略 缓存更新是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行更新，或者把他叫为淘汰更合适。\n**内存淘汰：**redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式)\n**超时剔除：**当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存\n**主动更新：**我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题\n2.3.1 、数据库缓存不一致解决方案： 由于我们的缓存的数据源来自于数据库,而数据库的数据是会发生变化的,因此,如果当数据库中数据发生变化,而缓存却没有同步,此时就会有一致性问题存在,其后果是:\n用户使用缓存中的过时数据,就会产生类似多线程数据安全问题,从而影响业务,产品口碑等;怎么解决呢？有如下几种方案\nCache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案\nRead/Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理\nWrite Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致\n2.3.2 、数据库和缓存不一致采用什么方案 综合考虑使用方案一，但是方案一调用者如何处理呢？这里有几个问题\n操作缓存和数据库时有三个问题需要考虑：\n如果采用第一个方案，那么假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来\n删除缓存还是更新缓存？\n更新缓存：每次更新数据库都更新缓存，无效写操作较多 删除缓存：更新数据库时让缓存失效，查询时再更新缓存 如何保证缓存与数据库的操作的同时成功或失败？\n单体系统，将缓存与数据库操作放在一个事务 分布式系统，利用TCC等分布式事务方案 应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存，当他写入缓存后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。\n先操作缓存还是先操作数据库？ 先删除缓存，再操作数据库 先操作数据库，再删除缓存 2.4 实现商铺和缓存与数据库双写一致 核心思路如下：\n修改ShopController中的业务逻辑，满足下面的需求：\n根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间\n根据id修改店铺时，先修改数据库，再删除缓存\n修改重点代码1：修改ShopServiceImpl的queryById方法\n设置redis缓存时添加过期时间\n修改重点代码2\n代码分析：通过之前的淘汰，我们确定了采用删除策略，来解决双写问题，当我们修改了数据之后，然后把缓存中的数据进行删除，查询时发现缓存中没有数据，则会从mysql中加载最新的数据，从而避免数据库和缓存不一致的问题\n2.5 缓存穿透问题的解决思路 缓存穿透 ：缓存穿透是指客户端请求的数据在缓存中和数据库中都不存在，这样缓存永远不会生效，这些请求都会打到数据库。\n常见的解决方案有两种：\n缓存空对象 优点：实现简单，维护方便 缺点： 额外的内存消耗 可能造成短期的不一致 布隆过滤 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 **缓存空对象思路分析：**当我们客户端访问不存在的数据时，先请求redis，但是此时redis中没有数据，此时会访问到数据库，但是数据库中也没有数据，这个数据穿透了缓存，直击数据库，我们都知道数据库能够承载的并发不如redis这么高，如果大量的请求同时过来访问这种不存在的数据，这些请求就都会访问到数据库，简单的解决方案就是哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到缓存了\n**布隆过滤：**布隆过滤器其实采用的是哈希思想来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，这个请求会去访问redis，哪怕此时redis中的数据过期了，但是数据库中一定存在这个数据，在数据库中查询出来这个数据后，再将其放入到redis中，\n假设布隆过滤器判断这个数据不存在，则直接返回\n这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要哈希思想，就可能存在哈希冲突\n2.6 编码解决商品查询的缓存穿透问题： 核心思路如下：\n在原来的逻辑中，我们如果发现这个数据在mysql中不存在，直接就返回404了，这样是会存在缓存穿透问题的\n现在的逻辑中：如果这个数据不存在，我们不会返回404 ，还是会把这个数据写入到Redis中，并且将value设置为空，欧当再次发起查询时，我们如果发现命中之后，判断这个value是否是null，如果是null，则是之前写入的数据，证明是缓存穿透数据，如果不是，则直接返回数据。\n小总结：\n缓存穿透产生的原因是什么？\n用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力 缓存穿透的解决方案有哪些？\n缓存null值 布隆过滤 增强id的复杂度，避免被猜测id规律 做好数据的基础格式校验 加强用户权限校验 做好热点参数的限流 2.7 缓存雪崩问题及解决思路 缓存雪崩是指在同一时段大量的缓存key同时失效或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。\n解决方案：\n给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 2.8 缓存击穿问题及解决思路 缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且缓存重建业务较复杂的key突然失效了，无数的请求访问会在瞬间给数据库带来巨大的冲击。\n常见的解决方案有两种：\n互斥锁 逻辑过期 逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大\n解决方案一、使用锁来解决：\n因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。\n假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。\n解决方案二、逻辑过期方案\n方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。\n我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。\n这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。\n进行对比\n**互斥锁方案：**由于保证了互斥性，所以数据一致，且实现简单，因为仅仅只需要加一把锁而已，也没其他的事情需要操心，所以没有额外的内存消耗，缺点在于有锁就有死锁问题的发生，且只能串行执行性能肯定受到影响\n逻辑过期方案： 线程读取过程中不需要等待，性能好，有一个额外的线程持有锁去进行重构数据，但是在重构数据完成前，其他的线程只能返回之前的数据，且实现起来麻烦\n2.9 利用互斥锁解决缓存击穿问题 核心思路：相较于原来从缓存中查询不到数据后直接查询数据库而言，现在的方案是 进行查询之后，如果从缓存没有查询到数据，则进行互斥锁的获取，获取互斥锁后，判断是否获得到了锁，如果没有获得到，则休眠，过一会再进行尝试，直到获取到锁为止，才能进行查询\n如果获取到了锁的线程，再去进行查询，查询后将数据写入redis，再释放锁，返回数据，利用互斥锁就能保证只有一个线程去执行操作数据库的逻辑，防止缓存击穿\n操作锁的代码：\n核心思路就是利用redis的setnx方法来表示获取锁，该方法含义是redis中如果没有这个key，则插入成功，返回1，在stringRedisTemplate中返回true， 如果有这个key则插入失败，则返回0，在stringRedisTemplate返回false，我们可以通过true，或者是false，来表示是否有线程成功插入key，成功插入的key的线程我们认为他就是获得到锁的线程。\n1 2 3 4 5 6 7 8 private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } 操作代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 public Shop queryWithMutex(Long id) { String key = CACHE_SHOP_KEY + id; // 1、从redis中查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(\u0026#34;key\u0026#34;); // 2、判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 存在,直接返回 return JSONUtil.toBean(shopJson, Shop.class); } //判断命中的值是否是空值 if (shopJson != null) { //返回一个错误信息 return null; } // 4.实现缓存重构 //4.1 获取互斥锁 String lockKey = \u0026#34;lock:shop:\u0026#34; + id; Shop shop = null; try { boolean isLock = tryLock(lockKey); // 4.2 判断否获取成功 if(!isLock){ //4.3 失败，则休眠重试 Thread.sleep(50); return queryWithMutex(id); } //4.4 成功，根据id查询数据库 shop = getById(id); // 5.不存在，返回错误 if(shop == null){ //将空值写入redis stringRedisTemplate.opsForValue().set(key,\u0026#34;\u0026#34;,CACHE_NULL_TTL,TimeUnit.MINUTES); //返回错误信息 return null; } //6.写入redis stringRedisTemplate.opsForValue().set(key,JSONUtil.toJsonStr(shop),CACHE_NULL_TTL,TimeUnit.MINUTES); }catch (Exception e){ throw new RuntimeException(e); } finally { //7.释放互斥锁 unlock(lockKey); } return shop; } 3.0 、利用逻辑过期解决缓存击穿问题 需求：修改根据id查询商铺的业务，基于逻辑过期方式来解决缓存击穿问题\n思路分析：当用户开始查询redis时，判断是否命中，如果没有命中则直接返回空数据，不查询数据库，而一旦命中后，将value取出，判断value中的过期时间是否满足，如果没有过期，则直接返回redis中的数据，如果过期，则在开启独立线程后直接返回之前的数据，独立线程去重构数据，重构完成后释放互斥锁。\n如果封装数据：因为现在redis中存储的数据的value需要带上过期时间，此时要么你去修改原来的实体类，要么你\n步骤一、\n新建一个实体类，我们采用第二个方案，这个方案，对原来代码没有侵入性。\n1 2 3 4 5 @Data public class RedisData { private LocalDateTime expireTime; private Object data; } 步骤二、\n在ShopServiceImpl 新增此方法，利用单元测试进行缓存预热\n在测试类中\n步骤三：正式代码\nShopServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public Shop queryWithLogicalExpire( Long id ) { String key = CACHE_SHOP_KEY + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); Shop shop = JSONUtil.toBean((JSONObject) redisData.getData(), Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return shop; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ CACHE_REBUILD_EXECUTOR.submit( ()-\u0026gt;{ try{ //重建缓存 this.saveShop2Redis(id,20L); }catch (Exception e){ throw new RuntimeException(e); }finally { unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return shop; } 3.1、封装Redis工具类 基于StringRedisTemplate封装一个缓存工具类，满足下列需求：\n方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间 方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓 存击穿问题\n方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题 方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题 将逻辑进行封装\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 @Slf4j @Component public class CacheClient { private final StringRedisTemplate stringRedisTemplate; private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public CacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public void set(String key, Object value, Long time, TimeUnit unit) { stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); } public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) { // 设置逻辑过期 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 写入Redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); } public \u0026lt;R,ID\u0026gt; R queryWithPassThrough( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit){ String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(json)) { // 3.存在，直接返回 return JSONUtil.toBean(json, type); } // 判断命中的是否是空值 if (json != null) { // 返回一个错误信息 return null; } // 4.不存在，根据id查询数据库 R r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \u0026#34;\u0026#34;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); return r; } public \u0026lt;R, ID\u0026gt; R queryWithLogicalExpire( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); R r = JSONUtil.toBean((JSONObject) redisData.getData(), type); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return r; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 6.3.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -\u0026gt; { try { // 查询数据库 R newR = dbFallback.apply(id); // 重建缓存 this.setWithLogicalExpire(key, newR, time, unit); } catch (Exception e) { throw new RuntimeException(e); }finally { // 释放锁 unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return r; } public \u0026lt;R, ID\u0026gt; R queryWithMutex( String keyPrefix, ID id, Class\u0026lt;R\u0026gt; type, Function\u0026lt;ID, R\u0026gt; dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); } // 判断命中的是否是空值 if (shopJson != null) { // 返回一个错误信息 return null; } // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try { boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) { // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); } // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \u0026#34;\u0026#34;, CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { // 7.释放锁 unlock(lockKey); } // 8.返回 return r; } private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \u0026#34;1\u0026#34;, 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } } 在ShopServiceImpl 中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Resource private CacheClient cacheClient; @Override public Result queryById(Long id) { // 解决缓存穿透 Shop shop = cacheClient .queryWithPassThrough(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 互斥锁解决缓存击穿 // Shop shop = cacheClient // .queryWithMutex(CACHE_SHOP_KEY, id, Shop.class, this::getById, CACHE_SHOP_TTL, TimeUnit.MINUTES); // 逻辑过期解决缓存击穿 // Shop shop = cacheClient // .queryWithLogicalExpire(CACHE_SHOP_KEY, id, Shop.class, this::getById, 20L, TimeUnit.SECONDS); if (shop == null) { return Result.fail(\u0026#34;店铺不存在！\u0026#34;); } // 7.返回 return Result.ok(shop); } 3、优惠卷秒杀 3.1 -全局唯一ID 每个店铺都可以发布优惠券：\n当用户抢购时，就会生成订单并保存到tb_voucher_order这张表中，而订单表如果使用数据库自增ID就存在一些问题：\nid的规律性太明显 受单表数据量的限制 场景分析：如果我们的id具有太明显的规则，用户或者说商业对手很容易猜测出来我们的一些敏感信息，比如商城在一天时间内，卖出了多少单，这明显不合适。\n场景分析二：随着我们商城规模越来越大，mysql的单表的容量不宜超过500W，数据量过大之后，我们要进行拆库拆表，但拆分表了之后，他们从逻辑上讲他们是同一张表，所以他们的id是不能一样的， 于是乎我们需要保证id的唯一性。\n全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性：\n为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息：\nID的组成部分：符号位：1bit，永远为0\n时间戳：31bit，以秒为单位，可以使用69年\n序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID\n3.2 -Redis实现全局唯一Id 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Component public class RedisIdWorker { /** * 开始时间戳 */ private static final long BEGIN_TIMESTAMP = 1640995200L; /** * 序列号的位数 */ private static final int COUNT_BITS = 32; private StringRedisTemplate stringRedisTemplate; public RedisIdWorker(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public long nextId(String keyPrefix) { // 1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; // 2.生成序列号 // 2.1.获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(\u0026#34;yyyy:MM:dd\u0026#34;)); // 2.2.自增长 long count = stringRedisTemplate.opsForValue().increment(\u0026#34;icr:\u0026#34; + keyPrefix + \u0026#34;:\u0026#34; + date); // 3.拼接并返回 return timestamp \u0026lt;\u0026lt; COUNT_BITS | count; } } 测试类\n知识小贴士：关于countdownlatch\ncountdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题\n我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch\nCountDownLatch 中有两个最重要的方法\n1、countDown\n2、await\nawait 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Test void testIdWorker() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); Runnable task = () -\u0026gt; { for (int i = 0; i \u0026lt; 100; i++) { long id = redisIdWorker.nextId(\u0026#34;order\u0026#34;); System.out.println(\u0026#34;id = \u0026#34; + id); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i \u0026lt; 300; i++) { es.submit(task); } latch.await(); long end = System.currentTimeMillis(); System.out.println(\u0026#34;time = \u0026#34; + (end - begin)); } 3.3 添加优惠卷 每个店铺都可以发布优惠券，分为平价券和特价券。平价券可以任意购买，而特价券需要秒杀抢购：\ntb_voucher：优惠券的基本信息，优惠金额、使用规则等 tb_seckill_voucher：优惠券的库存、开始抢购时间，结束抢购时间。特价优惠券才需要填写这些信息\n平价卷由于优惠力度并不是很大，所以是可以任意领取\n而代金券由于优惠力度大，所以像第二种卷，就得限制数量，从表结构上也能看出，特价卷除了具有优惠卷的基本信息以外，还具有库存，抢购时间，结束时间等等字段\n**新增普通卷代码： **VoucherController\n1 2 3 4 5 @PostMapping public Result addVoucher(@RequestBody Voucher voucher) { voucherService.save(voucher); return Result.ok(voucher.getId()); } 新增秒杀卷代码：\nVoucherController\n1 2 3 4 5 @PostMapping(\u0026#34;seckill\u0026#34;) public Result addSeckillVoucher(@RequestBody Voucher voucher) { voucherService.addSeckillVoucher(voucher); return Result.ok(voucher.getId()); } VoucherServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } 3.4 实现秒杀下单 下单核心思路：当我们点击抢购时，会触发右侧的请求，我们只需要编写对应的controller即可\n秒杀下单应该思考的内容：\n下单时需要判断两点：\n秒杀是否开始或结束，如果尚未开始或已经结束则无法下单 库存是否充足，不足则无法下单 下单核心逻辑分析：\n当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件\n比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //6.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 6.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 6.2.用户id Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); // 6.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } 3.5 库存超卖问题分析 有关超卖问题分析：在我们原有代码中是这么写的\n1 2 3 4 5 6 7 8 9 10 11 12 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } 假设线程1过来查询库存，判断出来库存大于1，正准备去扣减库存，但是还没有来得及去扣减，此时线程2过来，线程2也去查询库存，发现这个数量一定也大于1，那么这两个线程都会去扣减库存，最终多个线程相当于一起去扣减库存，此时就会出现库存的超卖问题。\n超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案：见下图：\n悲观锁：\n悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等\n乐观锁：\n乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas\n乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 == 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值\n其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。\n1 2 3 4 5 6 int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; 课程中的使用方式：\n课程中的使用方式是没有像cas一样带自旋的操作，也没有对version的版本号+1 ，他的操作逻辑是在操作时，对版本号进行+1 操作，然后要求version 如果是1 的情况下，才能操作，那么第一个线程在操作后，数据库中的version变成了2，但是他自己满足version=1 ，所以没有问题，此时线程2执行，线程2 最后也需要加上条件version =1 ，但是现在由于线程1已经操作过了，所以线程2，操作时就不满足version=1 的条件了，所以线程2无法执行成功\n3.6 乐观锁解决超卖问题 修改代码方案一、\nVoucherOrderServiceImpl 在扣减库存时，改为：\n1 2 3 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) //set stock = stock -1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).eq(\u0026#34;stock\u0026#34;,voucher.getStock()).update(); //where id = ？ and stock = ? 以上逻辑的核心含义是：只要我扣减库存时的库存和之前我查询到的库存是一样的，就意味着没有人在中间修改过库存，那么此时就是安全的，但是以上这种方式通过测试发现会有很多失败的情况，失败的原因在于：在使用乐观锁过程中假设100个线程同时都拿到了100的库存，然后大家一起去进行扣减，但是100个人中只有1个人能扣减成功，其他的人在处理时，他们在扣减时，库存已经被修改过了，所以此时其他线程都会失败\n修改代码方案二、\n之前的方式要修改前后都保持一致，但是这样我们分析过，成功的概率太低，所以我们的乐观锁需要变一下，改成stock大于0 即可\n1 2 3 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update().gt(\u0026#34;stock\u0026#34;,0); //where id = ? and stock \u0026gt; 0 知识小扩展：\n针对cas中的自旋压力过大，我们可以使用Longaddr这个类去解决\nJava8 提供的一个对AtomicLong改进后的一个类，LongAdder\n大量线程并发更新一个原子性的时候，天然的问题就是自旋，会导致并发性问题，当然这也比我们直接使用syn来的好\n所以利用这么一个类，LongAdder来进行优化\n如果获取某个值，则会对cell和base的值进行递增，最后返回一个完整的值\n3.6 优惠券秒杀-一人一单 需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单\n现在的问题在于：\n优惠卷是为了引流，但是目前的情况是，一个人可以无限制的抢这个优惠卷，所以我们应当增加一层逻辑，让一个用户只能下一个单，而不是让一个用户下多个单\n具体操作逻辑如下：比如时间是否充足，如果时间充足，则进一步判断库存是否足够，然后再根据优惠卷id和用户id查询是否已经下过这个订单，如果下过这个订单，则不再下单，否则进行下单\nVoucherOrderServiceImpl\n初步代码：增加一人一单逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 5.一人一单逻辑 // 5.1.用户id Long userId = UserHolder.getUser().getId(); int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } //6，扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock= stock -1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId).update(); if (!success) { //扣减库存 return Result.fail(\u0026#34;库存不足！\u0026#34;); } //7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } **存在问题：**现在的问题还是和之前一样，并发过来，查询数据库，都不存在订单，所以我们还是需要加锁，但是乐观锁比较适合更新数据，而现在是插入数据，所以我们需要使用悲观锁操作\n**注意：**在这里提到了非常多的问题，我们需要慢慢的来思考，首先我们的初始方案是封装了一个createVoucherOrder方法，同时为了确保他线程安全，在方法上添加了一把synchronized 锁\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 @Transactional public synchronized Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } ，但是这样添加锁，锁的粒度太粗了，在使用锁过程中，控制锁粒度 是一个非常重要的事情，因为如果锁的粒度太大，会导致每个线程进来都会锁住，所以我们需要去控制锁的粒度，以下这段代码需要修改为： intern() 这个方法是从常量池中拿到数据，如果我们直接使用userId.toString() 他拿到的对象实际上是不同的对象，new出来的对象，我们使用锁必须保证锁必须是同一把，所以我们需要使用intern()方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @Transactional public Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); synchronized(userId.toString().intern()){ // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherId).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 return Result.fail(\u0026#34;用户已经购买过一次！\u0026#34;); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherId).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 return Result.fail(\u0026#34;库存不足！\u0026#34;); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } } 但是以上代码还是存在问题，问题的原因在于当前方法被spring的事务控制，如果你在方法内部加锁，可能会导致当前方法事务还没有提交，但是锁已经释放也会导致问题，所以我们选择将当前方法整体包裹起来，确保事务不会出现问题：如下：\n在seckillVoucher 方法中，添加以下逻辑，这样就能保证事务的特性，同时也控制了锁的粒度\n但是以上做法依然有问题，因为你调用的方法，其实是this.的方式调用的，事务想要生效，还得利用代理来生效，所以这个地方，我们需要获得原始的事务对象， 来操作事务\n3.7 集群环境下的并发问题 通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。\n1、我们将服务启动两份，端口分别为8081和8082：\n2、然后修改nginx的conf目录下的nginx.conf文件，配置反向代理和负载均衡：\n具体操作(略)\n有关锁失效原因分析\n由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的，但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是 集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。\n4、分布式锁 4.1 、基本原理和实现方式对比 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。\n分布式锁的核心思想就是让大家都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住线程，不让线程进行，让程序串行执行，这就是分布式锁的核心思路\n那么分布式锁他应该满足一些什么样的条件呢？\n可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思\n互斥：互斥是分布式锁的最基本的条件，使得程序串行执行\n高可用：程序不易崩溃，时时刻刻都保证较高的可用性\n高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能\n安全性：安全也是程序中必不可少的一环\n常见的分布式锁有三种\nMysql：mysql本身就带有锁机制，但是由于mysql性能本身一般，所以采用分布式锁的情况下，其实使用mysql作为分布式锁比较少见\nRedis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁\nZookeeper：zookeeper也是企业级开发中较好的一个实现分布式锁的方案，由于本套视频并不讲解zookeeper的原理和分布式锁的实现，所以不过多阐述\n4.2 、Redis分布式锁的实现核心思路 实现分布式锁时需要实现的两个基本方法：\n获取锁：\n互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁：\n手动释放 超时释放：获取锁时添加一个超时时间 核心思路：\n我们利用redis 的setNx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试即可\n4.3 实现分布式锁版本一 加锁逻辑 锁的基本接口\nSimpleRedisLock\n利用setnx方法进行加锁，同时增加过期时间，防止死锁，此方法可以保证加锁和增加过期时间具有原子性\n1 2 3 4 5 6 7 8 9 10 private static final String KEY_PREFIX=\u0026#34;lock:\u0026#34; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = Thread.currentThread().getId() // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId + \u0026#34;\u0026#34;, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁逻辑 SimpleRedisLock\n释放锁，防止删除别人的锁\n1 2 3 4 public void unlock() { //通过del删除锁 stringRedisTemplate.delete(KEY_PREFIX + name); } 修改业务代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } Long userId = UserHolder.getUser().getId(); //创建锁对象(新增代码) SimpleRedisLock lock = new SimpleRedisLock(\u0026#34;order:\u0026#34; + userId, stringRedisTemplate); //获取锁对象 boolean isLock = lock.tryLock(1200); //加锁失败 if (!isLock) { return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } 4.4 Redis分布式锁误删情况说明 逻辑说明：\n持有锁的线程在锁的内部出现了阻塞，导致他的锁自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明\n解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。\n4.5 解决Redis分布式锁误删问题 需求：修改之前的分布式锁实现，满足：在获取锁时存入线程标示（可以用UUID表示） 在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致\n如果一致则释放锁 如果不一致则不释放锁 核心逻辑：在存入锁时，放入自己线程的标识，在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。\n具体代码如下：加锁\n1 2 3 4 5 6 7 8 9 10 private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \u0026#34;-\u0026#34;; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁\n1 2 3 4 5 6 7 8 9 10 11 public void unlock() { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁中的标示 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 判断标示是否一致 if(threadId.equals(id)) { // 释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } 有关代码实操说明：\n在我们修改完此处代码后，我们重启工程，然后启动两个线程，第一个线程持有锁后，手动释放锁，第二个线程 此时进入到锁内部，再放行第一个线程，此时第一个线程由于锁的value值并非是自己，所以不能释放锁，也就无法删除别人的锁，此时第二个线程能够正确释放锁，通过这个案例初步说明我们解决了锁误删的问题。\n4.6 分布式锁的原子性问题 更为极端的误删逻辑说明：\n线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时他的锁到期了，那么此时线程2进来，但是线程1他会接着往后执行，当他卡顿结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生，\n4.7 Lua脚本解决多条命令原子性问题 Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html，这里重点介绍Redis提供的调用函数，我们可以使用lua去操作redis，又能保证他的原子性，这样就可以实现拿锁比锁删锁是一个原子性动作了，作为Java程序员这一块并不作一个简单要求，并不需要大家过于精通，只需要知道他有什么作用即可。\n这里重点介绍Redis提供的调用函数，语法如下：\n1 redis.call(\u0026#39;命令名称\u0026#39;, \u0026#39;key\u0026#39;, \u0026#39;其它参数\u0026#39;, ...) 例如，我们要执行set name jack，则脚本是这样：\n1 2 # 执行 set name jack redis.call(\u0026#39;set\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;jack\u0026#39;) 例如，我们要先执行set name Rose，再执行get name，则脚本如下：\n1 2 3 4 5 6 # 先执行 set name jack redis.call(\u0026#39;set\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;Rose\u0026#39;) # 再执行 get name local name = redis.call(\u0026#39;get\u0026#39;, \u0026#39;name\u0026#39;) # 返回 return name 写好脚本以后，需要用Redis命令来调用脚本，调用脚本的常见命令如下：\n例如，我们要执行 redis.call(\u0026lsquo;set\u0026rsquo;, \u0026rsquo;name\u0026rsquo;, \u0026lsquo;jack\u0026rsquo;) 这个脚本，语法如下：\n如果脚本中的key、value不想写死，可以作为参数传递。key类型参数会放入KEYS数组，其它参数会放入ARGV数组，在脚本中可以从KEYS和ARGV数组获取这些参数：\n接下来我们来回一下我们释放锁的逻辑：\n释放锁的业务流程是这样的\n​\t1、获取锁中的线程标示\n​\t2、判断是否与指定的标示（当前线程标示）一致\n​\t3、如果一致则释放锁（删除）\n​\t4、如果不一致则什么都不做\n如果用Lua脚本来表示则是这样的：\n最终我们操作redis的拿锁比锁删锁的lua脚本就会变成这样\n1 2 3 4 5 6 7 8 -- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示 -- 获取锁中的标示，判断是否与当前线程标示一致 if (redis.call(\u0026#39;GET\u0026#39;, KEYS[1]) == ARGV[1]) then -- 一致，则删除锁 return redis.call(\u0026#39;DEL\u0026#39;, KEYS[1]) end -- 不一致，则直接返回 return 0 4.8 利用Java代码调用Lua脚本改造分布式锁 lua脚本本身并不需要大家花费太多时间去研究，只需要知道如何调用，大致是什么意思即可，所以在笔记中并不会详细的去解释这些lua表达式的含义。\n我们的RedisTemplate中，可以利用execute方法去执行lua脚本，参数对应关系就如下图股\nJava代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 private static final DefaultRedisScript\u0026lt;Long\u0026gt; UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript\u0026lt;\u0026gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(\u0026#34;unlock.lua\u0026#34;)); UNLOCK_SCRIPT.setResultType(Long.class); } public void unlock() { // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId()); } 经过以上代码改造后，我们就能够实现 拿锁比锁删锁的原子性动作了~ 小总结：\n基于Redis的分布式锁实现思路：\n利用set nx ex获取锁，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁 特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 笔者总结：我们一路走来，利用添加过期时间，防止死锁问题的发生，但是有了过期时间之后，可能出现误删别人锁的问题，这个问题我们开始是利用删之前 通过拿锁，比锁，删锁这个逻辑来解决的，也就是删之前判断一下当前这把锁是否是属于自己的，但是现在还有原子性问题，也就是我们没法保证拿锁比锁删锁是一个原子性的动作，最后通过lua表达式来解决这个问题\n但是目前还剩下一个问题锁不住，什么是锁不住呢，你想一想，如果当过期时间到了之后，我们可以给他续期一下，比如续个30s，就好像是网吧上网， 网费到了之后，然后说，来，网管，再给我来10块的，是不是后边的问题都不会发生了，那么续期问题怎么解决呢，可以依赖于我们接下来要学习redission啦\n测试逻辑：\n第一个线程进来，得到了锁，手动删除锁，模拟锁超时了，其他线程会执行lua来抢锁，当第一天线程利用lua删除锁时，lua能保证他不能删除他的锁，第二个线程删除锁时，利用lua同样可以保证不会删除别人的锁，同时还能保证原子性。\n5、分布式锁-redission 5.1 分布式锁-redission功能介绍 基于setnx实现的分布式锁存在下面的问题：\n重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，比如HashTable这样的代码中，他的方法都是使用synchronized修饰的，假如他在一个方法内，调用另一个方法，那么此时如果是不可重入的，不就死锁了吗？所以可重入锁他的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的。\n不可重试：是指目前的分布式只能尝试一次，我们认为合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。\n**超时释放：**我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟没有锁住，有安全隐患\n主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。\n那么什么是Redission呢\nRedisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。\nRedission提供了分布式锁的多种多样的功能\n5.2 分布式锁-Redission快速入门 引入依赖：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.13.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置Redisson客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ // 配置 Config config = new Config(); config.useSingleServer().setAddress(\u0026#34;redis://192.168.150.101:6379\u0026#34;) .setPassword(\u0026#34;123321\u0026#34;); // 创建RedissonClient对象 return Redisson.create(config); } } 如何使用Redission的分布式锁\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Resource private RedissionClient redissonClient; @Test void testRedisson() throws Exception{ //获取锁(可重入)，指定锁的名称 RLock lock = redissonClient.getLock(\u0026#34;anyLock\u0026#34;); //尝试获取锁，参数分别是：获取锁的最大等待时间(期间会重试)，锁自动释放时间，时间单位 boolean isLock = lock.tryLock(1,10,TimeUnit.SECONDS); //判断获取锁成功 if(isLock){ try{ System.out.println(\u0026#34;执行业务\u0026#34;); }finally{ //释放锁 lock.unlock(); } } } 在 VoucherOrderServiceImpl\n注入RedissonClient\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @Resource private RedissonClient redissonClient; @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀尚未开始！\u0026#34;); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\u0026#34;秒杀已经结束！\u0026#34;); } // 4.判断库存是否充足 if (voucher.getStock() \u0026lt; 1) { // 库存不足 return Result.fail(\u0026#34;库存不足！\u0026#34;); } Long userId = UserHolder.getUser().getId(); //创建锁对象 这个代码不用了，因为我们现在要使用分布式锁 //SimpleRedisLock lock = new SimpleRedisLock(\u0026#34;order:\u0026#34; + userId, stringRedisTemplate); RLock lock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); //获取锁对象 boolean isLock = lock.tryLock(); //加锁失败 if (!isLock) { return Result.fail(\u0026#34;不允许重复下单\u0026#34;); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } 5.3 分布式锁-redission可重入锁原理 在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state=0，假如有人持有这把锁，那么state=1，如果持有这把锁的人再次持有这把锁，那么state就会+1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成0 时，表示当前这把锁没有被人持有。\n在redission中，我们的也支持支持可重入锁\n在分布式锁中，他采用hash结构用来存储锁，其中大key表示表示这把锁是否存在，用小key表示当前这把锁被哪个线程持有，所以接下来我们一起分析一下当前的这个lua表达式\n这个地方一共有3个参数\nKEYS[1] ： 锁名称\nARGV[1]： 锁失效时间\nARGV[2]： id + \u0026ldquo;:\u0026rdquo; + threadId; 锁的小key\nexists: 判断数据是否存在 name：是lock是否存在,如果==0，就表示当前这把锁不存在\nredis.call(\u0026lsquo;hset\u0026rsquo;, KEYS[1], ARGV[2], 1);此时他就开始往redis里边去写数据 ，写成一个hash结构\nLock{\n​ id + \u0026quot;:\u0026quot; + threadId : 1\n}\n如果当前这把锁存在，则第一个条件不满足，再判断\nredis.call(\u0026lsquo;hexists\u0026rsquo;, KEYS[1], ARGV[2]) == 1\n此时需要通过大key+小key判断当前这把锁是否是属于自己的，如果是自己的，则进行\nredis.call(\u0026lsquo;hincrby\u0026rsquo;, KEYS[1], ARGV[2], 1)\n将当前这个锁的value进行+1 ，redis.call(\u0026lsquo;pexpire\u0026rsquo;, KEYS[1], ARGV[1]); 然后再对其设置过期时间，如果以上两个条件都不满足，则表示当前这把锁抢锁失败，最后返回pttl，即为当前这把锁的失效时间\n如果小伙帮们看了前边的源码， 你会发现他会去判断当前这个方法的返回值是否为null，如果是null，则对应则前两个if对应的条件，退出抢锁逻辑，如果返回的不是null，即走了第三个分支，在源码处会进行while(true)的自旋抢锁。\n1 2 3 4 5 6 7 8 9 10 11 \u0026#34;if (redis.call(\u0026#39;exists\u0026#39;, KEYS[1]) == 0) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hset\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then \u0026#34; + \u0026#34;redis.call(\u0026#39;hincrby\u0026#39;, KEYS[1], ARGV[2], 1); \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return nil; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;return redis.call(\u0026#39;pttl\u0026#39;, KEYS[1]);\u0026#34; 5.4 分布式锁-redission锁重试和WatchDog机制 说明：由于课程中已经说明了有关tryLock的源码解析以及其看门狗原理，所以笔者在这里给大家分析lock()方法的源码解析，希望大家在学习过程中，能够掌握更多的知识\n抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同\n1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null\n2、判断当前这把锁是否是属于当前线程，如果是，则返回null\n所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，同学们可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁\n1 2 3 4 5 6 long threadId = Thread.currentThread().getId(); Long ttl = tryAcquire(-1, leaseTime, unit, threadId); // lock acquired if (ttl == null) { return; } 接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带带参数传入的值是-1，如果传入参数，则leaseTime是他本身，所以如果传入了参数，此时leaseTime != -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑\n1 2 3 if (leaseTime != -1) { return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } 如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout()\nttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 RFuture\u0026lt;Long\u0026gt; ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.onComplete((ttlRemaining, e) -\u0026gt; { if (e != null) { return; } // lock acquired if (ttlRemaining == null) { scheduleExpirationRenewal(threadId); } }); return ttlRemainingFuture; 此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法\nMethod( new TimerTask() {},参数2 ，参数3 )\n指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情\n因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约\n那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 private void renewExpiration() { ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ee == null) { return; } Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ent == null) { return; } Long threadId = ent.getFirstThreadId(); if (threadId == null) { return; } RFuture\u0026lt;Boolean\u0026gt; future = renewExpirationAsync(threadId); future.onComplete((res, e) -\u0026gt; { if (e != null) { log.error(\u0026#34;Can\u0026#39;t update lock \u0026#34; + getName() + \u0026#34; expiration\u0026#34;, e); return; } if (res) { // reschedule itself renewExpiration(); } }); } }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task); } 5.5 分布式锁-redission锁的MutiLock原理 为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例\n此时我们去写命令，写在主机上， 主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。\n为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。\n那么MutiLock 加锁原理是什么呢？笔者画了一幅图来说明\n当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环去不停去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功， 那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试.\n6、秒杀优化 6.1 秒杀优化-异步秒杀思路 我们来回顾一下下单流程\n当用户发起请求，此时会请求nginx，nginx会访问到tomcat，而tomcat中的程序，会进行串行操作，分成如下几个步骤\n1、查询优惠卷\n2、判断秒杀库存是否足够\n3、查询订单\n4、校验是否是一人一单\n5、扣减库存\n6、创建订单\n在这六步操作中，又有很多操作是要去操作数据库的，而且还是一个线程串行执行， 这样就会导致我们的程序执行的很慢，所以我们需要异步程序执行，那么如何加速呢？\n在这里笔者想给大家分享一下课程内没有的思路，看看有没有小伙伴这么想，比如，我们可以不可以使用异步编排来做，或者说我开启N多线程，N多个线程，一个线程执行查询优惠卷，一个执行判断扣减库存，一个去创建订单等等，然后再统一做返回，这种做法和课程中有哪种好呢？答案是课程中的好，因为如果你采用我刚说的方式，如果访问的人很多，那么线程池中的线程可能一下子就被消耗完了，而且你使用上述方案，最大的特点在于，你觉得时效性会非常重要，但是你想想是吗？并不是，比如我只要确定他能做这件事，然后我后边慢慢做就可以了，我并不需要他一口气做完这件事，所以我们应当采用的是课程中，类似消息队列的方式来完成我们的需求，而不是使用线程池或者是异步编排的方式来完成这个需求\n优化方案：我们将耗时比较短的逻辑判断放入到redis中，比如是否库存足够，比如是否一人一单，这样的操作，只要这种逻辑可以完成，就意味着我们是一定可以下单完成的，我们只需要进行快速的逻辑判断，根本就不用等下单逻辑走完，我们直接给用户返回成功， 再在后台开一个线程，后台线程慢慢的去执行queue里边的消息，这样程序不就超级快了吗？而且也不用担心线程池消耗殆尽的问题，因为这里我们的程序中并没有手动使用任何线程池，当然这里边有两个难点\n第一个难点是我们怎么在redis中去快速校验一人一单，还有库存判断\n第二个难点是由于我们校验和tomct下单是两个线程，那么我们如何知道到底哪个单他最后是否成功，或者是下单完成，为了完成这件事我们在redis操作完之后，我们会将一些信息返回给前端，同时也会把这些信息丢到异步queue中去，后续操作中，可以通过这个id来查询我们tomcat中的下单逻辑是否完成了。\n我们现在来看看整体思路：当用户下单之后，判断库存是否充足只需要导redis中去根据key找对应的value是否大于0即可，如果不充足，则直接结束，如果充足，继续在redis中判断用户是否可以下单，如果set集合中没有这条数据，说明他可以下单，如果set集合中没有这条记录，则将userId和优惠卷存入到redis中，并且返回0，整个过程需要保证是原子性的，我们可以使用lua来操作\n当以上判断逻辑走完之后，我们可以判断当前redis中返回的结果是否是0 ，如果是0，则表示可以下单，则将之前说的信息存入到到queue中去，然后返回，然后再来个线程异步的下单，前端可以通过返回的订单id来判断是否下单成功。\n6.2 秒杀优化-Redis完成秒杀资格判断 需求：\n新增秒杀优惠券的同时，将优惠券信息保存到Redis中\n基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功\n如果抢购成功，将优惠券id和用户id封装后存入阻塞队列\n开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能\nVoucherServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 //SECKILL_STOCK_KEY 这个变量定义在RedisConstans中 //private static final String SECKILL_STOCK_KEY =\u0026#34;seckill:stock:\u0026#34; stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } 完整lua表达式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 -- 1.参数列表 -- 1.1.优惠券id local voucherId = ARGV[1] -- 1.2.用户id local userId = ARGV[2] -- 1.3.订单id local orderId = ARGV[3] -- 2.数据key -- 2.1.库存key local stockKey = \u0026#39;seckill:stock:\u0026#39; .. voucherId -- 2.2.订单key local orderKey = \u0026#39;seckill:order:\u0026#39; .. voucherId -- 3.脚本业务 -- 3.1.判断库存是否充足 get stockKey if(tonumber(redis.call(\u0026#39;get\u0026#39;, stockKey)) \u0026lt;= 0) then -- 3.2.库存不足，返回1 return 1 end -- 3.2.判断用户是否下单 SISMEMBER orderKey userId if(redis.call(\u0026#39;sismember\u0026#39;, orderKey, userId) == 1) then -- 3.3.存在，说明是重复下单，返回2 return 2 end -- 3.4.扣库存 incrby stockKey -1 redis.call(\u0026#39;incrby\u0026#39;, stockKey, -1) -- 3.5.下单（保存用户）sadd orderKey userId redis.call(\u0026#39;sadd\u0026#39;, orderKey, userId) -- 3.6.发送消息到队列中， XADD stream.orders * k1 v1 k2 v2 ... redis.call(\u0026#39;xadd\u0026#39;, \u0026#39;stream.orders\u0026#39;, \u0026#39;*\u0026#39;, \u0026#39;userId\u0026#39;, userId, \u0026#39;voucherId\u0026#39;, voucherId, \u0026#39;id\u0026#39;, orderId) return 0 当以上lua表达式执行完毕后，剩下的就是根据步骤3,4来执行我们接下来的任务了\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result seckillVoucher(Long voucherId) { //获取用户 Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \u0026#34;库存不足\u0026#34; : \u0026#34;不能重复下单\u0026#34;); } //TODO 保存阻塞队列 // 3.返回订单id return Result.ok(orderId); } 6.3 秒杀优化-基于阻塞队列实现秒杀优化 VoucherOrderServiceImpl\n修改下单动作，现在我们去下单时，是通过lua表达式去原子执行判断逻辑，如果判断我出来不为0 ，则要么是库存不足，要么是重复下单，返回错误信息，如果是0，则把下单的逻辑保存到队列中去，然后异步执行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 //异步处理线程池 private static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor(); //在类初始化之后执行，因为当这个类初始化好了之后，随时都是有可能要执行的 @PostConstruct private void init() { SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } // 用于线程池处理的任务 // 当初始化完毕后，就会去从对列中去拿信息 private class VoucherOrderHandler implements Runnable{ @Override public void run() { while (true){ try { // 1.获取队列中的订单信息 VoucherOrder voucherOrder = orderTasks.take(); // 2.创建订单 handleVoucherOrder(voucherOrder); } catch (Exception e) { log.error(\u0026#34;处理订单异常\u0026#34;, e); } } } private void handleVoucherOrder(VoucherOrder voucherOrder) { //1.获取用户 Long userId = voucherOrder.getUserId(); // 2.创建锁对象 RLock redisLock = redissonClient.getLock(\u0026#34;lock:order:\u0026#34; + userId); // 3.尝试获取锁 boolean isLock = redisLock.lock(); // 4.判断是否获得锁成功 if (!isLock) { // 获取锁失败，直接返回失败或者重试 log.error(\u0026#34;不允许重复下单！\u0026#34;); return; } try { //注意：由于是spring的事务是放在threadLocal中，此时的是多线程，事务会失效 proxy.createVoucherOrder(voucherOrder); } finally { // 释放锁 redisLock.unlock(); } } //a private BlockingQueue\u0026lt;VoucherOrder\u0026gt; orderTasks =new ArrayBlockingQueue\u0026lt;\u0026gt;(1024 * 1024); @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \u0026#34;库存不足\u0026#34; : \u0026#34;不能重复下单\u0026#34;); } VoucherOrder voucherOrder = new VoucherOrder(); // 2.3.订单id long orderId = redisIdWorker.nextId(\u0026#34;order\u0026#34;); voucherOrder.setId(orderId); // 2.4.用户id voucherOrder.setUserId(userId); // 2.5.代金券id voucherOrder.setVoucherId(voucherId); // 2.6.放入阻塞队列 orderTasks.add(voucherOrder); //3.获取代理对象 proxy = (IVoucherOrderService)AopContext.currentProxy(); //4.返回订单id return Result.ok(orderId); } @Transactional public void createVoucherOrder(VoucherOrder voucherOrder) { Long userId = voucherOrder.getUserId(); // 5.1.查询订单 int count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;voucher_id\u0026#34;, voucherOrder.getVoucherId()).count(); // 5.2.判断是否存在 if (count \u0026gt; 0) { // 用户已经购买过了 log.error(\u0026#34;用户已经购买过了\u0026#34;); return ; } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) // set stock = stock - 1 .eq(\u0026#34;voucher_id\u0026#34;, voucherOrder.getVoucherId()).gt(\u0026#34;stock\u0026#34;, 0) // where id = ? and stock \u0026gt; 0 .update(); if (!success) { // 扣减失败 log.error(\u0026#34;库存不足\u0026#34;); return ; } save(voucherOrder); } 小总结：\n秒杀业务的优化思路是什么？\n先利用Redis完成库存余量、一人一单判断，完成抢单业务 再将下单业务放入阻塞队列，利用独立线程异步下单 基于阻塞队列的异步秒杀存在哪些问题？ 内存限制问题 数据安全问题 7、Redis消息队列 7.1 Redis消息队列-认识消息队列 什么是消息队列：字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色：\n消息队列：存储和管理消息，也被称为消息代理（Message Broker） 生产者：发送消息到消息队列 消费者：从消息队列获取消息并处理消息 使用队列的好处在于 **解耦：**所谓解耦，举一个生活中的例子就是：快递员(生产者)把快递放到快递柜里边(Message Queue)去，我们(消费者)从快递柜里边去拿东西，这就是一个异步，如果耦合，那么这个快递员相当于直接把快递交给你，这事固然好，但是万一你不在家，那么快递员就会一直等你，这就浪费了快递员的时间，所以这种思想在我们日常开发中，是非常有必要的。\n这种场景在我们秒杀中就变成了：我们下单之后，利用redis去进行校验下单条件，再通过队列把消息发送出去，然后再启动一个线程去消费这个消息，完成解耦，同时也加快我们的响应速度。\n这里我们可以使用一些现成的mq，比如kafka，rabbitmq等等，但是呢，如果没有安装mq，我们也可以直接使用redis提供的mq方案，降低我们的部署和学习成本。\n7.2 Redis消息队列-基于List实现消息队列 基于List结构模拟消息队列\n消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。\n队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。 不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用BRPOP或者BLPOP来实现阻塞效果。\n基于List的消息队列有哪些优缺点？ 优点：\n利用Redis存储，不受限于JVM内存上限 基于Redis的持久化机制，数据安全性有保证 可以满足消息有序性 缺点：\n无法避免消息丢失 只支持单消费者 7.3 Redis消息队列-基于PubSub的消息队列 PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。\nSUBSCRIBE channel [channel] ：订阅一个或多个频道 PUBLISH channel msg ：向一个频道发送消息 PSUBSCRIBE pattern[pattern] ：订阅与pattern格式匹配的所有频道\n基于PubSub的消息队列有哪些优缺点？ 优点：\n采用发布订阅模型，支持多生产、多消费 缺点：\n不支持数据持久化 无法避免消息丢失 消息堆积有上限，超出时数据丢失 7.4 Redis消息队列-基于Stream的消息队列 Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。\n发送消息的命令：\n例如：\n读取消息的方式之一：XREAD\n例如，使用XREAD读取第一个消息：\nXREAD阻塞方式，读取最新的消息：\n在业务开发中，我们可以循环的调用XREAD阻塞方式来查询最新消息，从而实现持续监听队列的效果，伪代码如下\n注意：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息到达队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题\nSTREAM类型消息队列的XREAD命令特点：\n消息可回溯 一个消息可以被多个消费者读取 可以阻塞读取 有消息漏读的风险 7.5 Redis消息队列-基于Stream的消息队列-消费者组 消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。具备下列特点：\n创建消费者组： key：队列名称 groupName：消费者组名称 ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息 MKSTREAM：队列不存在时自动创建队列 其它常见命令：\n删除指定的消费者组\n1 XGROUP DESTORY key groupName 给指定的消费者组添加消费者\n1 XGROUP CREATECONSUMER key groupname consumername 删除消费者组中的指定消费者\n1 XGROUP DELCONSUMER key groupname consumername 从消费者组读取消息：\n1 XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group：消费组名称 consumer：消费者名称，如果消费者不存在，会自动创建一个消费者 count：本次查询的最大数量 BLOCK milliseconds：当没有消息时最长等待时间 NOACK：无需手动ACK，获取到消息后自动确认 STREAMS key：指定队列名称 ID：获取消息的起始ID： \u0026ldquo;\u0026gt;\u0026quot;：从下一个未消费的消息开始 其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始\n消费者监听消息的基本思路：\nSTREAM类型消息队列的XREADGROUP命令特点：\n消息可回溯 可以多消费者争抢消息，加快消费速度 可以阻塞读取 没有消息漏读的风险 有消息确认机制，保证消息至少被消费一次 最后我们来个小对比\n7.6 基于Redis的Stream结构作为消息队列，实现异步秒杀下单 需求：\n创建一个Stream类型的消息队列，名为stream.orders 修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId 项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单\\ 修改lua表达式,新增3.6\nVoucherOrderServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 private class VoucherOrderHandler implements Runnable { @Override public void run() { while (true) { try { // 1.获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 \u0026gt; List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(\u0026#34;stream.orders\u0026#34;, ReadOffset.lastConsumed()) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有消息，继续下一次循环 continue; } // 解析数据 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;, \u0026#34;g1\u0026#34;, record.getId()); } catch (Exception e) { log.error(\u0026#34;处理订单异常\u0026#34;, e); //处理异常消息 handlePendingList(); } } } private void handlePendingList() { while (true) { try { // 1.获取pending-list中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 0 List\u0026lt;MapRecord\u0026lt;String, Object, Object\u0026gt;\u0026gt; list = stringRedisTemplate.opsForStream().read( Consumer.from(\u0026#34;g1\u0026#34;, \u0026#34;c1\u0026#34;), StreamReadOptions.empty().count(1), StreamOffset.create(\u0026#34;stream.orders\u0026#34;, ReadOffset.from(\u0026#34;0\u0026#34;)) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有异常消息，结束循环 break; } // 解析数据 MapRecord\u0026lt;String, Object, Object\u0026gt; record = list.get(0); Map\u0026lt;Object, Object\u0026gt; value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\u0026#34;s1\u0026#34;, \u0026#34;g1\u0026#34;, record.getId()); } catch (Exception e) { log.error(\u0026#34;处理pendding订单异常\u0026#34;, e); try{ Thread.sleep(20); }catch(Exception e){ e.printStackTrace(); } } } } } 8、达人探店 8.1、达人探店-发布探店笔记 发布探店笔记\n探店笔记类似点评网站的评价，往往是图文结合。对应的表有两个： tb_blog：探店笔记表，包含笔记中的标题、文字、图片等 tb_blog_comments：其他用户对探店笔记的评价\n具体发布流程\n上传接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @RestController @RequestMapping(\u0026#34;upload\u0026#34;) public class UploadController { @PostMapping(\u0026#34;blog\u0026#34;) public Result uploadImage(@RequestParam(\u0026#34;file\u0026#34;) MultipartFile image) { try { // 获取原始文件名称 String originalFilename = image.getOriginalFilename(); // 生成新文件名 String fileName = createNewFileName(originalFilename); // 保存文件 image.transferTo(new File(SystemConstants.IMAGE_UPLOAD_DIR, fileName)); // 返回结果 log.debug(\u0026#34;文件上传成功，{}\u0026#34;, fileName); return Result.ok(fileName); } catch (IOException e) { throw new RuntimeException(\u0026#34;文件上传失败\u0026#34;, e); } } } 注意：同学们在操作时，需要修改SystemConstants.IMAGE_UPLOAD_DIR 自己图片所在的地址，在实际开发中图片一般会放在nginx上或者是云存储上。\nBlogController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @RestController @RequestMapping(\u0026#34;/blog\u0026#34;) public class BlogController { @Resource private IBlogService blogService; @PostMapping public Result saveBlog(@RequestBody Blog blog) { //获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUpdateTime(user.getId()); //保存探店博文 blogService.saveBlog(blog); //返回id return Result.ok(blog.getId()); } } 8.2 达人探店-查看探店笔记 实现查看发布探店笔记的接口\n实现代码：\nBlogServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 @Override public Result queryBlogById(Long id) { // 1.查询blog Blog blog = getById(id); if (blog == null) { return Result.fail(\u0026#34;笔记不存在！\u0026#34;); } // 2.查询blog有关的用户 queryBlogUser(blog); return Result.ok(blog); } 8.3 达人探店-点赞功能 初始代码\n1 2 3 4 5 6 @GetMapping(\u0026#34;/likes/{id}\u0026#34;) public Result queryBlogLikes(@PathVariable(\u0026#34;id\u0026#34;) Long id) { //修改点赞数量 blogService.update().setSql(\u0026#34;liked = liked +1 \u0026#34;).eq(\u0026#34;id\u0026#34;,id).update(); return Result.ok(); } 问题分析：这种方式会导致一个用户无限点赞，明显是不合理的\n造成这个问题的原因是，我们现在的逻辑，发起请求只是给数据库+1，所以才会出现这个问题\n完善点赞功能\n需求：\n同一个用户只能点赞一次，再次点击则取消点赞 如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性） 实现步骤：\n给Blog类中添加一个isLike字段，标示是否被当前用户点赞 修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1 修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段 修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段 为什么采用set集合：\n因为我们的数据是不能重复的，当用户操作过之后，无论他怎么操作，都是\n具体步骤：\n1、在Blog 添加一个字段\n1 2 @TableField(exist = false) private Boolean isLike; 2、修改代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Override public Result likeBlog(Long id){ // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Boolean isMember = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if(BooleanUtil.isFalse(isMember)){ //3.如果未点赞，可以点赞 //3.1 数据库点赞数+1 boolean isSuccess = update().setSql(\u0026#34;liked = liked + 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); //3.2 保存用户到Redis的set集合 if(isSuccess){ stringRedisTemplate.opsForSet().add(key,userId.toString()); } }else{ //4.如果已点赞，取消点赞 //4.1 数据库点赞数-1 boolean isSuccess = update().setSql(\u0026#34;liked = liked - 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); //4.2 把用户从Redis的set集合移除 if(isSuccess){ stringRedisTemplate.opsForSet().remove(key,userId.toString()); } } 8.4 达人探店-点赞排行榜 在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜：\n之前的点赞是放到set集合，但是set集合是不能排序的，所以这个时候，咱们可以采用一个可以排序的set集合，就是咱们的sortedSet\n我们接下来来对比一下这些集合的区别是什么\n所有点赞的人，需要是唯一的，所以我们应当使用set或者是sortedSet\n其次我们需要排序，就可以直接锁定使用sortedSet啦\n修改代码\nBlogServiceImpl\n点赞逻辑代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override public Result likeBlog(Long id) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); if (score == null) { // 3.如果未点赞，可以点赞 // 3.1.数据库点赞数 + 1 boolean isSuccess = update().setSql(\u0026#34;liked = liked + 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); // 3.2.保存用户到Redis的set集合 zadd key value score if (isSuccess) { stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis()); } } else { // 4.如果已点赞，取消点赞 // 4.1.数据库点赞数 -1 boolean isSuccess = update().setSql(\u0026#34;liked = liked - 1\u0026#34;).eq(\u0026#34;id\u0026#34;, id).update(); // 4.2.把用户从Redis的set集合移除 if (isSuccess) { stringRedisTemplate.opsForZSet().remove(key, userId.toString()); } } return Result.ok(); } private void isBlogLiked(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); if (user == null) { // 用户未登录，无需查询是否点赞 return; } Long userId = user.getId(); // 2.判断当前登录用户是否已经点赞 String key = \u0026#34;blog:liked:\u0026#34; + blog.getId(); Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); blog.setIsLike(score != null); } 点赞列表查询列表\nBlogController\n1 2 3 4 5 @GetMapping(\u0026#34;/likes/{id}\u0026#34;) public Result queryBlogLikes(@PathVariable(\u0026#34;id\u0026#34;) Long id) { return blogService.queryBlogLikes(id); } BlogService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Override public Result queryBlogLikes(Long id) { String key = BLOG_LIKED_KEY + id; // 1.查询top5的点赞用户 zrange key 0 4 Set\u0026lt;String\u0026gt; top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); if (top5 == null || top5.isEmpty()) { return Result.ok(Collections.emptyList()); } // 2.解析出其中的用户id List\u0026lt;Long\u0026gt; ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); // 3.根据用户id查询用户 WHERE id IN ( 5 , 1 ) ORDER BY FIELD(id, 5, 1) List\u0026lt;UserDTO\u0026gt; userDTOS = userService.query() .in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list() .stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); // 4.返回 return Result.ok(userDTOS); } 9、好友关注 9.1 好友关注-关注和取消关注 针对用户的操作：可以对用户进行关注和取消关注功能。\n实现思路：\n需求：基于该表数据结构，实现两个接口：\n关注和取关接口 判断是否关注的接口 关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示：\n注意: 这里需要把主键修改为自增长，简化开发。\nFollowController\n1 2 3 4 5 6 7 8 9 10 //关注 @PutMapping(\u0026#34;/{id}/{isFollow}\u0026#34;) public Result follow(@PathVariable(\u0026#34;id\u0026#34;) Long followUserId, @PathVariable(\u0026#34;isFollow\u0026#34;) Boolean isFollow) { return followService.follow(followUserId, isFollow); } //取消关注 @GetMapping(\u0026#34;/or/not/{id}\u0026#34;) public Result isFollow(@PathVariable(\u0026#34;id\u0026#34;) Long followUserId) { return followService.isFollow(followUserId); } FollowService\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 取消关注service @Override public Result isFollow(Long followUserId) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.查询是否关注 select count(*) from tb_follow where user_id = ? and follow_user_id = ? Integer count = query().eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId).count(); // 3.判断 return Result.ok(count \u0026gt; 0); } 关注service @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? remove(new QueryWrapper\u0026lt;Follow\u0026gt;() .eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId)); } return Result.ok(); } 9.2 好友关注-共同关注 想要去看共同关注的好友，需要首先进入到这个页面，这个页面会发起两个请求\n1、去查询用户的详情\n2、去查询用户的笔记\n以上两个功能和共同关注没有什么关系，大家可以自行将笔记中的代码拷贝到idea中就可以实现这两个功能了，我们的重点在于共同关注功能。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 // UserController 根据id查询用户 @GetMapping(\u0026#34;/{id}\u0026#34;) public Result queryUserById(@PathVariable(\u0026#34;id\u0026#34;) Long userId){ // 查询详情 User user = userService.getById(userId); if (user == null) { return Result.ok(); } UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); // 返回 return Result.ok(userDTO); } // BlogController 根据id查询博主的探店笔记 @GetMapping(\u0026#34;/of/user\u0026#34;) public Result queryBlogByUserId( @RequestParam(value = \u0026#34;current\u0026#34;, defaultValue = \u0026#34;1\u0026#34;) Integer current, @RequestParam(\u0026#34;id\u0026#34;) Long id) { // 根据用户查询 Page\u0026lt;Blog\u0026gt; page = blogService.query() .eq(\u0026#34;user_id\u0026#34;, id).page(new Page\u0026lt;\u0026gt;(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List\u0026lt;Blog\u0026gt; records = page.getRecords(); return Result.ok(records); } 接下来我们来看看共同关注如何实现：\n需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同关注呢。\n当然是使用我们之前学习过的set集合咯，在set集合中，有交集并集补集的api，我们可以把两人的关注的人分别放入到一个set集合中，然后再通过api去查看这两个set集合中的交集数据。\n我们先来改造当前的关注列表\n改造原因是因为我们需要在用户关注了某位用户后，需要将数据放入到set集合中，方便后续进行共同关注，同时当取消关注时，也需要从set集合中进行删除\nFollowServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); if (isSuccess) { // 把关注用户的id，放入redis的set集合 sadd userId followerUserId stringRedisTemplate.opsForSet().add(key, followUserId.toString()); } } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? boolean isSuccess = remove(new QueryWrapper\u0026lt;Follow\u0026gt;() .eq(\u0026#34;user_id\u0026#34;, userId).eq(\u0026#34;follow_user_id\u0026#34;, followUserId)); if (isSuccess) { // 把关注用户的id从Redis集合中移除 stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); } } return Result.ok(); } 具体的关注代码：\nFollowServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Override public Result followCommons(Long id) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); String key = \u0026#34;follows:\u0026#34; + userId; // 2.求交集 String key2 = \u0026#34;follows:\u0026#34; + id; Set\u0026lt;String\u0026gt; intersect = stringRedisTemplate.opsForSet().intersect(key, key2); if (intersect == null || intersect.isEmpty()) { // 无交集 return Result.ok(Collections.emptyList()); } // 3.解析id集合 List\u0026lt;Long\u0026gt; ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); // 4.查询用户 List\u0026lt;UserDTO\u0026gt; users = userService.listByIds(ids) .stream() .map(user -\u0026gt; BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(users); } 9.3 好友关注-Feed流实现方案 当我们关注了用户后，这个用户发了动态，那么我们应该把这些数据推送给用户，这个需求，其实我们又把他叫做Feed流，关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。\n对于传统的模式的内容解锁：我们是需要用户去通过搜索引擎或者是其他的方式去解锁想要看的内容\n对于新型的Feed流的的效果：不需要我们用户再去推送信息，而是系统分析用户到底想要什么，然后直接把内容推送给用户，从而使用户能够更加的节约时间，不用主动去寻找。\nFeed流的实现有两种模式：\nFeed流产品有两种常见模式： Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈\n优点：信息全面，不会有缺失。并且实现也相对简单 缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低 智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户\n优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷 缺点：如果算法不精准，可能起到反作用 本例中的个人页面，是基于关注的好友来做Feed流，因此采用Timeline的模式。该模式的实现方案有三种： 我们本次针对好友的操作，采用的就是Timeline的方式，只需要拿到我们关注用户的信息，然后按照时间排序即可\n，因此采用Timeline的模式。该模式的实现方案有三种：\n拉模式 推模式 推拉结合 拉模式：也叫做读扩散\n该模式的核心含义就是：当张三和李四和王五发了消息后，都会保存在自己的邮箱中，假设赵六要读取信息，那么他会从读取他自己的收件箱，此时系统会从他关注的人群中，把他关注人的信息全部都进行拉取，然后在进行排序\n优点：比较节约空间，因为赵六在读信息时，并没有重复读取，而且读取完之后可以把他的收件箱进行清楚。\n缺点：比较延迟，当用户读取数据时才去关注的人里边去读取数据，假设用户关注了大量的用户，那么此时就会拉取海量的内容，对服务器压力巨大。\n推模式：也叫做写扩散。\n推模式是没有写邮箱的，当张三写了一个内容，此时会主动的把张三写的内容发送到他的粉丝收件箱中去，假设此时李四再来读取，就不用再去临时拉取了\n优点：时效快，不用临时拉取\n缺点：内存压力大，假设一个大V写信息，很多人关注他， 就会写很多分数据到粉丝那边去\n推拉结合模式：也叫做读写混合，兼具推和拉两种模式的优点。\n推拉模式是一个折中的方案，站在发件人这一段，如果是个普通的人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝中去，因为普通的人他的粉丝关注量比较小，所以这样做没有压力，如果是大V，那么他是直接将数据先写入到一份到发件箱里边去，然后再直接写一份到活跃粉丝收件箱里边去，现在站在收件人这端来看，如果是活跃粉丝，那么大V和普通的人发的都会直接写入到自己收件箱里边来，而如果是普通的粉丝，由于他们上线不是很频繁，所以等他们上线时，再从发件箱里边去拉信息。\n9.4 好友关注-推送到粉丝收件箱 需求：\n修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱 收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现 查询收件箱数据时，可以实现分页查询 Feed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。\n传统了分页在feed流是不适用的，因为我们的数据会随时发生变化\n假设在t1 时刻，我们去读取第一页，此时page = 1 ，size = 5 ，那么我们拿到的就是106 这几条记录，假设现在t2时候又发布了一条记录，此时t3 时刻，我们来读取第二页，读取第二页传入的参数是page=2 ，size=5 ，那么此时读取到的第二页实际上是从6 开始，然后是62 ，那么我们就读取到了重复的数据，所以feed流的分页，不能采用原始方案来做。\nFeed流的滚动分页\n我们需要记录每次操作的最后一条，然后从这个位置开始去读取数据\n举个例子：我们从t1时刻开始，拿第一页数据，拿到了10~6，然后记录下当前最后一次拿取的记录，就是6，t2时刻发布了新的记录，此时这个11放到最顶上，但是不会影响我们之前记录的6，此时t3时刻来拿第二页，第二页这个时候拿数据，还是从6后一点的5去拿，就拿到了5-1的记录。我们这个地方可以采用sortedSet来做，可以进行范围查询，并且还可以记录当前获取数据时间戳最小值，就可以实现滚动分页了\n核心的意思：就是我们在保存完探店笔记后，获得到当前笔记的粉丝，然后把数据推送到粉丝的redis中去。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Override public Result saveBlog(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 2.保存探店笔记 boolean isSuccess = save(blog); if(!isSuccess){ return Result.fail(\u0026#34;新增笔记失败!\u0026#34;); } // 3.查询笔记作者的所有粉丝 select * from tb_follow where follow_user_id = ? List\u0026lt;Follow\u0026gt; follows = followService.query().eq(\u0026#34;follow_user_id\u0026#34;, user.getId()).list(); // 4.推送笔记id给所有粉丝 for (Follow follow : follows) { // 4.1.获取粉丝id Long userId = follow.getUserId(); // 4.2.推送 String key = FEED_KEY + userId; stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); } // 5.返回id return Result.ok(blog.getId()); } 9.5好友关注-实现分页查询收邮箱 需求：在个人主页的“关注”卡片中，查询并展示推送的Blog信息：\n具体操作如下：\n1、每次查询完成后，我们要分析出查询出数据的最小时间戳，这个值会作为下一次查询的条件\n2、我们需要找到与上一次查询相同的查询个数作为偏移量，下次查询时，跳过这些查询过的数据，拿到我们需要的数据\n综上：我们的请求参数中就需要携带 lastId：上一次查询的最小时间戳 和偏移量这两个参数。\n这两个参数第一次会由前端来指定，以后的查询就根据后台结果作为条件，再次传递到后台。\n一、定义出来具体的返回值实体类\n1 2 3 4 5 6 @Data public class ScrollResult { private List\u0026lt;?\u0026gt; list; private Long minTime; private Integer offset; } BlogController\n注意：RequestParam 表示接受url地址栏传参的注解，当方法上参数的名称和url地址栏不相同时，可以通过RequestParam 来进行指定\n1 2 3 4 5 @GetMapping(\u0026#34;/of/follow\u0026#34;) public Result queryBlogOfFollow( @RequestParam(\u0026#34;lastId\u0026#34;) Long max, @RequestParam(value = \u0026#34;offset\u0026#34;, defaultValue = \u0026#34;0\u0026#34;) Integer offset){ return blogService.queryBlogOfFollow(max, offset); } BlogServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @Override public Result queryBlogOfFollow(Long max, Integer offset) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); // 2.查询收件箱 ZREVRANGEBYSCORE key Max Min LIMIT offset count String key = FEED_KEY + userId; Set\u0026lt;ZSetOperations.TypedTuple\u0026lt;String\u0026gt;\u0026gt; typedTuples = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, max, offset, 2); // 3.非空判断 if (typedTuples == null || typedTuples.isEmpty()) { return Result.ok(); } // 4.解析数据：blogId、minTime（时间戳）、offset List\u0026lt;Long\u0026gt; ids = new ArrayList\u0026lt;\u0026gt;(typedTuples.size()); long minTime = 0; // 2 int os = 1; // 2 for (ZSetOperations.TypedTuple\u0026lt;String\u0026gt; tuple : typedTuples) { // 5 4 4 2 2 // 4.1.获取id ids.add(Long.valueOf(tuple.getValue())); // 4.2.获取分数(时间戳） long time = tuple.getScore().longValue(); if(time == minTime){ os++; }else{ minTime = time; os = 1; } } os = minTime == max ? os : os + offset; // 5.根据id查询blog String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); List\u0026lt;Blog\u0026gt; blogs = query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list(); for (Blog blog : blogs) { // 5.1.查询blog有关的用户 queryBlogUser(blog); // 5.2.查询blog是否被点赞 isBlogLiked(blog); } // 6.封装并返回 ScrollResult r = new ScrollResult(); r.setList(blogs); r.setOffset(os); r.setMinTime(minTime); return Result.ok(r); } 10、附近商户 10.1、附近商户-GEO数据结构的基本用法 GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有：\nGEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） GEODIST：计算指定的两个点之间的距离并返回 GEOHASH：将指定member的坐标转为hash字符串形式并返回 GEOPOS：返回指定member的坐标 GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃 GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能 GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能 10.2、 附近商户-导入店铺数据到GEO 具体场景说明：\n当我们点击美食之后，会出现一系列的商家，商家中可以按照多种排序方式，我们此时关注的是距离，这个地方就需要使用到我们的GEO，向后台传入当前app收集的地址(我们此处是写死的) ，以当前坐标作为圆心，同时绑定相同的店家类型type，以及分页信息，把这几个条件传入后台，后台查询出对应的数据再返回。\n我们要做的事情是：将数据库表中的数据导入到redis中去，redis中的GEO，GEO在redis中就一个menber和一个经纬度，我们把x和y轴传入到redis做的经纬度位置去，但我们不能把所有的数据都放入到menber中去，毕竟作为redis是一个内存级数据库，如果存海量数据，redis还是力不从心，所以我们在这个地方存储他的id即可。\n但是这个时候还有一个问题，就是在redis中并没有存储type，所以我们无法根据type来对数据进行筛选，所以我们可以按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可\n代码\nHmDianPingApplicationTests\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @Test void loadShopData() { // 1.查询店铺信息 List\u0026lt;Shop\u0026gt; list = shopService.list(); // 2.把店铺分组，按照typeId分组，typeId一致的放到一个集合 Map\u0026lt;Long, List\u0026lt;Shop\u0026gt;\u0026gt; map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); // 3.分批完成写入Redis for (Map.Entry\u0026lt;Long, List\u0026lt;Shop\u0026gt;\u0026gt; entry : map.entrySet()) { // 3.1.获取类型id Long typeId = entry.getKey(); String key = SHOP_GEO_KEY + typeId; // 3.2.获取同类型的店铺的集合 List\u0026lt;Shop\u0026gt; value = entry.getValue(); List\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt; locations = new ArrayList\u0026lt;\u0026gt;(value.size()); // 3.3.写入redis GEOADD key 经度 纬度 member for (Shop shop : value) { // stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString()); locations.add(new RedisGeoCommands.GeoLocation\u0026lt;\u0026gt;( shop.getId().toString(), new Point(shop.getX(), shop.getY()) )); } stringRedisTemplate.opsForGeo().add(key, locations); } } 10.3 附近商户-实现附近商户功能 SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM\n第一步：导入pom\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;spring-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.data\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;lettuce-core\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;io.lettuce\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.data\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.lettuce\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lettuce-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.1.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 第二步：\nShopController\n1 2 3 4 5 6 7 8 9 @GetMapping(\u0026#34;/of/type\u0026#34;) public Result queryShopByType( @RequestParam(\u0026#34;typeId\u0026#34;) Integer typeId, @RequestParam(value = \u0026#34;current\u0026#34;, defaultValue = \u0026#34;1\u0026#34;) Integer current, @RequestParam(value = \u0026#34;x\u0026#34;, required = false) Double x, @RequestParam(value = \u0026#34;y\u0026#34;, required = false) Double y ) { return shopService.queryShopByType(typeId, current, x, y); } ShopServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @Override public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) { // 1.判断是否需要根据坐标查询 if (x == null || y == null) { // 不需要坐标查询，按数据库查询 Page\u0026lt;Shop\u0026gt; page = query() .eq(\u0026#34;type_id\u0026#34;, typeId) .page(new Page\u0026lt;\u0026gt;(current, SystemConstants.DEFAULT_PAGE_SIZE)); // 返回数据 return Result.ok(page.getRecords()); } // 2.计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.DEFAULT_PAGE_SIZE; // 3.查询redis、按照距离排序、分页。结果：shopId、distance String key = SHOP_GEO_KEY + typeId; GeoResults\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt; results = stringRedisTemplate.opsForGeo() // GEOSEARCH key BYLONLAT x y BYRADIUS 10 WITHDISTANCE .search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); // 4.解析出id if (results == null) { return Result.ok(Collections.emptyList()); } List\u0026lt;GeoResult\u0026lt;RedisGeoCommands.GeoLocation\u0026lt;String\u0026gt;\u0026gt;\u0026gt; list = results.getContent(); if (list.size() \u0026lt;= from) { // 没有下一页了，结束 return Result.ok(Collections.emptyList()); } // 4.1.截取 from ~ end的部分 List\u0026lt;Long\u0026gt; ids = new ArrayList\u0026lt;\u0026gt;(list.size()); Map\u0026lt;String, Distance\u0026gt; distanceMap = new HashMap\u0026lt;\u0026gt;(list.size()); list.stream().skip(from).forEach(result -\u0026gt; { // 4.2.获取店铺id String shopIdStr = result.getContent().getName(); ids.add(Long.valueOf(shopIdStr)); // 4.3.获取距离 Distance distance = result.getDistance(); distanceMap.put(shopIdStr, distance); }); // 5.根据id查询Shop String idStr = StrUtil.join(\u0026#34;,\u0026#34;, ids); List\u0026lt;Shop\u0026gt; shops = query().in(\u0026#34;id\u0026#34;, ids).last(\u0026#34;ORDER BY FIELD(id,\u0026#34; + idStr + \u0026#34;)\u0026#34;).list(); for (Shop shop : shops) { shop.setDistance(distanceMap.get(shop.getId().toString()).getValue()); } // 6.返回 return Result.ok(shops); } 11、用户签到 11.1、用户签到-BitMap功能演示 我们针对签到功能完全可以通过mysql来完成，比如说以下这张表\n用户一次签到，就是一条记录，假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条\n每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节\n我们如何能够简化一点呢？其实可以考虑小时候一个挺常见的方案，就是小时候，咱们准备一张小小的卡片，你只要签到就打上一个勾，我最后判断你是否签到，其实只需要到小卡片上看一看就知道了\n我们可以采用类似这样的方案来实现我们的签到需求。\n我们按月来统计用户签到信息，签到记录为1，未签到则记录为0.\n把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）。这样我们就用极小的空间，来实现了大量数据的表示\nRedis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。\nBitMap的操作命令有：\nSETBIT：向指定位置（offset）存入一个0或1 GETBIT ：获取指定位置（offset）的bit值 BITCOUNT ：统计BitMap中值为1的bit位的数量 BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值 BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回 BITOP ：将多个BitMap的结果做位运算（与 、或、异或） BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置 11.2 、用户签到-实现签到功能 需求：实现签到接口，将当前用户当天签到信息保存到Redis中\n思路：我们可以把年和月作为bitMap的key，然后保存到一个bitMap中，每次签到就到对应的位上把数字从0变成1，只要对应是1，就表明说明这一天已经签到了，反之则没有签到。\n我们通过接口文档发现，此接口并没有传递任何的参数，没有参数怎么确实是哪一天签到呢？这个很容易，可以通过后台代码直接获取即可，然后到对应的地址上去修改bitMap。\n代码\nUserController\n1 2 3 4 @PostMapping(\u0026#34;/sign\u0026#34;) public Result sign(){ return userService.sign(); } UserServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Override public Result sign() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\u0026#34;:yyyyMM\u0026#34;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.写入Redis SETBIT key offset 1 stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true); return Result.ok(); } 11.3 用户签到-签到统计 **问题1：**什么叫做连续签到天数？ 从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。\nJava逻辑代码：获得当前这个月的最后一次签到数据，定义一个计数器，然后不停的向前统计，直到获得第一个非0的数字即可，每得到一个非0的数字计数器+1，直到遍历完所有的数据，就可以获得当前月的签到总天数了\n**问题2：**如何得到本月到今天为止的所有签到数据？\nBITFIELD key GET u[dayOfMonth] 0\n假设今天是10号，那么我们就可以从当前月的第一天开始，获得到当前这一天的位数，是10号，那么就是10位，去拿这段时间的数据，就能拿到所有的数据了，那么这10天里边签到了多少次呢？统计有多少个1即可。\n问题3：如何从后向前遍历每个bit位？\n注意：bitMap返回的数据是10进制，哪假如说返回一个数字8，那么我哪儿知道到底哪些是0，哪些是1呢？我们只需要让得到的10进制数字和1做与运算就可以了，因为1只有遇见1 才是1，其他数字都是0 ，我们把签到结果和1进行与操作，每与一次，就把签到结果向右移动一位，依次内推，我们就能完成逐个遍历的效果了。\n需求：实现下面接口，统计当前用户截止当前时间在本月的连续签到天数\n有用户有时间我们就可以组织出对应的key，此时就能找到这个用户截止这天的所有签到记录，再根据这套算法，就能统计出来他连续签到的次数了\n代码\nUserController\n1 2 3 4 @GetMapping(\u0026#34;/sign/count\u0026#34;) public Result signCount(){ return userService.signCount(); } UserServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Override public Result signCount() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\u0026#34;:yyyyMM\u0026#34;)); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.获取本月截止今天为止的所有的签到记录，返回的是一个十进制的数字 BITFIELD sign:5:202203 GET u14 0 List\u0026lt;Long\u0026gt; result = stringRedisTemplate.opsForValue().bitField( key, BitFieldSubCommands.create() .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0) ); if (result == null || result.isEmpty()) { // 没有任何签到结果 return Result.ok(0); } Long num = result.get(0); if (num == null || num == 0) { return Result.ok(0); } // 6.循环遍历 int count = 0; while (true) { // 6.1.让这个数字与1做与运算，得到数字的最后一个bit位 // 判断这个bit位是否为0 if ((num \u0026amp; 1) == 0) { // 如果为0，说明未签到，结束 break; }else { // 如果不为0，说明已签到，计数器+1 count++; } // 把数字右移一位，抛弃最后一个bit位，继续下一个bit位 num \u0026gt;\u0026gt;\u0026gt;= 1; } return Result.ok(count); } 11.4 额外加餐-关于使用bitmap来解决缓存穿透的方案 回顾缓存穿透：\n发起了一个数据库不存在的，redis里边也不存在的数据，通常你可以把他看成一个攻击\n解决方案：\n判断id\u0026lt;0\n如果数据库是空，那么就可以直接往redis里边把这个空数据缓存起来\n第一种解决方案：遇到的问题是如果用户访问的是id不存在的数据，则此时就无法生效\n第二种解决方案：遇到的问题是：如果是不同的id那就可以防止下次过来直击数据\n所以我们如何解决呢？\n我们可以将数据库的数据，所对应的id写入到一个list集合中，当用户过来访问的时候，我们直接去判断list中是否包含当前的要查询的数据，如果说用户要查询的id数据并不在list集合中，则直接返回，如果list中包含对应查询的id数据，则说明不是一次缓存穿透数据，则直接放行。\n现在的问题是这个主键其实并没有那么短，而是很长的一个 主键\n哪怕你单独去提取这个主键，但是在11年左右，淘宝的商品总量就已经超过10亿个\n所以如果采用以上方案，这个list也会很大，所以我们可以使用bitmap来减少list的存储空间\n我们可以把list数据抽象成一个非常大的bitmap，我们不再使用list，而是将db中的id数据利用哈希思想，比如：\nid % bitmap.size = 算出当前这个id对应应该落在bitmap的哪个索引上，然后将这个值从0变成1，然后当用户来查询数据时，此时已经没有了list，让用户用他查询的id去用相同的哈希算法， 算出来当前这个id应当落在bitmap的哪一位，然后判断这一位是0，还是1，如果是0则表明这一位上的数据一定不存在， 采用这种方式来处理，需要重点考虑一个事情，就是误差率，所谓的误差率就是指当发生哈希冲突的时候，产生的误差。\n12、UV统计 12.1 、UV统计-HyperLogLog 首先我们搞懂两个概念：\nUV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。 通常来说UV会比PV大很多，所以衡量同一个网站的访问量，我们需要综合考虑很多因素，所以我们只是单纯的把这两个值作为一个参考值\nUV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖，那怎么处理呢？\nHyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0 Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。\n12.2 UV统计-测试百万数据的统计 测试思路：我们直接利用单元测试，向HyperLogLog中添加100万条数据，看看内存占用和统计效果如何\n经过测试：我们会发生他的误差是在允许范围内，并且内存占用极小\n","date":"2025-07-19T00:00:00Z","image":"https://nova-bryan.github.io/p/redis%E5%AE%9E%E6%88%98/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/redis%E5%AE%9E%E6%88%98/","title":"Redis实战"},{"content":"1.题目 接雨水题解 这题真他妈经典，老子搞了半小时才彻底搞懂。记录一下思路，免得下次又忘。\n核心思路 每个位置能接的雨水量 = min(左边最高的柱子, 右边最高的柱子) - 当前柱子高度\n简单来说就是：\n找当前柱子左边最高的（不包括自己） 找当前柱子右边最高的（不包括自己） 取这俩的较小值 如果这个值比当前柱子高，差值就是能接的雨水 具体实现 第一步：预处理左右最大值 先搞两个数组：\nleftMax[i]：表示第i个柱子左边最高的柱子高度（不包含自己） rightMax[i]：表示第i个柱子右边最高的柱子高度（不包含自己） 计算leftMax：\n从左往右扫 维护一个当前最大值max 对于每个位置i，leftMax[i] = max（记录之前遇到的最大值） 然后更新max = Math.max(max, height[i]) 1 2 3 4 5 int max = 0; for (int i = 0; i \u0026lt; n; i++) { leftMax[i] = max; max = Math.max(max, height[i]); } 计算rightMax：\n从右往左扫 同样维护一个当前最大值max 对于每个位置i，rightMax[i] = max 然后更新max = Math.max(max, height[i]) 1 2 3 4 5 max = 0; for (int i = n - 1; i \u0026gt;= 0; i--) { rightMax[i] = max; max = Math.max(max, height[i]); } 第二步：计算雨水 现在有了leftMax和rightMax，对于每个位置：\n取leftMax[i]和rightMax[i]的较小值 如果这个值 \u0026gt; height[i]，说明能接雨水 雨水 = 较小值 - height[i] 1 2 3 4 5 6 for (int i = 0; i \u0026lt; n; i++) { int min = Math.min(leftMax[i], rightMax[i]); if (min \u0026gt; height[i]) { ans += (min - height[i]); } } 例子分析 以[0,1,0,2,1,0,1,3,2,1,2,1]为例：\n计算leftMax：\n从左往右，记录遇到的最大值 得到：[0,0,1,1,2,2,2,2,3,3,3,3] 计算rightMax：\n从右往左，记录遇到的最大值 得到：[3,3,3,3,3,3,3,2,2,2,1,0] 计算雨水：\n比如i=2，height=0 leftMax=1, rightMax=3 min=1 1 \u0026gt; 0，所以接1单位水 复杂度 时间：O(n)，扫了3遍数组 空间：O(n)，用了两个辅助数组 优化思路 其实可以用双指针把空间降到O(1)，不过这个解法已经够直观了，下次再研究优化的。\n代码总结 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 public int trap(int[] height) { int n = height.length; int ans = 0; // 左边最大值 int[] leftMax = new int[n]; int max = 0; for (int i = 0; i \u0026lt; n; i++) { leftMax[i] = max; max = Math.max(max, height[i]); } // 右边最大值 int[] rightMax = new int[n]; max = 0; for (int i = n - 1; i \u0026gt;= 0; i--) { rightMax[i] = max; max = Math.max(max, height[i]); } // 计算雨水 for (int i = 0; i \u0026lt; n; i++) { int min = Math.min(leftMax[i], rightMax[i]); if (min \u0026gt; height[i]) { ans += (min - height[i]); } } return ans; } 这题真他妈经典，记住这个思路就完事了！\n","date":"2025-07-03T00:00:00Z","image":"https://nova-bryan.github.io/p/leetcode42.%E6%8E%A5%E9%9B%A8%E6%B0%B4/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/leetcode42.%E6%8E%A5%E9%9B%A8%E6%B0%B4/","title":"LeetCode42.接雨水"},{"content":"MapStruct完全学习指南：从零到一掌握Java对象映射 📚 目录 MapStruct是什么？ 为什么要使用MapStruct？ 环境搭建与配置 第一个MapStruct示例 基本映射用法 高级映射技巧 集合与复杂对象映射 最佳实践与性能优化 实战案例 常见问题与解决方案 MapStruct是什么？ MapStruct 是一个基于注解处理器的Java代码生成工具，专门用于简化Java Bean之间的映射转换。它在编译时生成类型安全且高性能的映射代码，大大减少了手工编写模板代码的工作量。\n核心特点 🚀 编译时代码生成：无运行时反射，性能优异 🔒 类型安全：编译时检查，避免运行时错误 📝 简洁注解：通过简单注解即可完成复杂映射 🔄 双向映射：支持自动生成反向映射 🎯 Spring集成：完美支持依赖注入框架 为什么要使用MapStruct？ 传统映射的痛点 在日常开发中，我们经常需要在不同的对象之间进行数据转换：\n1 2 3 4 5 6 7 8 9 10 11 // 传统手工映射 - 繁琐且容易出错 public UserDTO convertToDTO(User user) { UserDTO dto = new UserDTO(); dto.setId(user.getId()); dto.setFirstName(user.getFirstName()); dto.setLastName(user.getLastName()); dto.setEmail(user.getEmail()); dto.setPhoneNumber(user.getPhone()); // ... 更多字段 return dto; } 问题显而易见：\n🔴 代码重复，维护困难 🔴 容易遗漏字段或写错字段名 🔴 当对象结构变化时，需要手动更新所有映射代码 🔴 性能问题（如果使用反射） MapStruct的解决方案 1 2 3 4 5 6 7 8 // MapStruct方式 - 简洁优雅 @Mapper public interface UserMapper { @Mapping(source = \u0026#34;phone\u0026#34;, target = \u0026#34;phoneNumber\u0026#34;) UserDTO toDTO(User user); User toEntity(UserDTO dto); } 优势立即显现：\n✅ 代码简洁，一目了然 ✅ 自动字段匹配，减少错误 ✅ 编译时验证，及早发现问题 ✅ 高性能，无反射开销 环境搭建与配置 Maven配置 在pom.xml中添加依赖：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 \u0026lt;properties\u0026gt; \u0026lt;mapstruct.version\u0026gt;1.5.5.Final\u0026lt;/mapstruct.version\u0026gt; \u0026lt;lombok.version\u0026gt;1.18.30\u0026lt;/lombok.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- MapStruct核心依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Lombok支持（可选但推荐） --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.11.0\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;11\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;11\u0026lt;/target\u0026gt; \u0026lt;annotationProcessorPaths\u0026gt; \u0026lt;!-- MapStruct注解处理器 --\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct-processor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;!-- Lombok注解处理器 --\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;!-- Lombok与MapStruct绑定 --\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok-mapstruct-binding\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.2.0\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;/annotationProcessorPaths\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; Gradle配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 plugins { id \u0026#39;java\u0026#39; id \u0026#39;org.springframework.boot\u0026#39; version \u0026#39;3.1.0\u0026#39; } ext { mapstructVersion = \u0026#39;1.5.5.Final\u0026#39; lombokVersion = \u0026#39;1.18.30\u0026#39; } dependencies { implementation \u0026#34;org.mapstruct:mapstruct:${mapstructVersion}\u0026#34; implementation \u0026#34;org.projectlombok:lombok:${lombokVersion}\u0026#34; annotationProcessor \u0026#34;org.mapstruct:mapstruct-processor:${mapstructVersion}\u0026#34; annotationProcessor \u0026#34;org.projectlombok:lombok:${lombokVersion}\u0026#34; annotationProcessor \u0026#34;org.projectlombok:lombok-mapstruct-binding:0.2.0\u0026#34; } 第一个MapStruct示例 让我们从最简单的例子开始，创建一个用户实体和DTO的映射。\n1. 定义实体类 1 2 3 4 5 6 7 8 9 10 11 12 @Data // Lombok注解 @AllArgsConstructor @NoArgsConstructor public class User { private Long id; private String firstName; private String lastName; private String email; private String phone; private LocalDate birthDate; private Boolean active; } 2. 定义DTO类 1 2 3 4 5 6 7 8 9 10 11 12 @Data @AllArgsConstructor @NoArgsConstructor public class UserDTO { private Long id; private String firstName; private String lastName; private String email; private String phoneNumber; // 注意：字段名与实体不同 private String birthDateStr; // 类型也不同 private Boolean isActive; // 字段名略有不同 } 3. 创建Mapper接口 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Mapper(componentModel = \u0026#34;spring\u0026#34;) // Spring组件 public interface UserMapper { @Mapping(source = \u0026#34;phone\u0026#34;, target = \u0026#34;phoneNumber\u0026#34;) @Mapping(source = \u0026#34;birthDate\u0026#34;, target = \u0026#34;birthDateStr\u0026#34;, dateFormat = \u0026#34;yyyy-MM-dd\u0026#34;) @Mapping(source = \u0026#34;active\u0026#34;, target = \u0026#34;isActive\u0026#34;) UserDTO toDTO(User user); @InheritInverseConfiguration // 自动反向配置 User toEntity(UserDTO dto); // 批量转换 List\u0026lt;UserDTO\u0026gt; toDTOList(List\u0026lt;User\u0026gt; users); } 4. 生成的代码 编译后，MapStruct会自动生成实现类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @Component // 因为componentModel = \u0026#34;spring\u0026#34; public class UserMapperImpl implements UserMapper { @Override public UserDTO toDTO(User user) { if (user == null) { return null; } UserDTO userDTO = new UserDTO(); userDTO.setId(user.getId()); userDTO.setFirstName(user.getFirstName()); userDTO.setLastName(user.getLastName()); userDTO.setEmail(user.getEmail()); userDTO.setPhoneNumber(user.getPhone()); userDTO.setIsActive(user.getActive()); if (user.getBirthDate() != null) { userDTO.setBirthDateStr( DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd\u0026#34;) .format(user.getBirthDate()) ); } return userDTO; } @Override public User toEntity(UserDTO dto) { // 自动生成的反向映射逻辑 // ... } } 5. 使用映射器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Service public class UserService { @Autowired private UserMapper userMapper; public UserDTO getUserDTO(Long id) { User user = userRepository.findById(id); return userMapper.toDTO(user); // 一行搞定！ } public User createUser(UserDTO dto) { User user = userMapper.toEntity(dto); return userRepository.save(user); } } 基本映射用法 字段名映射 1 2 3 4 5 6 7 8 @Mapper public interface ProductMapper { @Mapping(source = \u0026#34;productName\u0026#34;, target = \u0026#34;name\u0026#34;) @Mapping(source = \u0026#34;productPrice\u0026#34;, target = \u0026#34;price\u0026#34;) @Mapping(source = \u0026#34;productDescription\u0026#34;, target = \u0026#34;desc\u0026#34;) ProductDTO toDTO(Product product); } 类型转换 MapStruct支持多种自动类型转换：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Mapper public interface ConversionMapper { // 数字类型转换 @Mapping(source = \u0026#34;count\u0026#34;, target = \u0026#34;countStr\u0026#34;) TargetObject convert(SourceObject source); // 日期格式化 @Mapping(source = \u0026#34;createTime\u0026#34;, target = \u0026#34;createTimeStr\u0026#34;, dateFormat = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) TargetObject convertDate(SourceObject source); // 布尔值转换 @Mapping(source = \u0026#34;enabled\u0026#34;, target = \u0026#34;status\u0026#34;, expression = \u0026#34;java(source.getEnabled() ? \\\u0026#34;ACTIVE\\\u0026#34; : \\\u0026#34;INACTIVE\\\u0026#34;)\u0026#34;) TargetObject convertBoolean(SourceObject source); } 常量赋值 1 2 3 4 5 6 7 8 @Mapper public interface ConstantMapper { @Mapping(target = \u0026#34;status\u0026#34;, constant = \u0026#34;CREATED\u0026#34;) @Mapping(target = \u0026#34;version\u0026#34;, constant = \u0026#34;1.0\u0026#34;) @Mapping(target = \u0026#34;createdBy\u0026#34;, constant = \u0026#34;SYSTEM\u0026#34;) UserDTO toDTO(User user); } 默认值 1 2 3 4 5 6 7 8 @Mapper public interface DefaultMapper { @Mapping(target = \u0026#34;country\u0026#34;, defaultValue = \u0026#34;CN\u0026#34;) @Mapping(target = \u0026#34;language\u0026#34;, defaultValue = \u0026#34;zh_CN\u0026#34;) @Mapping(target = \u0026#34;timezone\u0026#34;, defaultValue = \u0026#34;Asia/Shanghai\u0026#34;) UserProfileDTO toDTO(UserProfile profile); } 忽略字段 1 2 3 4 5 6 7 8 @Mapper public interface IgnoreMapper { @Mapping(target = \u0026#34;password\u0026#34;, ignore = true) @Mapping(target = \u0026#34;internalId\u0026#34;, ignore = true) @Mapping(target = \u0026#34;createdAt\u0026#34;, ignore = true) UserDTO toSafeDTO(User user); } 高级映射技巧 自定义方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @Mapper public interface AdvancedUserMapper { @Mapping(source = \u0026#34;firstName\u0026#34;, target = \u0026#34;fullName\u0026#34;, qualifiedByName = \u0026#34;buildFullName\u0026#34;) @Mapping(source = \u0026#34;email\u0026#34;, target = \u0026#34;emailDomain\u0026#34;, qualifiedByName = \u0026#34;extractDomain\u0026#34;) UserDTO toDTO(User user); @Named(\u0026#34;buildFullName\u0026#34;) default String buildFullName(String firstName, String lastName) { if (firstName == null \u0026amp;\u0026amp; lastName == null) { return null; } return (firstName != null ? firstName : \u0026#34;\u0026#34;) + \u0026#34; \u0026#34; + (lastName != null ? lastName : \u0026#34;\u0026#34;); } @Named(\u0026#34;extractDomain\u0026#34;) default String extractDomain(String email) { if (email == null || !email.contains(\u0026#34;@\u0026#34;)) { return null; } return email.substring(email.indexOf(\u0026#34;@\u0026#34;) + 1); } } 多源对象映射 1 2 3 4 5 6 7 8 9 10 @Mapper public interface MultiSourceMapper { @Mapping(source = \u0026#34;user.id\u0026#34;, target = \u0026#34;userId\u0026#34;) @Mapping(source = \u0026#34;user.name\u0026#34;, target = \u0026#34;userName\u0026#34;) @Mapping(source = \u0026#34;address.street\u0026#34;, target = \u0026#34;street\u0026#34;) @Mapping(source = \u0026#34;address.city\u0026#34;, target = \u0026#34;city\u0026#34;) @Mapping(source = \u0026#34;profile.bio\u0026#34;, target = \u0026#34;biography\u0026#34;) UserDetailDTO combine(User user, Address address, Profile profile); } 条件映射 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Mapper public interface ConditionalMapper { @Mapping(target = \u0026#34;discountedPrice\u0026#34;, expression = \u0026#34;java(calculateDiscount(product))\u0026#34;) ProductDTO toDTO(Product product); default BigDecimal calculateDiscount(Product product) { if (product.getCategory().equals(\u0026#34;VIP\u0026#34;)) { return product.getPrice().multiply(new BigDecimal(\u0026#34;0.8\u0026#34;)); } return product.getPrice(); } } 枚举映射 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // 源枚举 public enum UserStatus { ACTIVE, INACTIVE, PENDING } // 目标枚举 public enum UserState { ENABLED, DISABLED, WAITING } @Mapper public interface EnumMapper { @ValueMapping(source = \u0026#34;ACTIVE\u0026#34;, target = \u0026#34;ENABLED\u0026#34;) @ValueMapping(source = \u0026#34;INACTIVE\u0026#34;, target = \u0026#34;DISABLED\u0026#34;) @ValueMapping(source = \u0026#34;PENDING\u0026#34;, target = \u0026#34;WAITING\u0026#34;) UserState mapStatus(UserStatus status); // 或使用表达式 @Mapping(target = \u0026#34;state\u0026#34;, expression = \u0026#34;java(mapUserStatus(user.getStatus()))\u0026#34;) UserDTO toDTO(User user); default UserState mapUserStatus(UserStatus status) { return switch (status) { case ACTIVE -\u0026gt; UserState.ENABLED; case INACTIVE -\u0026gt; UserState.DISABLED; case PENDING -\u0026gt; UserState.WAITING; }; } } 集合与复杂对象映射 集合映射 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Mapper(componentModel = \u0026#34;spring\u0026#34;) public interface CollectionMapper { // List映射 List\u0026lt;UserDTO\u0026gt; toDTOList(List\u0026lt;User\u0026gt; users); // Set映射 Set\u0026lt;UserDTO\u0026gt; toDTOSet(Set\u0026lt;User\u0026gt; users); // Map映射 Map\u0026lt;String, UserDTO\u0026gt; toDTOMap(Map\u0026lt;String, User\u0026gt; userMap); // 嵌套集合映射 @Mapping(source = \u0026#34;tags\u0026#34;, target = \u0026#34;tagNames\u0026#34;) ArticleDTO toDTO(Article article); // 自定义集合转换 default List\u0026lt;String\u0026gt; mapTags(List\u0026lt;Tag\u0026gt; tags) { return tags.stream() .map(Tag::getName) .collect(Collectors.toList()); } } 嵌套对象映射 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Mapper(uses = {AddressMapper.class, ProfileMapper.class}) public interface ComplexUserMapper { @Mapping(source = \u0026#34;personalInfo.firstName\u0026#34;, target = \u0026#34;firstName\u0026#34;) @Mapping(source = \u0026#34;personalInfo.lastName\u0026#34;, target = \u0026#34;lastName\u0026#34;) @Mapping(source = \u0026#34;contactInfo.email\u0026#34;, target = \u0026#34;email\u0026#34;) @Mapping(source = \u0026#34;contactInfo.phone\u0026#34;, target = \u0026#34;phoneNumber\u0026#34;) // address和profile会自动使用对应的Mapper UserCompleteDTO toDTO(User user); } @Mapper public interface AddressMapper { AddressDTO toDTO(Address address); } @Mapper public interface ProfileMapper { ProfileDTO toDTO(Profile profile); } 更新现有对象 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Mapper public interface UpdateMapper { @Mapping(target = \u0026#34;id\u0026#34;, ignore = true) // 不更新ID @Mapping(target = \u0026#34;createdAt\u0026#34;, ignore = true) // 不更新创建时间 @Mapping(target = \u0026#34;updatedAt\u0026#34;, expression = \u0026#34;java(java.time.LocalDateTime.now())\u0026#34;) void updateUserFromDTO(UserDTO dto, @MappingTarget User user); } // 使用方式 @Service public class UserService { @Autowired private UpdateMapper updateMapper; public User updateUser(Long id, UserDTO dto) { User existingUser = userRepository.findById(id); updateMapper.updateUserFromDTO(dto, existingUser); return userRepository.save(existingUser); } } 最佳实践与性能优化 1. 组件模型选择 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 推荐：Spring环境下使用spring组件模型 @Mapper(componentModel = \u0026#34;spring\u0026#34;) public interface UserMapper { // ... } // CDI环境 @Mapper(componentModel = \u0026#34;cdi\u0026#34;) public interface UserMapper { // ... } // 默认方式（手动获取实例） @Mapper public interface UserMapper { UserMapper INSTANCE = Mappers.getMapper(UserMapper.class); // ... } 2. 继承配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Mapper public interface BaseMapper { @Mapping(target = \u0026#34;createdAt\u0026#34;, ignore = true) @Mapping(target = \u0026#34;updatedAt\u0026#34;, ignore = true) @Mapping(target = \u0026#34;version\u0026#34;, ignore = true) BaseDTO toDTO(BaseEntity entity); } // 继承基础配置 @Mapper public interface UserMapper extends BaseMapper { @InheritConfiguration // 继承父接口配置 @Mapping(source = \u0026#34;phone\u0026#34;, target = \u0026#34;phoneNumber\u0026#34;) UserDTO toDTO(User user); } 3. 性能优化技巧 1 2 3 4 5 6 7 8 9 10 11 12 @Mapper( componentModel = \u0026#34;spring\u0026#34;, unmappedTargetPolicy = ReportingPolicy.ERROR, // 未映射字段报错 nullValuePropertyMappingStrategy = NullValuePropertyMappingStrategy.IGNORE // 忽略null值 ) public interface OptimizedMapper { // 对于大型对象，考虑使用Builder模式 @Builder @Mapping(source = \u0026#34;user.profile.bio\u0026#34;, target = \u0026#34;biography\u0026#34;) UserDTO toDTO(User user); } 4. 调试与验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Mapper( componentModel = \u0026#34;spring\u0026#34;, unmappedTargetPolicy = ReportingPolicy.WARN, // 警告未映射字段 unmappedSourcePolicy = ReportingPolicy.WARN // 警告未使用源字段 ) public interface DebuggableMapper { // 使用@BeanMapping进行详细配置 @BeanMapping( nullValuePropertyMappingStrategy = NullValuePropertyMappingStrategy.SET_TO_NULL, nullValueCheckStrategy = NullValueCheckStrategy.ALWAYS ) UserDTO toDTO(User user); } 实战案例 案例1：电商订单系统 假设我们有一个电商系统，需要处理订单数据的转换：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 // 实体类 @Entity @Data public class Order { private Long id; private String orderNumber; private LocalDateTime orderTime; private OrderStatus status; private BigDecimal totalAmount; private User customer; private List\u0026lt;OrderItem\u0026gt; items; private Address shippingAddress; } @Entity @Data public class OrderItem { private Long id; private Product product; private Integer quantity; private BigDecimal unitPrice; private BigDecimal subtotal; } // DTO类 @Data public class OrderDTO { private Long orderId; private String orderNumber; private String orderTimeStr; private String statusName; private BigDecimal totalAmount; private String customerName; private List\u0026lt;OrderItemDTO\u0026gt; items; private String shippingAddressStr; private Integer totalItems; private BigDecimal averageItemPrice; } @Data public class OrderItemDTO { private String productName; private String productCode; private Integer quantity; private BigDecimal unitPrice; private BigDecimal subtotal; } // Mapper实现 @Mapper( componentModel = \u0026#34;spring\u0026#34;, uses = {OrderItemMapper.class}, imports = {DateTimeFormatter.class, Collectors.class} ) public interface OrderMapper { @Mapping(source = \u0026#34;id\u0026#34;, target = \u0026#34;orderId\u0026#34;) @Mapping(source = \u0026#34;orderTime\u0026#34;, target = \u0026#34;orderTimeStr\u0026#34;, dateFormat = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) @Mapping(source = \u0026#34;status\u0026#34;, target = \u0026#34;statusName\u0026#34;, qualifiedByName = \u0026#34;mapOrderStatus\u0026#34;) @Mapping(source = \u0026#34;customer.firstName\u0026#34;, target = \u0026#34;customerName\u0026#34;, qualifiedByName = \u0026#34;buildCustomerName\u0026#34;) @Mapping(source = \u0026#34;shippingAddress\u0026#34;, target = \u0026#34;shippingAddressStr\u0026#34;, qualifiedByName = \u0026#34;formatAddress\u0026#34;) @Mapping(target = \u0026#34;totalItems\u0026#34;, expression = \u0026#34;java(calculateTotalItems(order.getItems()))\u0026#34;) @Mapping(target = \u0026#34;averageItemPrice\u0026#34;, expression = \u0026#34;java(calculateAveragePrice(order.getItems()))\u0026#34;) OrderDTO toDTO(Order order); @Named(\u0026#34;mapOrderStatus\u0026#34;) default String mapOrderStatus(OrderStatus status) { return switch (status) { case PENDING -\u0026gt; \u0026#34;待处理\u0026#34;; case CONFIRMED -\u0026gt; \u0026#34;已确认\u0026#34;; case SHIPPED -\u0026gt; \u0026#34;已发货\u0026#34;; case DELIVERED -\u0026gt; \u0026#34;已送达\u0026#34;; case CANCELLED -\u0026gt; \u0026#34;已取消\u0026#34;; }; } @Named(\u0026#34;buildCustomerName\u0026#34;) default String buildCustomerName(String firstName, String lastName) { return (firstName != null ? firstName : \u0026#34;\u0026#34;) + (lastName != null ? \u0026#34; \u0026#34; + lastName : \u0026#34;\u0026#34;); } @Named(\u0026#34;formatAddress\u0026#34;) default String formatAddress(Address address) { if (address == null) return null; return String.format(\u0026#34;%s %s, %s, %s\u0026#34;, address.getStreet(), address.getCity(), address.getState(), address.getZipCode()); } default Integer calculateTotalItems(List\u0026lt;OrderItem\u0026gt; items) { return items.stream() .mapToInt(OrderItem::getQuantity) .sum(); } default BigDecimal calculateAveragePrice(List\u0026lt;OrderItem\u0026gt; items) { if (items.isEmpty()) return BigDecimal.ZERO; BigDecimal total = items.stream() .map(OrderItem::getUnitPrice) .reduce(BigDecimal.ZERO, BigDecimal::add); return total.divide(new BigDecimal(items.size()), 2, RoundingMode.HALF_UP); } } @Mapper(componentModel = \u0026#34;spring\u0026#34;) public interface OrderItemMapper { @Mapping(source = \u0026#34;product.name\u0026#34;, target = \u0026#34;productName\u0026#34;) @Mapping(source = \u0026#34;product.code\u0026#34;, target = \u0026#34;productCode\u0026#34;) OrderItemDTO toDTO(OrderItem item); } 案例2：用户权限系统 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // 复杂的用户权限映射 @Mapper( componentModel = \u0026#34;spring\u0026#34;, uses = {RoleMapper.class, PermissionMapper.class} ) public interface UserSecurityMapper { @Mapping(source = \u0026#34;user.id\u0026#34;, target = \u0026#34;userId\u0026#34;) @Mapping(source = \u0026#34;user.username\u0026#34;, target = \u0026#34;username\u0026#34;) @Mapping(source = \u0026#34;user.roles\u0026#34;, target = \u0026#34;roleNames\u0026#34;, qualifiedByName = \u0026#34;extractRoleNames\u0026#34;) @Mapping(source = \u0026#34;user.roles\u0026#34;, target = \u0026#34;permissions\u0026#34;, qualifiedByName = \u0026#34;extractAllPermissions\u0026#34;) @Mapping(target = \u0026#34;hasAdminRole\u0026#34;, expression = \u0026#34;java(checkAdminRole(user.getRoles()))\u0026#34;) UserSecurityDTO toSecurityDTO(User user); @Named(\u0026#34;extractRoleNames\u0026#34;) default Set\u0026lt;String\u0026gt; extractRoleNames(Set\u0026lt;Role\u0026gt; roles) { return roles.stream() .map(Role::getName) .collect(Collectors.toSet()); } @Named(\u0026#34;extractAllPermissions\u0026#34;) default Set\u0026lt;String\u0026gt; extractAllPermissions(Set\u0026lt;Role\u0026gt; roles) { return roles.stream() .flatMap(role -\u0026gt; role.getPermissions().stream()) .map(Permission::getName) .collect(Collectors.toSet()); } default Boolean checkAdminRole(Set\u0026lt;Role\u0026gt; roles) { return roles.stream() .anyMatch(role -\u0026gt; \u0026#34;ADMIN\u0026#34;.equals(role.getName())); } } 常见问题与解决方案 1. 编译时错误 问题： \u0026ldquo;No property named \u0026lsquo;xxx\u0026rsquo; exists in source parameter\u0026rdquo;\n解决方案：\n1 2 // 确保字段名正确，或使用@Mapping指定 @Mapping(source = \u0026#34;firstName\u0026#34;, target = \u0026#34;fname\u0026#34;) // 明确指定映射关系 问题： 循环依赖\n解决方案：\n1 2 3 4 5 6 @Mapper(uses = {DepartmentMapper.class}) public interface EmployeeMapper { @Mapping(target = \u0026#34;department.employees\u0026#34;, ignore = true) // 忽略循环引用 EmployeeDTO toDTO(Employee employee); } 2. 运行时问题 问题： NullPointerException\n解决方案：\n1 2 3 4 5 6 7 8 9 10 @Mapper( nullValuePropertyMappingStrategy = NullValuePropertyMappingStrategy.IGNORE, nullValueCheckStrategy = NullValueCheckStrategy.ALWAYS ) public interface SafeMapper { @Mapping(target = \u0026#34;address\u0026#34;, expression = \u0026#34;java(user.getAddress() != null ? mapAddress(user.getAddress()) : null)\u0026#34;) UserDTO toDTO(User user); } 3. 性能问题 问题： 大量对象转换性能差\n解决方案：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Mapper( componentModel = \u0026#34;spring\u0026#34;, implementationName = \u0026#34;Fast\u0026lt;CLASS_NAME\u0026gt;Impl\u0026#34;, // 自定义实现类名 implementationPackage = \u0026#34;com.example.mapper.impl\u0026#34; // 指定包名 ) public interface FastMapper { // 使用Stream API进行批量转换 default List\u0026lt;UserDTO\u0026gt; toBatchDTO(List\u0026lt;User\u0026gt; users) { return users.parallelStream() // 并行处理 .map(this::toDTO) .collect(Collectors.toList()); } } 4. 调试技巧 启用详细日志：\n1 2 3 4 5 6 7 8 @Mapper( componentModel = \u0026#34;spring\u0026#34;, unmappedTargetPolicy = ReportingPolicy.WARN, unmappedSourcePolicy = ReportingPolicy.WARN ) public interface DebuggableMapper { // MapStruct会在编译时输出详细的映射信息 } 查看生成的代码： 生成的实现类位于：target/generated-sources/annotations/\n🎯 总结 MapStruct是Java开发中处理对象映射的强大工具，通过本教程你已经掌握了：\n核心知识点 ✅ MapStruct的基本概念和优势 ✅ 环境搭建和配置方法 ✅ 基础映射和高级映射技巧 ✅ 集合和复杂对象的处理 ✅ 最佳实践和性能优化 实用技巧 🔧 使用@Mapping进行字段映射 🔧 通过@Named创建自定义转换方法 🔧 利用@InheritConfiguration减少重复配置 🔧 使用expression处理复杂业务逻辑 最佳实践 选择合适的组件模型（推荐使用componentModel = \u0026quot;spring\u0026quot;） 合理使用继承配置减少代码重复 处理null值避免运行时异常 注意循环依赖问题 关注性能优化，特别是大批量数据转换 下一步建议 深入实践：在实际项目中应用MapStruct 探索高级特性：如装饰器模式、条件映射等 集成测试：编写单元测试验证映射逻辑 监控性能：在生产环境中观察映射性能 MapStruct让对象映射变得简单而高效，掌握了这个工具，你将在Java开发中如虎添翼！🚀\n推荐阅读：\nMapStruct官方文档 Spring Boot集成最佳实践 Java性能优化指南 Happy Coding! 🎉\n","date":"2025-07-03T00:00:00Z","image":"https://nova-bryan.github.io/p/mapstruct%E5%AE%8C%E5%85%A8%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/mapstruct%E5%AE%8C%E5%85%A8%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97/","title":"MapStruct完全学习指南"},{"content":"数据库索引完全指南 目录 什么是数据库索引 索引的基本原理 索引的类型 索引的创建和使用 索引的优缺点 索引失效的场景 索引优化实践 总结 什么是数据库索引 数据库索引是一种数据结构，它提供了快速访问数据库表中数据的路径。就像书籍的目录一样，索引可以帮助数据库快速定位到具体的数据行，而不需要扫描整个表。\n形象化理解 想象你要在一本1000页的字典中查找单词\u0026quot;database\u0026quot;：\n没有索引：你需要从第1页开始逐页翻阅，直到找到为止 有索引：你直接翻到\u0026quot;D\u0026quot;开头的目录页，快速定位到具体页码 索引就是数据库的\u0026quot;目录\u0026quot;。\n性能对比示意图 1 2 3 4 5 6 7 8 9 graph TD A[\u0026#34;全表扫描\u0026lt;br/\u0026gt;O(n) 时间复杂度\u0026#34;] --\u0026gt; B[\u0026#34;逐行检查每条记录\u0026#34;] B --\u0026gt; C[\u0026#34;找到匹配记录\u0026#34;] D[\u0026#34;使用索引\u0026lt;br/\u0026gt;O(log n) 时间复杂度\u0026#34;] --\u0026gt; E[\u0026#34;通过索引快速定位\u0026#34;] E --\u0026gt; F[\u0026#34;直接访问目标记录\u0026#34;] G[\u0026#34;1000万记录对比\u0026#34;] --\u0026gt; H[\u0026#34;全表扫描: 最多1000万次比较\u0026#34;] G --\u0026gt; I[\u0026#34;B+树索引: 最多4次比较\u0026#34;] 索引的基本原理 数据存储结构 数据库中的数据通常存储在**页(Page)**中，每个页包含多条记录。\n1 2 3 4 5 表数据存储示意： 页1: [记录1, 记录2, 记录3, ...] 页2: [记录101, 记录102, 记录103, ...] 页3: [记录201, 记录202, 记录203, ...] ... B+树索引原理 大多数数据库使用B+树作为索引的数据结构：\n1 2 3 4 5 6 B+树索引结构： 根节点 / \\ 内部节点1 内部节点2 / | \\ / | \\ 叶子节点1 叶子2 叶子3 叶子4 叶子5 叶子6 B+树的特点：\n所有数据都存储在叶子节点 叶子节点之间有指针连接，便于范围查询 树的高度较低，减少磁盘I/O次数 支持顺序访问和随机访问 B+树详细结构图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 graph TD subgraph \u0026#34;B+树索引结构\u0026#34; Root[\u0026#34;根节点\u0026lt;br/\u0026gt;[30, 60]\u0026#34;] Internal1[\u0026#34;内部节点1\u0026lt;br/\u0026gt;[10, 20]\u0026#34;] Internal2[\u0026#34;内部节点2\u0026lt;br/\u0026gt;[40, 50]\u0026#34;] Internal3[\u0026#34;内部节点3\u0026lt;br/\u0026gt;[70, 80]\u0026#34;] Leaf1[\u0026#34;叶子节点1\u0026lt;br/\u0026gt;[1,5,8,10]\u0026lt;br/\u0026gt;→ 数据页1\u0026#34;] Leaf2[\u0026#34;叶子节点2\u0026lt;br/\u0026gt;[15,18,20,25]\u0026lt;br/\u0026gt;→ 数据页2\u0026#34;] Leaf3[\u0026#34;叶子节点3\u0026lt;br/\u0026gt;[35,38,40,42]\u0026lt;br/\u0026gt;→ 数据页3\u0026#34;] Leaf4[\u0026#34;叶子节点4\u0026lt;br/\u0026gt;[45,48,50,55]\u0026lt;br/\u0026gt;→ 数据页4\u0026#34;] Leaf5[\u0026#34;叶子节点5\u0026lt;br/\u0026gt;[65,68,70,75]\u0026lt;br/\u0026gt;→ 数据页5\u0026#34;] Leaf6[\u0026#34;叶子节点6\u0026lt;br/\u0026gt;[78,80,85,90]\u0026lt;br/\u0026gt;→ 数据页6\u0026#34;] Root --\u0026gt; Internal1 Root --\u0026gt; Internal2 Root --\u0026gt; Internal3 Internal1 --\u0026gt; Leaf1 Internal1 --\u0026gt; Leaf2 Internal2 --\u0026gt; Leaf3 Internal2 --\u0026gt; Leaf4 Internal3 --\u0026gt; Leaf5 Internal3 --\u0026gt; Leaf6 Leaf1 -.-\u0026gt; Leaf2 Leaf2 -.-\u0026gt; Leaf3 Leaf3 -.-\u0026gt; Leaf4 Leaf4 -.-\u0026gt; Leaf5 Leaf5 -.-\u0026gt; Leaf6 end 索引查找过程 从根节点开始 根据比较结果选择分支 逐层向下直到叶子节点 在叶子节点中找到目标数据 查找过程流程图 1 2 3 4 5 6 7 8 9 10 11 flowchart TD A[\u0026#34;查询: SELECT * FROM users WHERE id = 42\u0026#34;] --\u0026gt; B[\u0026#34;开始从根节点查找\u0026#34;] B --\u0026gt; C[\u0026#34;根节点: [30, 60]\u0026lt;br/\u0026gt;42 \u0026gt; 30 且 42 \u0026lt; 60\u0026#34;] C --\u0026gt; D[\u0026#34;进入中间节点2: [40, 50]\u0026lt;br/\u0026gt;42 \u0026gt; 40 且 42 \u0026lt; 50\u0026#34;] D --\u0026gt; E[\u0026#34;进入叶子节点3: [35,38,40,42]\u0026lt;br/\u0026gt;找到目标值 42\u0026#34;] E --\u0026gt; F[\u0026#34;通过指针访问数据页3\u0026#34;] F --\u0026gt; G[\u0026#34;返回完整记录数据\u0026#34;] style A fill:#e1f5fe style G fill:#c8e6c9 style E fill:#fff3e0 索引的类型 索引分类总览 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 graph TD A[\u0026#34;数据库索引分类\u0026#34;] --\u0026gt; B[\u0026#34;按数据结构\u0026#34;] A --\u0026gt; C[\u0026#34;按字段数量\u0026#34;] A --\u0026gt; D[\u0026#34;按功能特性\u0026#34;] B --\u0026gt; B1[\u0026#34;B+树索引\u0026lt;br/\u0026gt;（最常用）\u0026#34;] B --\u0026gt; B2[\u0026#34;哈希索引\u0026lt;br/\u0026gt;（等值查询）\u0026#34;] B --\u0026gt; B3[\u0026#34;全文索引\u0026lt;br/\u0026gt;（文本搜索）\u0026#34;] C --\u0026gt; C1[\u0026#34;单列索引\u0026lt;br/\u0026gt;CREATE INDEX idx_name ON users(name)\u0026#34;] C --\u0026gt; C2[\u0026#34;复合索引\u0026lt;br/\u0026gt;CREATE INDEX idx_name_age ON users(name,age)\u0026#34;] D --\u0026gt; D1[\u0026#34;普通索引\u0026lt;br/\u0026gt;（可重复）\u0026#34;] D --\u0026gt; D2[\u0026#34;唯一索引\u0026lt;br/\u0026gt;（不可重复）\u0026#34;] D --\u0026gt; D3[\u0026#34;主键索引\u0026lt;br/\u0026gt;（主键约束）\u0026#34;] style B1 fill:#c8e6c9 style C2 fill:#fff3e0 style D3 fill:#ffcdd2 1. 按数据结构分类 B+树索引（最常用） 特点：平衡多路搜索树 适用：等值查询、范围查询、排序 存储引擎：InnoDB、MyISAM 哈希索引 特点：基于哈希表 适用：等值查询 限制：不支持范围查询、排序 存储引擎：Memory 2. 按字段数量分类 单列索引 1 CREATE INDEX idx_name ON users(name); 复合索引（联合索引） 1 CREATE INDEX idx_name_age ON users(name, age); 3. 按功能分类 普通索引 1 CREATE INDEX idx_email ON users(email); 唯一索引 1 CREATE UNIQUE INDEX idx_username ON users(username); 主键索引 1 ALTER TABLE users ADD PRIMARY KEY(id); 全文索引 1 CREATE FULLTEXT INDEX idx_content ON articles(content); 索引的创建和使用 创建索引 建表时创建 1 2 3 4 5 6 7 8 9 CREATE TABLE users ( id INT PRIMARY KEY, username VARCHAR(50) UNIQUE, email VARCHAR(100), age INT, created_at TIMESTAMP, INDEX idx_email (email), INDEX idx_age_created (age, created_at) ); 建表后创建 1 2 3 4 5 6 7 8 -- 普通索引 CREATE INDEX idx_email ON users(email); -- 唯一索引 CREATE UNIQUE INDEX idx_username ON users(username); -- 复合索引 CREATE INDEX idx_name_age ON users(name, age); 查看索引 1 2 3 4 5 -- 查看表的所有索引 SHOW INDEX FROM users; -- 查看索引使用情况 EXPLAIN SELECT * FROM users WHERE email = \u0026#39;john@example.com\u0026#39;; 删除索引 1 DROP INDEX idx_email ON users; 索引的优缺点 优点 1. 提高查询速度 无索引：全表扫描，时间复杂度 O(n) 有索引：B+树查找，时间复杂度 O(log n) 2. 加速排序 1 2 -- 如果name字段有索引，以下查询会很快 SELECT * FROM users ORDER BY name; 3. 提升连接效率 1 2 3 -- 如果user_id有索引，JOIN操作会更快 SELECT * FROM orders o JOIN users u ON o.user_id = u.id; 4. 加速分组 1 2 -- 如果status有索引，GROUP BY会更快 SELECT status, COUNT(*) FROM orders GROUP BY status; 缺点 1. 占用存储空间 索引需要额外的存储空间 复合索引占用空间更大 2. 降低写操作性能 INSERT：需要维护索引结构 UPDATE：可能需要更新索引 DELETE：需要从索引中删除条目 3. 维护成本 数据变更时需要同步更新索引 索引越多，维护成本越高 索引失效的场景 索引失效场景总览 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 graph LR A[\u0026#34;索引失效场景\u0026#34;] --\u0026gt; B[\u0026#34;函数操作\u0026lt;br/\u0026gt;WHERE UPPER(name) = \u0026#39;JOHN\u0026#39;\u0026#34;] A --\u0026gt; C[\u0026#34;类型转换\u0026lt;br/\u0026gt;WHERE phone = 123456\u0026#34;] A --\u0026gt; D[\u0026#34;前导模糊查询\u0026lt;br/\u0026gt;WHERE name LIKE \u0026#39;%john\u0026#39;\u0026#34;] A --\u0026gt; E[\u0026#34;OR连接条件\u0026lt;br/\u0026gt;WHERE name=\u0026#39;john\u0026#39; OR age=25\u0026#34;] A --\u0026gt; F[\u0026#34;非最左匹配\u0026lt;br/\u0026gt;复合索引(name,age,city)\u0026lt;br/\u0026gt;WHERE age=25\u0026#34;] A --\u0026gt; G[\u0026#34;不等于操作\u0026lt;br/\u0026gt;WHERE status != \u0026#39;active\u0026#39;\u0026#34;] A --\u0026gt; H[\u0026#34;NULL值判断\u0026lt;br/\u0026gt;WHERE email IS NULL\u0026#34;] style B fill:#ffcdd2 style C fill:#ffcdd2 style D fill:#ffcdd2 style E fill:#ffcdd2 style F fill:#ffcdd2 style G fill:#ffcdd2 style H fill:#ffcdd2 1. 函数操作 1 2 3 4 5 -- 索引失效 SELECT * FROM users WHERE UPPER(name) = \u0026#39;JOHN\u0026#39;; -- 正确做法 SELECT * FROM users WHERE name = \u0026#39;john\u0026#39;; 2. 类型转换 1 2 3 4 5 -- 索引失效（假设phone是VARCHAR类型） SELECT * FROM users WHERE phone = 12345678901; -- 正确做法 SELECT * FROM users WHERE phone = \u0026#39;12345678901\u0026#39;; 3. 前导模糊查询 1 2 3 4 5 -- 索引失效 SELECT * FROM users WHERE name LIKE \u0026#39;%john\u0026#39;; -- 可以使用索引 SELECT * FROM users WHERE name LIKE \u0026#39;john%\u0026#39;; 4. OR连接的条件 1 2 3 4 5 6 7 -- 如果age没有索引，整个查询无法使用索引 SELECT * FROM users WHERE name = \u0026#39;john\u0026#39; OR age = 25; -- 改写为UNION（如果两个字段都有索引） SELECT * FROM users WHERE name = \u0026#39;john\u0026#39; UNION SELECT * FROM users WHERE age = 25; 5. 复合索引的非最左匹配 1 2 3 4 5 6 7 8 9 -- 假设有复合索引 (name, age, city) -- 可以使用索引 SELECT * FROM users WHERE name = \u0026#39;john\u0026#39;; SELECT * FROM users WHERE name = \u0026#39;john\u0026#39; AND age = 25; -- 索引失效 SELECT * FROM users WHERE age = 25; SELECT * FROM users WHERE city = \u0026#39;beijing\u0026#39;; 6. 不等于操作 1 2 3 4 5 -- 可能导致索引失效 SELECT * FROM users WHERE status != \u0026#39;active\u0026#39;; -- 建议改写 SELECT * FROM users WHERE status IN (\u0026#39;inactive\u0026#39;, \u0026#39;deleted\u0026#39;); 7. NULL值判断 1 2 3 -- 索引效果有限 SELECT * FROM users WHERE email IS NULL; SELECT * FROM users WHERE email IS NOT NULL; 索引优化实践 1. 选择合适的字段创建索引 高选择性字段 1 2 3 4 5 -- 好的选择：用户邮箱（几乎唯一） CREATE INDEX idx_email ON users(email); -- 差的选择：性别字段（只有几个值） -- 不建议：CREATE INDEX idx_gender ON users(gender); 经常用于查询条件的字段 1 2 3 4 5 -- 经常用于WHERE子句 CREATE INDEX idx_status ON orders(status); -- 经常用于JOIN CREATE INDEX idx_user_id ON orders(user_id); 2. 复合索引设计原则 最左前缀原则 1 2 3 4 5 6 7 8 9 -- 索引：(name, age, city) -- 可以使用索引的查询： SELECT * FROM users WHERE name = \u0026#39;john\u0026#39;; -- ✓ SELECT * FROM users WHERE name = \u0026#39;john\u0026#39; AND age = 25; -- ✓ SELECT * FROM users WHERE name = \u0026#39;john\u0026#39; AND city = \u0026#39;bj\u0026#39;; -- ✓（但不是最优） -- 不能使用索引的查询： SELECT * FROM users WHERE age = 25; -- ✗ SELECT * FROM users WHERE city = \u0026#39;beijing\u0026#39;; -- ✗ 字段顺序优化 1 2 3 4 -- 原则：选择性高的字段放前面，经常一起查询的字段放一起 -- 假设查询：WHERE status = \u0026#39;active\u0026#39; AND created_at \u0026gt; \u0026#39;2023-01-01\u0026#39; -- 如果status选择性更高： CREATE INDEX idx_status_created ON orders(status, created_at); 3. 覆盖索引优化 1 2 3 4 5 6 -- 普通查询：需要回表 CREATE INDEX idx_user_id ON orders(user_id); SELECT order_id, total_amount FROM orders WHERE user_id = 123; -- 覆盖索引：无需回表 CREATE INDEX idx_user_id_cover ON orders(user_id, order_id, total_amount); 4. 分页查询优化 1 2 3 4 5 6 7 8 -- 深分页问题 SELECT * FROM users ORDER BY created_at LIMIT 100000, 20; -- 优化方案：使用索引字段 SELECT * FROM users WHERE created_at \u0026gt; \u0026#39;2023-01-01 10:30:00\u0026#39; ORDER BY created_at LIMIT 20; 5. 索引监控和维护 查看索引使用情况 1 2 3 4 5 -- MySQL SELECT * FROM sys.schema_unused_indexes; -- 查看索引统计 SHOW INDEX FROM users; 定期重建索引 1 2 3 4 5 -- 重建索引（MySQL） ALTER TABLE users DROP INDEX idx_email, ADD INDEX idx_email(email); -- 或者 OPTIMIZE TABLE users; 实际案例分析 案例1：慢查询优化 问题查询：\n1 2 3 4 SELECT * FROM orders WHERE status = \u0026#39;pending\u0026#39; AND created_at \u0026gt;= \u0026#39;2023-01-01\u0026#39; AND user_id IN (1,2,3,4,5); 分析过程：\n使用 EXPLAIN 分析执行计划 发现全表扫描，耗时很长 分析查询条件的选择性 优化方案：\n1 2 3 4 5 -- 创建复合索引 CREATE INDEX idx_status_created_user ON orders(status, created_at, user_id); -- 或者根据实际查询模式创建 CREATE INDEX idx_user_status_created ON orders(user_id, status, created_at); 案例2：复合索引设计 业务场景：\n用户表经常按地区、年龄、状态查询 查询模式分析： WHERE region = \u0026lsquo;beijing\u0026rsquo; WHERE region = \u0026lsquo;beijing\u0026rsquo; AND age BETWEEN 20 AND 30 WHERE region = \u0026lsquo;beijing\u0026rsquo; AND status = \u0026lsquo;active\u0026rsquo; 索引设计：\n1 2 3 4 5 6 -- 方案1：按查询频率和选择性 CREATE INDEX idx_region_age_status ON users(region, age, status); -- 方案2：如果经常只按region和status查询 CREATE INDEX idx_region_status ON users(region, status); CREATE INDEX idx_region_age ON users(region, age); 总结 核心要点 索引本质：用空间换时间的数据结构，类似书籍目录 主要原理：B+树结构，减少磁盘I/O，提高查询效率 使用场景：频繁查询、排序、连接的字段 设计原则： 选择性高的字段 遵循最左前缀原则 考虑覆盖索引 避免过多索引 最佳实践清单 为经常出现在WHERE子句的字段创建索引 为经常用于JOIN的字段创建索引 复合索引遵循最左前缀原则 避免在索引字段上使用函数 定期监控和优化索引使用情况 平衡查询性能和存储空间 考虑业务场景，不要盲目创建索引 记住这句话 索引不是万能的，但没有索引是万万不能的。合理使用索引，让你的数据库查询飞起来！\n这份指南涵盖了数据库索引的方方面面，建议结合实际项目练习，加深理解。\n","date":"2025-07-03T00:00:00Z","image":"https://nova-bryan.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%AE%8C%E5%85%A8%E6%8C%87%E5%8D%97/","title":"数据库索引完全指南"},{"content":"一、常见集合篇 1. 为什么数组索引从0开始呢？假如从1开始不行咩 数组（Array）：一种用连续的内存空间存储相同数据类型数据的线性数据结构\n（1）在根据数组索引获取元素的时候，会用索引和寻址公式来计算内存所对应的元素数据，寻址公式是:数组的首地址+索引乘以存储数据的类型大小\n（2）如果数组的索引从1开始，寻址公式中，就需要增加一次减法操作，对于CPU来说就多了一次指令，性能不高。\n复杂度\n随机(通过下标)查询的时间复杂度是O(1)\n查找元素(未知下标)的时间复杂度是O(n)\n查找元素(未知下标但排序)通过二分查找的时间复杂度是O(logn)\n插入和删除的时候，为了保证数组的内存连续性，需要挪动数组元素，平均时间复杂度为O(n)\n2. ArrayList源码分析 ArrayList底层是用动态的数组实现的\n初始容量 ArrayList初始容量为0，当第一次添加数据的时候才会初始化容量为10\n扩容逻辑 ArrayList在进行扩容的时候是原来容量的1.5倍，每次扩容都需要拷贝数组\n添加逻辑 确保数组已使用长度（size）加1之后足够存下下一个数据 计算数组的容量，如果当前数组已使用长度+1后的大于当前的数组长度，则调用grow方法扩容（原来的1.5倍） 确保新增的数据有地方存储之后，则将新元素添加到位于size的位置上。 返回添加成功布尔值。 3. ArrayList list=new ArrayList(10)中的list扩容几次 该语句只是声明和实例了一个 ArrayList，指定了容量为 10，未扩容\n4. 如何实现数组和List之间的转换 数组转List ，使用JDK中java.util.Arrays工具类的asList方法 List转数组，使用List的toArray方法。无参toArray方法返回 Object数组，传入初始化长度的数组对象，返回该对象数组 用Arrays.asList转List后，如果修改了数组内容，list受影响吗 会受影响\n当你使用 Arrays.asList 方法将数组转换为 List 时，Arrays.asList 返回的 List 实际上是 Arrays 类的一个内部类 ArrayList 的实例。这个内部类并没有创建一个新的、独立的列表来存储数组元素，而是直接对传入的数组进行了包装。也就是说，这个 List 内部维护的是对原始数组的引用，它们指向的是同一块内存地址。\nList用toArray转数组后，如果修改了List内容，数组受影响吗 使用无参的 toArray 方法：这种情况下，返回的是一个 Object 类型的数组。toArray 方法会创建一个新的数组，并将 List 中的元素复制到这个新数组中。但是，这里复制的是元素的引用（对于引用类型），而不是元素本身。所以，如果 List 中的元素是可变对象，修改 List 中对象的属性会影响到数组中的对应对象；如果只是改变 List 中元素的引用（比如替换某个元素），则不会影响数组。 当传入一个指定类型的数组时，如果该数组长度足够，List 元素会直接填充到这个数组中；如果长度不够，会创建一个新的同类型数组。同样，对于引用类型的元素，复制的是引用，修改元素属性会相互影响，修改引用则不会。 5. ArrayList和LinkedList的区别是什么？ 底层数据结构 ArrayList 是动态数组的数据结构实现 LinkedList 是双向链表的数据结构实现 操作数据效率 ArrayList按照下标查询的时间复杂度O(1)【内存是连续的，根据寻址公式】， LinkedList不支持下标查询 查找（未知索引）： ArrayList需要遍历，链表也需要遍历，时间复杂度都是O(n) 新增和删除 ArrayList尾部插入和删除，时间复杂度是O(1)；其他部分增删需要挪动数组，时间复杂度是O(n) LinkedList头尾节点增删时间复杂度是O(1)，其他都需要遍历链表，时间复杂度是O(n) 内存空间占用 ArrayList底层是数组，内存连续，节省内存 LinkedList 是双向链表需要存储数据，和两个指针，更占用内存 线程安全 ArrayList和LinkedList都不是线程安全的 如果需要保证线程安全，有两种方案： 在方法内使用，由于每个线程都有自己独立的栈空间，局部变量在不同线程之间是隔离的，因此是线程安全的。 使用线程安全的替代类：可以使用线程安全的 ArrayList 和 LinkedList 替代类，例如 Vector （类似于线程安全的 ArrayList），或者使用 Collections.synchronizedList 方法将 ArrayList 或 LinkedList 包装成线程安全的列表。 6. 二叉树 每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。不过，二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。\n二叉树每个节点的左子树和右子树也分别满足二叉树的定义。\n二叉搜索树 二叉搜索树(Binary Search Tree,BST)又名二叉查找树，有序二叉树或者排序二叉树，是二叉树中比较常用的一种类型\n二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值\n插入，查找，删除的时间复杂度O(logn)，极端情况下二叉查找树已经退化成了链表，二叉搜索的时间复杂度O(n)\n红黑树 （1）红黑树的特质\n性质1：节点要么是红色,要么是黑色\n性质2：根节点是黑色\n性质3：叶子节点都是黑色的空节点\n性质4：红黑树中红色节点的子节点都是黑色\n性质5：从任一节点到叶子节点的所有路径都包含相同数目的黑色节点\n在添加或删除节点的时候，如果不符合这些性质会发生旋转，以达到所有的性质，保证红黑树的平衡\n（2）红黑树的复杂度\n查找： 红黑树也是一棵BST（二叉搜索树）树，查找操作的时间复杂度为：O(log n) 添加： 添加先要从根节点开始找到元素添加的位置，时间复杂度O(log n) 添加完成后涉及到复杂度为O(1)的旋转调整操作 故整体复杂度为：O(log n) 删除： 首先从根节点开始找到被删除元素的位置，时间复杂度O(log n) 删除完成后涉及到复杂度为O(1)的旋转调整操作 故整体复杂度为：O(log n) 7. 散列表 根据键（Key）直接访问在内存存储位置值（Value）的数据结构，它是由数组演化而来的，利用了数组支持按照下标进行随机访问数据的特性。\n散列函数 将键(key)映射为数组下标的函数叫做散列函数。可以表示为：hashValue = hash(key)\n散列函数的基本要求\n散列函数计算得到的散列值必须是大于等于0的正整数，因为hashValue需要作为数组的下标。 如果key1==key2，那么经过hash后得到的哈希值也必相同即：hash(key1) == hash(key2） 如果key1 != key2，那么经过hash后得到的哈希值也必不相同即：hash(key1) != hash(key2) 散列冲突 或者哈希冲突，哈希碰撞，指多个key映射到同一个数组下标位置\n散列冲突-链表法（拉链） 在散列表中，数组的每个下标位置我们可以称之为桶（bucket）或者槽（slot），每个桶(槽)会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。\n简单就是，如果有多个key最终的hash值是一样的，就会存入数组的同一个下标中，下标中挂一个链表存入多个数据\n时间复杂度\n平均情况下基于链表法解决冲突时查询的时间复杂度是O(1)\n散列表可能会退化为链表,查询的时间复杂度就从 O(1) 退化为 O(n)\n将链表法中的链表改造为其他高效的动态数据结构，比如红黑树，查询的时间复杂度是 O(logn)\n将链表法中的链表改造红黑树还有一个非常重要的原因，可以防止DDos攻击\nDDos 攻击:\n分布式拒绝服务攻击(英文意思是Distributed Denial of Service，简称DDoS）\n指处于不同位置的多个攻击者同时向一个或数个目标发动攻击，或者一个攻击者控制了位于不同位置的多台机器并利用这些机器对受害者同时实施攻击。由于攻击的发出点是分布在不同地方的，这类攻击称为分布式拒绝服务攻击，其中的攻击者可以有多个\n8. 说一下HashMap的实现原理？ HashMap的数据结构： 底层使用hash表数据结构，即数组和链表或红黑树\n当我们往HashMap中put元素时，利用key的hashCode重新hash计算出当前对象的元素在数组中的下标 存储时，如果出现hash值相同的key，此时有两种情况。 a. 如果key相同，则覆盖原始值；\nb. 如果key不同（出现冲突），则将当前的key-value放入链表或红黑树中\n获取时，直接找到hash值对应的下标，在进一步判断key是否相同，从而找到对应值。 HashMap 底层采用哈希表，结合了数组、链表和红黑树。数组是主体结构，每个数组元素是一个桶。当发生哈希冲突时，会用链表或红黑树来存储冲突元素。正常情况下用链表存储，若链表长度超过 8 且数组长度大于 64，链表就会转成红黑树，因为链表过长时，查找元素的时间复杂度会变成 O (n)，而红黑树能将查找、插入和删除操作的时间复杂度控制在 O (log n)。当红黑树节点数量少于 6 时，又会转回链表。以此提升查找效率。\n9. HashMap的jdk1.7和jdk1.8有什么区别 JDK1.8之前采用的是拉链法。拉链法：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8） 时并且数组长度达到64时，将链表转化为红黑树，以减少搜索时间。扩容 resize( ) 时，红黑树拆分成的树的结点数小于等于临界值6个，则退化成链表 10. HashMap的put方法的具体流程 判断键值对数组table是否为空或为null，否则执行resize()进行扩容（初始化）\n根据键值key计算hash值得到数组索引\n判断table[i]==null，条件成立，直接新建节点添加\n如果table[i]==null ,不成立\n4.1 判断table[i]的首个元素是否和key一样，如果相同直接覆盖value\n4.2 判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对\n4.3 遍历table[i]，链表的尾部插入数据，然后判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操 作，遍历过程中若发现key已经存在直接覆盖value\n插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold（数组长度*0.75），如果超过，进行扩容。\n11. 讲一讲HashMap的扩容机制 HashMap 的扩容机制是为了保证其性能，避免哈希冲突过多导致查询和插入效率下降。当元素数量达到扩容阈值（数组长度 * 负载因子，默认负载因子是 0.75）时，就会触发扩容操作。\n在添加元素或初始化的时候需要调用resize方法进行扩容，第一次添加数据初始化数组长度为16，以后每次每次扩容都是达到了扩容阈值（数组长度 * 0.75） 每次扩容的时候，都是扩容之前容量的2倍； 扩容之后，会新创建一个数组，需要把老数组中的数据挪动到新的数组中 无哈希冲突的节点：则直接使用 e.hash \u0026amp; (newCap - 1) 计算新数组的索引位置 红黑树节点：按照红黑树的规则将节点添加到新数组对应的位置。红黑树是自平衡二叉搜索树，添加节点时会进行旋转和变色等操作来保持平衡。 **链表节点：**需要遍历链表，可能会拆分链表。通过e.hash \u0026amp; oldCap判断元素在新数组中的位置： 若结果为 0，说明该元素在新数组中的索引位置和原数组相同。因为扩容后多考虑的那一位二进制为 0，对索引计算结果无影响。 若结果不为 0，该元素在新数组中的索引位置是原数组中的索引位置加上 oldCap。这是因为扩容后多考虑的那一位二进制为 1，使得索引结果增加了原数组的长度。 12. hashMap的寻址算法 首先获取key的hashCode值，然后z再调用hash()方法进行二次哈希，右移16位 异或运算 原来的hashCode值，主要作用就是使原来的hash值更加均匀，减少hash冲突\n最后（capacity-1）\u0026amp; hash得到索引\n13. 关于hash值的其他面试题：为何HashMap的数组长度一定是2的次幂？ 计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模 扩容时重新计算索引效率更高： hash \u0026amp; oldCap == 0 的元素留在原来位置 ，否则新位置 = 旧位置 + oldCap 14. hashmap在1.7情况下的多线程死循环问题 在 JDK 1.7 里，HashMap 扩容时把旧数组的数据迁移到新数组，链表采用的是头插法。\n假设有两个线程同时操作 HashMap 的扩容。\n线程一读取数据准备扩容：线程一读取当前 HashMap 的数据，发现有一个链表，准备对数组进行扩容。可就在这时候，线程二介入了。 线程二完成扩容：线程二也读取了 HashMap 的数据，然后直接开始扩容。因为用的是头插法，扩容后链表的顺序就颠倒了。打个比方，原来链表顺序是 A -\u0026gt; B，扩容后就变成 B -\u0026gt; A 了，之后线程二执行结束。 线程一继续扩容引发死循环：线程一接着干活，先把 A 移到新链表，再把 B 插到新链表的头部。但由于线程二已经把链表顺序颠倒了，B 的 next 指针指向了 A。这样一来，新链表就变成 B -\u0026gt; A -\u0026gt; B，形成了一个循环。之后要是有线程去访问这个链表，就会一直在这个循环里出不来，也就是出现了死循环问题。 JDK 8 对扩容算法做了调整，链表数据迁移时用的是尾插法，就是新元素会被插到链表的尾部。这样扩容后链表元素的顺序和扩容前是一样的，就不会出现因为链表顺序颠倒而导致的死循环问题了。\n总的来说，JDK 1.7 的 HashMap 在多线程扩容时，头插法可能会让链表顺序错乱，进而引发死循环；而 JDK 8 采用尾插法，避免了这个问题，提升了多线程环境下的稳定性。\n关于 HashMap、HashTable、ConcurrentHashMap、TreeMap 和 HashSet 1. 基本概念\nHashMap：基于哈希表实现的 Map 接口，允许 null 键和 null 值，非线程安全，适用于单线程环境下快速查找和存储键值对。 HashTable：古老的 Map 实现类，与 HashMap 类似，但不允许 null 键和 null 值，并且是线程安全的（方法都被 synchronized 修饰），不过性能相对 HashMap 较差，现在已不常用。 ConcurrentHashMap：线程安全的 Map 实现类，采用分段锁（锁分段技术）提高并发性能，允许多个线程同时操作不同的段，适用于高并发环境下的键值对存储和访问。 TreeMap：实现了 SortedMap 接口，基于红黑树实现，会对键进行排序（默认升序，也可自定义比较器），不允许 null 键，值可以为 null，非线程安全，适合需要对键进行有序操作的场景。 HashSet：实现了 Set 接口，基于 HashMap 实现（元素存储在 HashMap 的键中，值为一个默认的 Object 对象），不允许重复元素，允许 null 元素，非线程安全，用于存储不重复的元素集合。 2. 线程安全性\nHashMap 和 HashSet：非线程安全，在多线程环境下可能会出现数据不一致等问题。 HashTable：线程安全，通过 synchronized 保证同一时间只有一个线程能访问其方法。 ConcurrentHashMap：线程安全，采用分段锁技术，提高并发访问性能，允许多个线程同时访问不同的段。 TreeMap：非线程安全，在多线程环境下若有多个线程同时修改，可能会导致数据混乱。 3. 性能特点\n查找、插入和删除操作 HashMap 和 HashSet：在理想情况下，基于哈希表的操作时间复杂度为 O(1)，但在哈希冲突严重时性能会下降。 HashTable：由于线程安全的实现方式（synchronized），在单线程环境下性能比 HashMap 差，操作时间复杂度与 HashMap 类似，但在多线程并发时会有锁竞争。 ConcurrentHashMap：并发性能较好，在高并发环境下，由于分段锁机制，不同段的操作可以并行进行，总体性能优于 HashTable。 TreeMap：基于红黑树，查找、插入和删除操作的时间复杂度为 O(logn)，因为需要维护树的平衡和排序。 4. 键和值的允许情况\nHashMap：键和值都可以为 null，但键的 null 只能有一个。 HashTable：键和值都不允许为 null，否则会抛出 NullPointerException。 ConcurrentHashMap：键和值都不允许为 null，否则会在多线程环境下导致不确定行为。 TreeMap：键不允许为 null（因为需要进行比较和排序），值可以为 null。 HashSet：允许一个 null 元素，不允许重复元素。 5. 排序功能\nHashMap、HashTable 和 HashSet：不具备排序功能，元素的存储顺序与插入顺序无关（HashMap 在 JDK 8 后引入了链表转红黑树机制，在一定程度上会影响元素顺序，但不是真正的排序）。 TreeMap：实现了 SortedMap 接口，会根据键的自然顺序或自定义比较器对键进行排序。 6. 应用场景\nHashMap 和 HashSet：适用于单线程环境下，对元素顺序无要求，只需要快速查找和存储不重复元素的场景，如缓存、数据统计等。 HashTable：由于性能问题，现在很少使用，仅在一些遗留代码或对线程安全有严格要求且性能要求不高的场景中可能会用到。 ConcurrentHashMap：适用于高并发环境下，需要线程安全的 Map 操作，如多线程的计数器、缓存等。 TreeMap：适用于需要对键进行排序，并且按照顺序遍历或查找元素的场景，如按照时间顺序存储和查询日志记录等。 15. HashSet与HashMap的区别 (1)HashSet实现了Set接口, 仅存储对象; HashMap实现了 Map接口, 存储的是键值对.\n(2)HashSet底层其实是用HashMap实现存储的, HashSet封装了一系列HashMap的方法. 依靠HashMap来存储元素值,(利用hashMap的key键进行存储), 而value值默认为Object对象. 所以HashSet也不允许出现重复值, 判断标准和HashMap判断标准相同, 两个元素的hashCode相等并且通过equals()方法返回true.\n16. HashTable与HashMap的区别 嗯，他们的主要区别是有几个吧\n第一，数据结构不一样，hashtable是数组+链表，hashmap在1.8之后改为了数组+链表+红黑树\n第二，hashtable存储数据的时候都不能为null，而hashmap是可以的\n第三，hash算法不同，hashtable是用本地修饰的hashcode值，而hashmap进行了二次hash\n第四，扩容方式不同，hashtable是当前容量翻倍+1，hashmap是当前容量翻倍\n第五，hashtable是线程安全的，操作数据的时候加了锁synchronized，hashmap不是线程安全的，效率更高一些\n在实际开中不建议使用HashTable，在多线程环境下可以使用ConcurrentHashMap类\n17. TreeMap与HashMap的区别 HashMap 和 TreeMap 都继承自 AbstractMap。但 TreeMap 还实现了 NavigableMap 和 SortedMap 接口，这是它与 HashMap 的主要区别。\nTreeMap 实现 NavigableMap 接口后：\n能定向搜索，像 ceilingEntry() 找大于等于键的最近元素，floorEntry() 找小于等于键的最近元素等。 可进行子集操作，如 submap()、headMap()、tailMap() 来获取子集视图，不复制全集合。 有逆序视图，descendingMap() 能反向迭代集合。 方便边界操作，firstEntry() 取首元素，lastEntry() 取尾元素，pollFirstEntry() 和 pollLastEntry() 取并移除首、尾元素。 这些基于红黑树实现，搜索时间复杂度 O(logn)。 实现 SortedMap 接口后，TreeMap 能按键排序，默认升序，也能自定义比较器。\n而 HashMap 基于哈希表，侧重快速键值对操作，无上述顺序相关功能，理想时操作时间复杂度 O(1)。\n18. ConcurrentHashMap 与 Hashtable 的区别 线程安全实现方式 Hashtable： 实现方式：使用 synchronized 方法来保证线程安全。 特点：所有操作（如 put、get）都是同步的，这意味着同一时间只有一个线程可以访问整个哈希表。 缺点：效率较低，尤其是在高并发场景下，因为所有线程都会竞争同一把锁，导致其他线程阻塞，性能下降。 ConcurrentHashMap： JDK 1.7： 实现方式：使用分段锁（Segment）来实现线程安全。 特点：将哈希表分成多个段（Segment），每个段是一个独立的锁。不同段的数据可以被不同线程并发访问，大大提高了并发性能。 优点：减少了锁竞争，提高了并发访问率。 JDK 1.8： 实现方式：取消了 Segment 的概念，改为使用 Node 数组 + 链表 + 红黑树的数据结构。 特点：使用 synchronized 和 CAS（Compare-And-Swap）操作来保证线程安全。锁粒度更细，synchronized 只锁定当前链表或红黑树的首节点。 优点：进一步减少了锁竞争，提高了并发性能。链表长度超过一定阈值时，链表会转换为红黑树，优化了查找性能。 底层数据结构 Hashtable： 数据结构：数组 + 链表。 特点：数组是哈希表的主体，链表用于解决哈希冲突。 缺点：在高并发场景下，所有操作都需要竞争同一把锁，导致性能瓶颈。 ConcurrentHashMap： JDK 1.7： 数据结构：Segment 数组 + HashEntry 数组 + 链表。 特点：每个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素。通过分段锁机制，不同段的数据可以被不同线程并发访问。 JDK 1.8： 数据结构：Node 数组 + 链表 + 红黑树。 特点：当链表长度超过一定阈值（默认为 8）时，链表会转换为红黑树，以优化查找性能。使用 synchronized 和 CAS 操作来保证线程安全，锁粒度更细。 并发度 Hashtable： 并发度：最大并发度为 1，因为所有操作都同步在同一个锁上。 特点：在高并发场景下，性能较差，因为所有线程都会竞争同一把锁。 ConcurrentHashMap： JDK 1.7： 并发度：最大并发度为 Segment 数组的大小，默认是 16。 特点：通过分段锁机制，不同段的数据可以被不同线程并发访问，提高了并发性能。 JDK 1.8： 并发度：最大并发度为 Node 数组的大小，通常比 JDK 1.7 更高。 特点：锁粒度更细，减少了锁竞争，进一步提高了并发性能。 19. JDK 1.7 和 JDK 1.8 的 ConcurrentHashMap 实现差异 线程安全实现方式 JDK 1.7： 实现方式：使用分段锁（Segment）来实现线程安全。 特点：每个 Segment 是一个独立的锁，不同 Segment 的数据可以被不同线程并发访问，减少了锁竞争。 JDK 1.8： 实现方式：取消了 Segment 的概念，改为使用 Node 数组 + 链表 + 红黑树。 特点：使用 synchronized 和 CAS 操作来保证线程安全，锁粒度更细，synchronized 只锁定当前链表或红黑树的首节点，进一步减少了锁竞争。 底层数据结构 JDK 1.7： 数据结构：Segment 数组 + HashEntry 数组 + 链表。 特点：每个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素。通过分段锁机制，不同段的数据可以被不同线程并发访问。 JDK 1.8： 数据结构：Node 数组 + 链表 + 红黑树。 特点：当链表长度超过一定阈值（默认为 8）时，链表会转换为红黑树，以优化查找性能。使用 synchronized 和 CAS 操作来保证线程安全，锁粒度更细。 性能优化 JDK 1.7： 优化：通过分段锁机制，减少了锁竞争，提高了并发性能。 JDK 1.8： 优化：锁粒度更细，进一步减少了锁竞争。链表转换为红黑树，优化了查找性能，特别是在高冲突情况下。 20. 为什么 ConcurrentHashMap 的 key 和 value 不能为 null 在 Java 集合框架中，ConcurrentHashMap是线程安全的哈希表实现，其设计上不允许key和value为null。\n1. 避免二义性\nnull作为一种特殊值，代表没有对象或引用。在ConcurrentHashMap中，若允许null作为key，将无法区分该key是实际存在于ConcurrentHashMap中且值为null，还是根本不存在此key。同理，对于value，也难以判断返回的null是真实存储的值，还是因未找到对应key而产生的结果。例如get方法返回null时，存在值不在集合中以及值本身为null这两种可能，这就是所谓的二义性。\n2. 多线程环境的复杂性\n在多线程环境下，当一个线程操作ConcurrentHashMap时，其他线程可能同时对其进行修改。这种情况下，无法依赖containsKey(key)方法来准确判断键值对是否存在。因为在调用containsKey方法之后，到依据该结果进行后续操作之前的这段时间内，ConcurrentHashMap可能已被其他线程修改，从而导致判断失误，使得二义性问题无法得到有效解决。\n3. 与 HashMap 对比\nHashMap则有所不同，它允许null作为key和value。其中null作为key只能有一个，null作为value可以有多个。在单线程环境中，由于不存在其他线程同时修改HashMap的情况，所以可以通过containsKey(key)方法准确判断键值对是否存在，进而做出相应处理，不会出现二义性问题。\n21. comparable 和 Comparator 的区别 接口来源和定义 comparable：接口出自java.lang包，它有一个compareTo(Object obj)方法用于排序。实现了Comparable接口的类，意味着该类的对象具有自然排序的能力。例如，String类、Integer类等都实现了Comparable接口，它们的对象可以直接进行比较和排序。 Comparator：接口出自java.util包，它有一个compare(Object obj1, Object obj2)方法用于排序。Comparator接口用于定义一种外部比较规则，当一个类没有实现Comparable接口，或者需要使用与自然排序不同的排序规则时，可以使用Comparator接口。 使用场景 comparable：一般用于对一个集合中的元素进行统一的自然排序。比如，要对一个ArrayList\u0026lt;Integer\u0026gt;进行升序排序，由于Integer类实现了Comparable接口，直接调用Collections.sort()方法即可按照自然顺序（升序）对列表中的元素进行排序。 Comparator：当需要对某一个集合实现自定义排序方式，或者需要对一个集合实现多种排序方式时使用。例如，有一个Song类，需要对其对象根据歌名和歌手名分别采用不同的排序方式，此时可以通过实现Comparator接口来定义不同的比较规则。可以创建两个不同的Comparator实现类，一个用于按歌名排序，一个用于按歌手名排序，然后在调用Collections.sort()方法时传入相应的Comparator实例来实现不同的排序需求。 二、多线程篇 1. 线程和进程的区别？ 进程：程序由指令和数据组成，但这些指令要运行，数据要读写，就必须将指令加载至 CPU，数据加载至内存。在指令运行过程中还需要用到磁盘、网络等设备。进程就是用来加载指令、管理内存、管理 IO 的。当一个程序被运行，从磁盘加载这个程序的代码至内存，这时就开启了一个进程。\n线程：一个进程之内可以分为一到多个线程。一个线程就是一个指令流，将指令流中的一条条指令以一定的顺序交给 CPU 执行\n二者对比\n（1）进程是正在运行程序的实例，进程中包含了线程，每个线程执行不同的任务\n（2）不同的进程使用不同的内存空间，在当前进程下的所有线程可以共享内存空间\n（3）线程更轻量，线程上下文切换成本一般上要比进程上下文切换低(上下文切换指的是从一个线程切换到另一个线程)\n2. 并行和并发有什么区别？ 现在都是多核CPU，在多核CPU下\n并发是同一时间应对多件事情的能力，多个线程轮流使用一个或多个CPU，每个线程在不同的时间片内获得 CPU 资源来执行任务。\n并行是同一时间动手做多件事情的能力，例如，一个 4 核 CPU 可以同时执行 4 个线程，每个线程在一个独立的 CPU 核心上运行，这些线程的执行是完全并行的，不会相互等待 CPU 资源。\n3. 创建线程的四种方式 在java中一共有四种常见的创建方式，分别是：继承Thread类、实现runnable接口、实现Callable接口、线程池创建线程。通常情况下，我们项目中都会采用线程池的方式创建线程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class MyExecutors implements Runnable{ @Override public void run() { System.out.println(\u0026#34;MyRunnable...run...\u0026#34;); } public static void main(String[] args) { // 创建线程池对象 ExecutorService threadPool = Executors.newFixedThreadPool(3); threadPool.submit(new MyExecutors()) ; // 关闭线程池 threadPool.shutdown(); } } 4. runnable 和 callable 有什么区别 Runnable 接口run方法没有返回值；Callable接口call方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果 Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。 Callable接口的call()方法允许抛出异常；而Runnable接口的run()方法的异常只能在内部消化，不能继续上抛 5. 线程的 run()和 start()有什么区别？ start(): 用来启动线程，通过该线程调用run方法执行run方法中所定义的逻辑代码。start方法只能被调用一次。\nrun(): 封装了要被线程执行的代码，可以被调用多次。\n6. 线程包括哪些状态，状态之间是如何变化的 在JDK中的Thread类中的枚举State里面定义了6种线程的状态分别是：新建、可运行、终结、阻塞、等待和有时限等待六种。\n关于线程的状态切换情况比较多。我分别介绍一下\n当一个线程对象被创建，但还未调用 start 方法时处于新建状态，此时，线程仅仅是在 Java 堆中分配了内存，操作系统还没有为其分配任何资源，线程尚未启动。调用了 start 方法，就会由新建进入可运行状态。如果线程内代码已经执行完毕，由可运行进入终结状态。当然这些是一个线程正常执行情况。\n当线程在获取对象的锁时，如果该锁已经被其他线程持有，线程会进入阻塞状态。线程会被放入该对象的 Monitor 阻塞队列中等待。只有当持锁线程释放锁时，会按照一定规则唤醒阻塞队列中的阻塞线程，唤醒后的线程进入可运行状态\n如果线程获取锁成功后，但由于条件不满足，调用了 wait() 方法，线程会释放当前持有的锁，并进入等待状态。当其它持锁线程调用 notify() 或 notifyAll() 方法，会恢复为可运行状态，等待获取锁并继续执行。\n还有一种情况是调用 sleep(long) 方法也会从可运行状态进入有时限等待状态，不需要主动唤醒，超时时间到自然恢复为可运行状态。\n7. 新建 T1、T2、T3 三个线程，如何保证它们按顺序执行？ 嗯~~，我思考一下 （适当的思考或想一下属于正常情况，脱口而出反而太假[背诵痕迹]）\n可以这么做，在多线程中有多种方法让线程按特定顺序执行，可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。\n比如说：\n使用join方法，T3调用T2，T2调用T1，这样就能确保T1就会先完成而T3最后完成\n8. notify()和 notifyAll()有什么区别？ notifyAll：唤醒所有wait的线程\nnotify：只随机唤醒一个 wait 线程\n9. 在 java 中 wait 和 sleep 方法的不同？ 共同点\nwait() ，wait(long) 和 sleep(long) 的效果都是让当前线程暂时放弃 CPU 的使用权，进入阻塞状态\n这三个方法都可以被其他线程通过调用 interrupt() 方法打断唤醒，此时会抛出 InterruptedException 异常，需要在代码中进行捕获和处理。\n不同点\n方法归属不同 sleep(long) 是 Thread 的静态方法 而 wait()，wait(long) 都是 Object类的成员方法，每个对象都有 醒来时机不同 执行 sleep(long) 和 wait(long) 的线程都会在等待相应毫秒后醒来 wait(long) 和 wait() 还可以被 notify 唤醒，wait() 如果不唤醒就一直等下去 它们都可以被打断唤醒 锁特性不同（重点） wait 方法的调用必须先获取 wait 对象的锁，也就是说 wait 方法必须在 synchronized 代码块或方法中使用。而 sleep 方法则没有这个限制，可以在任何地方调用。 wait 方法执行后会释放对象锁，允许其它线程获得该对象锁（我放弃 cpu，但你们还可以用） 而 sleep 如果在 synchronized 代码块中执行，并不会释放对象锁（我放弃 cpu，你们也用不了） 10. 如何停止一个正在运行的线程？ 有三种方式可以停止线程\n​\t使用退出标志，使线程正常退出，也就是当run方法完成后线程终止\n​\t使用stop方法强行终止（不推荐，方法已作废）\n​\t使用interrupt方法中断线程\n11. 讲一下synchronized关键字的底层原理？ synchronized 翻译成中文是同步的的意思，主要解决的是多个线程之间访问资源的同步性，可以保 证被它修饰的⽅法或者代码块在任意时刻只能有⼀个线程执行。\nsynchronized 属于悲观锁。底层使用的JVM级别中的Monitor 来决定当前线程是否获得了锁，如果某一个线程获得了锁，在没有释放锁之前，其他线程是不能或得到锁的。\nsynchronized 因为需要依赖于JVM级别的Monitor ，相对性能也比较低。\nMonitor 对象存在于每个 Java 对象的对象头中，所以每个 Java 对象都可以作为锁 。对象头是 Java 对象在内存中的一部分，用于存储对象的一些元数据信息，如哈希码、分代年龄等，同时也包含了指向 Monitor 的指针。当一个线程尝试获取某个对象的锁时，实际上就是在尝试获取该对象对应的 Monitor。\nmonitor内部维护了三个变量\nWaitSet：保存处于Waiting状态的线程，当一个线程在持有锁的情况下调用了对象的 wait() 方法，它会释放当前持有的锁，并进入 WaitSet 中等待。只有当其他线程调用了该对象的 notify() 或 notifyAll() 方法时，WaitSet 中的线程才有可能被唤醒，重新参与锁的竞争。 EntryList：保存处于Blocked状态的线程，当一个线程尝试获取已经被其他线程持有的锁时，它会被放入 EntryList 中进行阻塞等待。这些线程会在持有锁的线程释放锁后，被唤醒并参与锁的竞争。 Owner：用于记录当前持有锁的线程。当一个线程成功获取到 Monitor 时，Monitor 中的 Owner 会被设置为该线程，表示该线程成为了锁的持有者。 只有一个线程获取到的标志就是在monitor中设置成功了Owner，一个monitor中只能有一个Owner\n在上锁的过程中，如果有其他线程也来抢锁，则进入EntryList 进行阻塞，当获得锁的线程执行完了，释放了锁，就会唤醒EntryList 中等待的线程竞争锁，竞争的时候是非公平的。\n12. 讲一下synchronized关键字的使用方式？ 在 Java 里，synchronized 关键字主要有三种使用方式，目的是实现线程间的同步，保证同一时刻只有一个线程能访问被保护的代码或资源。\n修饰实例方法：给当前对象实例加锁。当一个线程调用该实例方法时，在进入同步代码之前，必须先获取这个对象实例的锁。只有获得锁后，线程才能执行方法里的代码；如果锁被其他线程占用，当前线程就会被阻塞，直到锁被释放。 修饰静态方法：给当前类加锁，这个锁会作用于类的所有对象实例。由于静态成员归整个类所有，被类的所有实例共享，所以进入同步代码前要获取当前 class 的锁。要注意，静态 synchronized 方法和非静态 synchronized 方法之间的调用不会互斥。比如线程 A 调用一个实例对象的非静态 synchronized 方法，同时线程 B 调用这个实例对象所属类的静态 synchronized 方法，这种情况是允许的，因为它们占用的锁不同，前者是当前实例对象锁，后者是当前类的锁。 **修饰代码块：**对括号里指定的对象或类加锁。 synchronized(object)：表示在进入同步代码块之前，需要获得给定对象的锁。只有拿到这个对象的锁，线程才能执行代码块中的内容。 synchronized(类.class)：意味着进入同步代码前要获得给定 Class 的锁。这种方式和修饰静态方法类似，都是对类进行加锁。 12. Monitor实现的锁属于重量级锁，你了解过锁升级吗？ Java中的synchronized有偏向锁、轻量级锁、重量级锁三种形式，分别对应了锁只被一个线程持有、不同线程交替持有锁、多线程竞争锁三种情况。\n偏向锁 偏向锁适用于在很长一段时间内都只有一个线程使用锁的场景。当一个线程第一次获取锁时，会通过 CAS 操作把自己的线程 ID 记录在对象头的 mark word 里。之后这个线程再次获取这把锁时，只需检查 mark word 中的线程 ID 是不是自己的，是的话就能直接获取锁，避免了频繁 CAS 操作的开销。\n轻量级锁 轻量级锁适用于不同线程交替持有锁，也就是线程加锁时间错开、没有锁竞争的情况。线程尝试获取轻量级锁时，会先在自己的栈帧中创建一个锁记录，然后用 CAS 操作把对象头的 mark word 复制到锁记录里，并让对象头指向锁记录。如果操作成功，就获取到了轻量级锁；如果失败，就自旋等待。轻量级锁主要通过修改对象头的锁标志实现，避免了用户态和内核态的切换，性能比重量级锁好很多。这就像两个人交替用一个房间，一个人来的时候在自己本子上记房间信息，把门口牌子指向自己本子，另一个人来发现牌子指向别人本子就等一会儿。\n重量级锁 重量级锁适用于多线程竞争锁的场景。它底层用 Monitor 实现，当线程竞争锁失败时会被阻塞，进入等待队列。这涉及到用户态和内核态的切换、进程的上下文切换，成本高，性能低。就像很多人同时抢一个房间，门口有管理员管理，竞争不到的人要去等待区，等房间空了管理员再叫人进来，这个过程很麻烦，效率低。\n锁升级机制 锁的状态会根据竞争情况升级，一旦出现竞争，会按照偏向锁 -\u0026gt; 轻量级锁 -\u0026gt; 重量级锁的顺序升级，而且锁升级是单向的，升为重量级锁后就不会再降级。\n13. 你谈谈 JMM（Java 内存模型） Java内存模型是Java虚拟机规范中定义的一种非常重要的内存模型。它的主要作用是描述Java程序中线程共享变量的访问规则，以及这些变量在JVM中是如何被存储和读取的，这些规则保证了多线程环境下数据的一致性和程序的正确性。\n这个模型有几个核心的特点。首先，所有的共享变量，包括实例变量（属于对象的变量）和类变量（用 static 修饰的变量），都被存储在主内存中，也就是计算机的RAM。主内存就像是一个公共仓库，所有线程都能访问它。需要注意的是，局部变量并不包含在内，因为它们是线程私有的，所以不存在竞争问题。\n其次，每个线程都有自己的工作内存，线程会把主内存中的共享变量复制一份到自己的工作内存中，形成工作副本。这意味着，线程对变量的所有操作，无论是读还是写，都必须在自己的工作内存中完成，而不能直接读写主内存中的变量。\n最后，不同线程之间不能直接访问对方工作内存中的变量。如果线程间需要传递变量的值，那么这个过程必须通过主内存来完成。threadA 先把修改后的值写回主内存，threadB 再从主内存读取这个值，这样就完成了变量的传递。\n14. CAS 你知道吗？ CAS，是一种无锁算法，用于实现多线程同步的原子操作\nCAS 是一种**乐观锁**策略，它假设在大多数情况下不会发生冲突，因此在操作共享资源时不会加锁，而是在更新数据时检查数据是否被其他线程修改过。\n原理\nCAS 操作包含三个操作数：内存位置（V）、预期原值（A）和新值（B）。执行时，它会先比较内存位置 V 的值是否与预期原值 A 相等，如果相等，就将内存位置 V 的值更新为新值 B；如果不相等，则不进行更新。整个操作是原子性的，由 CPU 硬件保证。\n优缺点\n优点：避免了锁的使用，减少了线程上下文切换的开销，在并发冲突较少时性能较高。\n缺点：存在 ABA 问题，即一个值从 A 变为 B 再变回 A，CAS 会认为值未改变；并且在并发冲突激烈时，线程会频繁自旋，消耗大量 CPU 资源。对于 ABA 问题，Java 提供了 AtomicStampedReference 类来解决。\n15. 什么是AQS？ 什么是 AQS？ AQS（AbstractQueuedSynchronizer）是 Java 提供的一个框架，用于构建锁和其他同步组件。它的核心思想是通过一个整数变量（state）来表示同步状态，并通过一个FIFO队列来管理线程的等待和唤醒。它是一个构建同步器的基础，提供了许多阻塞锁和同步工具的实现，如 ReentrantLock、Semaphore 和 CountDownLatch 等。\nAQS 的工作机制是什么？ AQS 通过维护一个 state 属性来表示资源的状态。它提供了一个基于 FIFO 的等待队列来管理获取锁的线程。当一个线程尝试获取锁时，如果锁不可用，该线程会被加入到等待队列中。AQS 还支持条件变量和相应的等待/唤醒机制。\nAQS 如何保证线程安全地修改 state？ AQS 使用 CAS（Compare-AndSwap）操作来安全地修改 state 属性，确保这一操作的原子性，避免多线程同时修改导致的问题。\nAQS 是公平锁还是非公平锁？ AQS 本身并不直接实现公平性或非公平性，但基于 AQS 实现的锁可以是公平或非公平的。例如，ReentrantLock 默认是非公平锁，但可以配置为公平锁。公平锁确保等待时间最长的线程优先获取锁。\nAQS 常见的实现类有哪些？ 常见的基于 AQS 实现的类包括：\nReentrantLock：一种可重入的互斥锁。 Semaphore：用于控制同时访问特定资源的线程数量。 CountDownLatch：允许一个或多个线程等待其他线程完成操作。 面试说：AQS是Java并发包中的一个基础框架，用于构建锁和其他同步器。它通过一个state变量来表示同步状态，并通过一个FIFO队列来管理线程的等待和唤醒。AQS有两种模式：独占模式和共享模式。独占模式下，同一时间只有一个线程可以获取锁；共享模式下，多个线程可以共享资源。AQS的核心在于它提供了一套标准化的方法来实现线程同步，使得开发者可以更方便地实现自己的同步器。\n16. ReentrantLock的实现原理 首先，ReentrantLock是一种可重入的排它锁，主要用来解决多线程对共享资源竞争的问题。\n它的核心特性有几个：\n它支持可重入，也就是获得锁的线程在释放锁之前再次去竞争同一把锁的时候，不需要加锁就可以直接访问。\n它支持公平和非公平特性\n它提供了阻塞竞争锁和非阻塞竞争锁的两种方法，分别是lock()和tryLock()。\n然后，ReentrantLock的底层实现有几个非常关键的技术。\n锁的竞争，ReentrantLock是通过互斥变量，使用CAS机制来实现的。\n没有竞争到锁的线程，使用了AbstractQueuedSynchronizer这样一个队列同步器来存储，底层是通过双向链表来实现的。当锁被释放之后，会从AQS队列里面的头部唤醒下一个等待锁的线程。\n公平和非公平的特性，主要是体现在竞争锁的时候，是否需要判断AQS队列存在等待中的线程。\n最后，关于锁的重入特性，在AQS里面有一个成员变量来保存当前获得锁的线程，当同一个线程下次再来竞争锁的时候，就不会去走锁竞争的逻辑，而是直接增加重入次数。\n17. synchronized和Lock有什么区别 ? 第一，语法层面\nsynchronized 是关键字，源码在 jvm 中，用 c++ 语言实现，退出同步代码块锁会自动释放 Lock 是接口，源码由 jdk 提供，用 java 语言实现，需要手动调用 unlock 方法释放锁 第二，功能层面\n二者均属于悲观锁、都具备基本的互斥、同步、锁重入功能 Lock 提供了许多 synchronized 不具备的功能，例如获取等待状态、公平锁、可打断、可超时、多条件变量，同时Lock 可以实现不同的场景，如 ReentrantLock， ReentrantReadWriteLock 第三，性能层面\n在没有竞争时，synchronized 做了很多优化，如偏向锁、轻量级锁，性能不赖 在竞争激烈时，Lock 的实现通常会提供更好的性能，比如可中断获取锁，可超时获取锁，公平锁的选择，多条件变量的实现。 Lock 相比 synchronized 所具备的这些独特功能： 1. 获取等待状态 synchronized 的局限：使用 synchronized 时，我们没办法知道当前有没有线程在等待获取这把锁，也不清楚等待的线程数量。就好比你去一个公共卫生间，用 synchronized 的话，你只能知道里面有没有人，但不知道外面排了多少人等着进去。 Lock 的优势：Lock 可以让我们获取到等待锁的线程的相关状态。以 ReentrantLock 为例，借助 getQueueLength() 方法就能知道有多少线程正在等待获取这把锁。这就好像卫生间门口有个显示屏，能显示排队的人数，让我们对整体情况有更清晰的了解。 2. 公平锁 synchronized 的情况：synchronized 实现的是非公平锁。这就好比大家在抢公交车的座位，不管谁先来，只要有空座，谁抢到就是谁的。可能先来的人反而没抢到座，后来的人却捷足先登了。 Lock 的公平锁：Lock 可以实现公平锁。公平锁就像是大家排队坐公交车，先到的人先上车有座坐，后到的人依次排队等待。ReentrantLock 可以在创建时通过构造函数指定为公平锁，即 new ReentrantLock(true)，这样线程获取锁的顺序就和它们请求锁的顺序一致了。 3. 可打断 synchronized 的问题：当一个线程使用 synchronized 获取锁时，如果这个锁被其他线程持有，当前线程就会一直等待，而且在等待过程中不能被其他线程打断。这就好比你在排队买东西，前面的人一直不买完，你就只能干等着，没办法中途离开去做别的事。 Lock 的可打断特性：Lock 提供了可打断的功能。使用 lockInterruptibly() 方法获取锁时，如果线程在等待锁的过程中被其他线程打断，它会抛出 InterruptedException 异常，然后线程可以去做其他事情。就像排队买东西时，你可以选择在等得不耐烦的时候，听到别人叫你去做别的事，你就可以离开队伍去处理其他事情。 4. 可超时 synchronized 的不足：synchronized 没有超时机制，线程一旦开始等待锁，就会一直等下去，可能会造成长时间的阻塞。这就好比你去餐厅吃饭，前面的人一直占着桌子不走，你只能一直等，没有别的选择。 Lock 的可超时功能：Lock 可以设置获取锁的超时时间。使用 tryLock(long timeout, TimeUnit unit) 方法时，如果在指定的时间内没有获取到锁，线程就会放弃等待，返回 false。这就像你去餐厅吃饭，等了一段时间桌子还没腾出来，你就可以选择去别的餐厅，不用一直干等着。 5. 多条件变量 synchronized 的单一性：synchronized 配合 wait()、notify() 和 notifyAll() 方法只能实现单一的条件等待和唤醒。就好比一个房间里只有一个门铃，不管是谁在等消息，门铃一响所有人都得醒来看是不是自己的消息。 Lock 的多条件变量：Lock 可以通过 newCondition() 方法创建多个条件变量。每个条件变量都可以单独进行等待和唤醒操作。线程可以根据不同的条件在不同的队列中等待，唤醒时也可以精确地唤醒特定条件下等待的线程，减少了不必要的唤醒和上下文切换，提高了线程间协作的效率。 18. 死锁产生的条件是什么？ 嗯，是这样的，一个线程需要同时获取多把锁，这时就容易发生死锁，举个例子来说：\nt1 线程获得A对象锁，接下来想获取B对象的锁\nt2 线程获得B对象锁，接下来想获取A对象的锁\n这个时候t1线程和t2线程都在互相等待对方的锁，就产生了死锁\n19. 如何进行死锁诊断？ 可以使用jdk自带的工具：jps和 jstack\n先通过jps来查看当前java程序运行的进程id\n然后通过jstack来查看这个进程id，就能展示出来死锁的问题，并且，可以定位代码的具体行号范围，我们再去找到对应的代码进行排查就行了。\n或者用一些可视化工具：jconsole、VisualVM，bin目录下 直接启动就行\n20. ConcurrentHashMap ConcurrentHashMap 是一种线程安全的高效Map集合，jdk1.7和1.8也做了很多调整。\nJDK1.7的底层采用是分段的数组+链表 实现 JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。 在jdk1.7中 ConcurrentHashMap 里包含一个 Segment 数组。Segment 的结构和HashMap类似，是一 种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，Segment 是一种可重入的锁 ReentrantLock，每个Segment 守护着一个HashEntry数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment的锁。\n在jdk1.8中的ConcurrentHashMap 做了较大的优化，性能提升了不少。首先是它的数据结构与jdk1.8的hashMap数据结构完全一致。其次是放弃了Segment臃肿的设计，取而代之的是采用Node + CAS + Synchronized来保证并发安全进行实现，synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发 , 效率得到提升\n在 JDK 1.8 中，ConcurrentHashMap将锁机制从Segment（继承自ReentrantLock）改为synchronized后效率得到提升，主要原因如下： 1. 锁粒度的减小\nJDK 1.7 的 Segment 锁：在 JDK 1.7 中，ConcurrentHashMap使用Segment数组来分段管理数据，每个Segment是一个可重入锁，并且守护着一个HashEntry数组。这意味着如果有多个线程访问不同Segment的数据，这些线程可以并发执行；但如果访问同一个Segment的数据，就会被阻塞。由于锁的粒度是Segment级别，而一个Segment可能包含多个键值对，在高并发场景下，不同线程竞争同一个Segment锁的概率较高，从而限制了并发性能。 JDK 1.8 的 synchronized 锁：JDK 1.8 中，ConcurrentHashMap放弃了Segment，直接使用Node数组来存储数据。当需要修改数据时，synchronized只锁定当前链表或红黑二叉树的首节点。如果多个线程访问的节点位于不同的链表或红黑二叉树，或者即使在同一个链表 / 树中但首节点不同，这些线程就可以并发执行。锁粒度从Segment（包含多个键值对）细化到了链表或树的首节点，大大降低了线程之间的锁竞争，提高了并发访问的效率。 2. CAS（Compare and Swap）操作的辅助\nCAS 操作原理：CAS 是一种无锁的原子操作，它包含三个操作数：内存位置、预期原值和新值。当且仅当内存位置的值与预期原值相同时，CAS 才会将内存位置的值更新为新值。在 JDK 1.8 的ConcurrentHashMap中，对于一些操作（如插入新节点），首先会尝试使用 CAS 操作来完成。只有在 CAS 操作失败（例如发生了哈希冲突，导致 CAS 更新操作失败）时，才会使用synchronized锁来保证操作的原子性。 提升效率方式：由于大部分情况下，插入操作不会发生哈希冲突，因此 CAS 操作能够快速完成，避免了使用锁带来的开销。这使得在高并发环境下，ConcurrentHashMap可以利用 CAS 的高效性来处理大部分的操作，仅在必要时使用synchronized锁，从而整体上提升了效率。 3. 红黑树的引入优化数据结构\n链表与红黑树的转换：JDK 1.8 的ConcurrentHashMap中，当链表长度超过一定阈值（8）时，链表会转换为红黑树。红黑树相比于链表，在查找、插入和删除操作上具有更好的时间复杂度（链表的查找时间复杂度为 O (n)，红黑树为 O (log n)）。 对锁效率的影响：当使用synchronized锁定首节点时，由于红黑树的结构特性，在处理大量数据时，即使存在锁竞争，基于红黑树的操作效率也比基于链表的操作效率更高。这意味着在相同的并发环境下，JDK 1.8 的ConcurrentHashMap能够更快地完成各种操作，从而提升了整体效率。 4. JVM 对 synchronized 的优化\n锁升级机制：JVM 在 JDK 1.6 及之后对synchronized进行了一系列优化，引入了偏向锁、轻量级锁和重量级锁的锁升级机制。当一个线程访问同步块时，首先会尝试获取偏向锁，如果没有其他线程竞争，该线程可以直接进入同步块，不需要进行 CAS 操作和重量级锁的获取，从而降低了锁的开销。只有在出现锁竞争时，才会逐步升级到轻量级锁和重量级锁。 在 ConcurrentHashMap 中的作用：在 JDK 1.8 的ConcurrentHashMap中，synchronized锁的使用正好可以利用 JVM 的这些优化机制。在大部分情况下，由于锁粒度小，竞争不激烈，synchronized能够以较低的开销工作，进一步提升了ConcurrentHashMap的性能。 在 Java 中，ConcurrentHashMap在不同版本中使用了不同的数据结构来实现并发安全的哈希映射，Segment数组和Node数组是其中两个重要的组成部分，以下是对它们的详细介绍： JDK 1.7 中的Segment数组\n结构与作用：ConcurrentHashMap在 JDK 1.7 中使用Segment数组来实现分段锁机制。每个Segment内部包含一个HashEntry数组，类似于HashMap的结构，它将整个哈希表分成多个段，每个段独立进行锁控制。这样，在多线程并发访问时，不同的线程可以同时访问不同的Segment，从而提高并发性能。 锁机制：Segment继承自ReentrantLock，当对Segment中的HashEntry数组进行修改操作（如插入、删除、更新等）时，需要先获取对应的Segment的锁。这意味着同一时刻只有一个线程可以访问同一个Segment中的数据，而不同Segment之间的操作可以并发进行。 JDK 1.8 中的Node数组\n结构与作用：JDK 1.8 中的ConcurrentHashMap摒弃了Segment数组的设计，采用了Node数组来存储数据。Node是哈希表中的基本节点类型，每个Node包含键值对以及指向下一个节点的引用（用于解决哈希冲突，形成链表或红黑树结构）。数组的每个位置要么是一个Node节点，要么是null，通过哈希值来确定键值对在数组中的存储位置。 锁机制与并发控制：在 JDK 1.8 中，ConcurrentHashMap通过Node + CAS + Synchronized来保证并发安全。当对Node数组进行操作时，首先会尝试使用CAS（比较并交换）操作来更新数组中的节点，这是一种无锁的并发操作方式，能在大多数情况下提高并发性能。对于一些复杂的操作，如在链表或红黑树中插入、删除节点时，会使用synchronized关键字来锁定当前链表或红黑树的首节点。这种方式相比 JDK 1.7 的Segment锁更加精细，只锁定当前操作的节点所在的链表或红黑树，而不是整个Segment，从而减少了锁的粒度，提高了并发访问的效率。 21. 导致并发程序出现问题的根本原因是什么 Java并发编程有三大核心特性，分别是原子性、可见性和有序性。\n首先，原子性指的是一个线程在CPU中的操作是不可暂停也不可中断的，要么执行完成，要么不执行。比如，一些简单的操作如赋值可能是原子的，但复合操作如自增就不是原子的。为了保证原子性，我们可以使用synchronized关键字或JUC里面的Lock来进行加锁。\n其次，可见性是指让一个线程对共享变量的修改对另一个线程可见。由于线程可能在自己的工作内存中缓存共享变量的副本，因此一个线程对共享变量的修改可能不会立即反映在其他线程的工作内存中。为了解决这个问题，我们可以使用synchronized关键字、volatile关键字或Lock来确保可见性。\n最后，有序性是指处理器为了提高程序运行效率，可能会对输入代码进行优化，导致程序中各个语句的执行先后顺序与代码中的顺序不一致。虽然处理器会保证程序最终执行结果与代码顺序执行的结果一致，但在某些情况下我们可能需要确保特定的执行顺序。为了解决这个问题，我们可以使用volatile关键字来禁止指令重排。\n22. 说一下线程池的核心参数（线程池的执行原理知道嘛） 在线程池中一共有7个核心参数：\ncorePoolSize 核心线程数目 - 池中会保留的最多线程数 maximumPoolSize 最大线程数目 - 核心线程+救急线程的最大数目 keepAliveTime 生存时间 - 救急线程的生存时间，生存时间内没有新任务，此线程资源会释放 unit 时间单位 - 救急线程的生存时间单位，如秒、毫秒等 workQueue - 当没有空闲核心线程时，新来任务会加入到此队列排队，队列满会创建救急线程执行任务 threadFactory 线程工厂 - 可以定制线程对象的创建，例如设置线程名字、是否是守护线程等 handler 拒绝策略 - 如果所有线程都在忙着（核心线程+临时线程），则走拒绝策略，拒绝策略有4种 1.AbortPolicy：直接抛出异常，默认策略；\n2.CallerRunsPolicy：用调用者所在的线程来执行任务；\n3.DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务；\n4.DiscardPolicy：直接丢弃任务；\n23. 线程池中有哪些常见的阻塞队列 Jdk中提供了很多阻塞队列，开发中常见的有两个：ArrayBlockingQueue和LinkedBlockingQueue\nArrayBlockingQueue和LinkedBlockingQueue是Java中两种常见的阻塞队列，它们在实现和使用上有一些关键的区别。\n首先，ArrayBlockingQueue是一个有界队列，它在创建时必须指定容量，并且这个容量不能改变。而LinkedBlockingQueue默认是无界的，但也可以在创建时指定最大容量，使其变为有界队列。\n其次，它们在内部数据结构上也有所不同。ArrayBlockingQueue是基于数组实现的，而LinkedBlockingQueue则是基于链表实现的。这意味着ArrayBlockingQueue在访问元素时可能会更快，因为它可以直接通过索引访问数组中的元素。而LinkedBlockingQueue则在添加和删除元素时可能更快，因为它不需要移动其他元素来填充空间。\n另外，它们在加锁机制上也有所不同。ArrayBlockingQueue使用一把锁来控制对队列的访问，这意味着读写操作都是互斥的。而LinkedBlockingQueue则使用两把锁，一把用于控制读操作，另一把用于控制写操作，这样可以提高并发性能。\n24. 如何确定核心线程数 在设置核心线程数之前，需要先熟悉一些执行线程池执行任务的类型\nIO密集型任务 一般来说：文件读写、DB读写、网络请求等\n推荐：核心线程数大小设置为2N+1 （N为计算机的CPU核数）\nCPU密集型任务 一般来说：计算型代码、Bitmap转换、Gson转换等\n推荐：核心线程数大小设置为N+1 （N为计算机的CPU核数）\n① 高并发、任务执行时间短 \u0026ndash;\u0026gt;（ CPU核数+1 ），减少线程上下文的切换，因为任务执行时间短，线程很快就能完成任务并释放资源，较少的线程数可以避免频繁的上下文切换带来的性能损耗。\n② 并发不高、任务执行时间长\nIO密集型的任务 \u0026ndash;\u0026gt; (CPU核数 * 2 + 1) 由于任务执行时间长且涉及大量 IO 操作，较多的线程可以在某个线程等待 IO 时继续执行其他任务，提高 CPU 的利用率。 计算密集型任务 \u0026ndash;\u0026gt; （ CPU核数+1 ）因为任务主要是计算工作，过多的线程会增加上下文切换的开销，所以保持与 CPU 核数相近的线程数较为合适。 ③ 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，首先要考虑业务中的某些数据是否可以做缓存，通过缓存可以减少不必要的计算和 IO 操作，提高系统的响应速度。其次，如果缓存无法满足需求，可以考虑增加服务器来分担压力。\n25. 线程池的种类有哪些 在jdk中默认提供了4种方式创建线程池\n第一个是：newCachedThreadPool创建一个可缓存线程池\n线程池中的线程数量是灵活可变的。当有新任务提交时，如果线程池中有空闲线程，就会复用这些空闲线程来执行任务；如果没有空闲线程，就会创建新的线程来处理任务。 对于空闲线程有一定的回收机制。如果某个线程在 60 秒内都处于空闲状态，就会被回收销毁，这样可以避免资源的浪费。 第二个是：newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 适用于需要控制并发线程数量的场景，例如数据库连接池，为了避免过多的线程同时访问数据库导致资源耗尽，可以使用定长线程池来限制并发线程数。\n第三个是：newScheduledThreadPool 创建一个定时任务线程池，支持定时及周期性任务执行。 适用于需要定时执行任务或周期性执行任务的场景，例如定时数据备份、定时统计系统性能指标等。\n第四个是：newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。\n26. 为什么不建议用Executors创建线程池 主要原因是如果使用Executors创建线程池的话，它允许的请求队列默认长度是Integer.MAX_VALUE，这样的话，有可能导致堆积大量的请求，从而导致OOM（内存溢出）。\n所以，我们一般推荐使用ThreadPoolExecutor来创建线程池，这样可以明确规定线程池的参数，避免资源的耗尽。\n27. 你们项目哪里用到了多线程 参考场景一：\nes数据批量导入\n在我们项目上线之前，我们需要把数据量的数据一次性的同步到es索引库中，但是当时的数据好像是1000万左右，一次性读取数据肯定不行（oom异常），如果分批执行的话，耗时也太久了。所以，当时我就想到可以使用线程池的方式导入，利用CountDownLatch+Future来控制，就能大大提升导入的时间。\n参考场景二：\n在我做那个xx电商网站的时候，里面有一个数据汇总的功能，在用户下单之后需要查询订单信息，也需要获得订单中的商品详细信息（可能是多个），还需要查看物流发货信息。因为它们三个对应的分别三个微服务，如果一个一个的操作的话，互相等待的时间比较长。所以，我当时就想到可以使用线程池，让多个线程同时处理，最终再汇总结果就可以了，当然里面需要用到Future来获取每个线程执行之后的结果才行\n参考场景三：\n《黑马头条》项目中使用的\n我当时做了一个文章搜索的功能，用户输入关键字要搜索文章，同时需要保存用户的搜索记录（搜索历史），这块我设计的时候，为了不影响用户的正常搜索，我们采用的异步的方式进行保存的，为了提升性能，我们加入了线程池，也就说在调用异步方法的时候，直接从线程池中获取线程使用\n28. 如何控制某个方法允许并发访问线程的数量？ 在jdk中提供了一个Semaphore[seməfɔːr]类（信号量）\n它提供了两个方法，semaphore.acquire() 请求信号量，可以限制线程的个数，是一个正数，如果信号量是-1,就代表已经用完了信号量，其他线程需要阻塞了\n第二个方法是semaphore.release()，代表是释放一个信号量，此时信号量的个数+1\n29. 谈谈你对ThreadLocal的理解 ThreadLocal 主要功能有两个，第一个是可以实现资源对象的线程隔离，让每个线程各用各的资源对象，避免争用引发的线程安全问题，第二个是实现了线程内的资源共享\n30. 知道ThreadLocal的底层原理实现吗？ 在ThreadLocal内部维护了一个一个 ThreadLocalMap 类型的成员变量，用来存储资源对象\n当我们调用 set 方法，就是以 ThreadLocal 自己作为 key，资源对象作为 value，放入当前线程的 ThreadLocalMap 集合中\n当调用 get 方法，就是以 ThreadLocal 自己作为 key，到当前线程中查找关联的资源值\n当调用 remove 方法，就是以 ThreadLocal 自己作为 key，移除当前线程关联的资源值\n31. 关于ThreadLocal会导致内存溢出这个事情，了解吗？ ThreadLocalMap 中的 Entry 类的key 是一个弱引用，而 value 是一个强引用。弱引用的特点是，当垃圾回收器进行垃圾回收时，如果一个对象只被弱引用所引用，那么无论当前内存是否充足，该对象都会被回收。所以，当外部对 ThreadLocal 对象的强引用被释放后，ThreadLocalMap 中的 key 会被垃圾回收器回收，即 key 变为 null。\n然而，value 是强引用，只要当前线程还存在，ThreadLocalMap 就不会被回收，value 也不会被回收。这样就会导致 ThreadLocalMap 中存在一些 key 为 null，但 value 不为 null 的 Entry，这些 Entry 无法被访问到，却占用着内存，随着时间的推移，可能会导致内存泄漏。\n解决办法 为了避免 ThreadLocal 导致的内存泄漏问题，我们应该在使用完 ThreadLocal 后，及时调用 remove 方法。\n另外，在使用 ThreadLocal 时，尽量避免将其作为静态变量使用，因为静态变量的生命周期和类的生命周期一样长，可能会导致 ThreadLocal 对象一直存在，增加内存泄漏的风险。同时，在使用线程池时，由于线程会被复用，更要注意及时调用 remove 方法，防止 ThreadLocal 中的数据在不同任务之间产生混淆，也避免内存泄漏问题的发生。\n32. 说说线程的生命周期和状态? Java 线程具有 6 种不同的生命周期状态，分别为：\nNEW（初始状态）：线程已创建，但尚未调用start()方法启动。 RUNNABLE（运行状态）：调用start()方法后进入该状态，此时线程等待获取 CPU 资源以执行。 BLOCKED（阻塞状态）：线程在获取对象锁时被阻塞，需等待锁的释放。 WAITING（等待状态）：线程需要等待其他线程执行特定动作（如通知或中断），会一直等待直到收到相应信号。 TIME_WAITING（超时等待状态）：与 WAITING 类似，但可在指定时间后自行返回，无需一直等待。 TERMINATED（终止状态）：线程执行完毕，生命周期结束。 线程在运行过程中，会依据代码的执行情况在这些状态之间进行切换，并非固定处于某一状态 。\n33. 线程池如何知道一个线程的任务已经执行完成 线程池判断一个线程的任务是否执行完成，可从线程池内部机制和线程池外部获取状态两个方面来理解：\n线程池内部机制\n当向线程池提交任务后，线程池会调度工作线程执行任务的run方法。工作线程通过同步调用任务的run方法，等待run方法返回，一旦run方法正常结束，即表明任务完成，此时工作线程会统计任务的完成数量。\n线程池外部获取状态\n使用isTerminated()方法：线程池提供了isTerminated()方法用于判断其运行状态。通过循环判断该方法的返回结果，可了解线程池状态。当线程池状态为Terminated时，意味着所有任务都已执行完毕。不过，使用此方法的前提是程序中主动调用了线程池的shutdown()方法。在实际业务中，主动关闭线程池的情况并不常见，所以该方法在实用性和灵活性上有所欠缺。 利用submit()方法的Future返回值：线程池的submit()方法会返回一个Future对象。通过调用Future.get()方法可获取任务的执行结果。在任务未执行完成前，Future.get()方法会一直阻塞，直至任务执行结束正常返回，这也就表明传入线程池的任务已执行完成。 借助CountDownLatch计数器：CountDownLatch可以通过初始化指定一个计数器进行倒计时，其有await()方法用于阻塞线程，以及countDown()方法用于倒计时。基于此原理，可定义一个计数器为 1 的CountDownLatch对象，在线程池代码块后面调用await()方法阻塞主线程。当传入线程池的任务执行完成后，调用countDown()方法表示任务结束。此时计数器归零，被阻塞在await()方法的线程将被唤醒。 总的来说，无论是在线程池内部还是外部，要知晓线程是否执行结束，关键在于获取线程执行结束后的状态。由于线程本身没有返回值，所以常通过阻塞 - 唤醒的方式来达成，Future.get和CountDownLatch都是基于这一原理。\n34. 线程池有哪几种状态 每种状态分别表示什么 在 Java 线程池中，线程池共有以下 5 种状态，每种状态都有其特定的含义和行为：\nRUNNING（运行状态）：线程池处于正常运行状态，可以接受新任务，并处理已提交的任务。这是线程池创建后的初始状态。在这种状态下，线程池会根据任务队列和线程数量的情况，动态地创建和管理工作线程来执行任务。 SHUTDOWN（关闭状态）：当调用线程池的 shutdown() 方法后，线程池进入此状态。此时，线程池不再接受新任务，但会继续处理已提交到任务队列中尚未执行的任务。只有当任务队列中的所有任务都处理完成后，线程池才会进入下一个状态。在处理任务的过程中，线程池不会中断正在执行任务的线程，而是等待任务自然结束。 STOP（停止状态）：当调用线程池的 shutdownNow() 方法后，线程池进入此状态。与 SHUTDOWN 状态不同，STOP 状态下线程池不仅不再接受新任务，还会尝试立即停止所有正在执行的任务，并且会中断等待任务队列中任务的线程。也就是说，处于 STOP 状态的线程池会立即终止所有的工作线程，而不会等待任务完成。 TIDYING（整理状态）：当线程池中的所有任务都已执行完毕，并且工作线程数量为 0 时，线程池会自动进入 TIDYING 状态。在进入该状态时，线程池会调用 terminated() 方法，这个方法可以由用户自定义实现，用于在线程池关闭前进行一些清理和资源释放的操作。 TERMINATED（终止状态）：当 terminated() 方法执行完毕后，线程池就会进入 TERMINATED 状态。此时，线程池已经完全关闭，不再有任何活动，所有的资源也都已释放。 悲观锁与乐观锁：概念、实现及应用场景 一、悲观锁\n（一）定义与原理\n悲观锁总是假设最坏的情况，即共享资源每次被访问时，极有可能发生冲突。所以，它在每次操作共享资源前，都会先获取锁，确保同一时刻只有一个线程能访问该资源，其他线程只能阻塞等待，直到持有锁的线程释放锁资源。这种策略如同在一个公共资源前设置了一道关卡，每次只允许一个人通过，其他人必须排队等待。\n（二）Java 中的实现方式\nsynchronized 关键字：这是 Java 内置的一种同步机制，当一个线程进入被synchronized修饰的代码块或方法时，它会自动获取对象锁（若是静态方法，则获取类锁）。例如： 1 2 3 4 5 6 7 public class SynchronizedExample { private int sharedResource; public synchronized void performTask() { // 需要同步的操作 sharedResource++; } } 在上述代码中，performTask方法被synchronized修饰，当一个线程执行该方法时，其他线程若也想执行此方法，就会被阻塞，直到当前线程执行完毕并释放锁。\nReentrantLock 类：这是 Java 并发包java.util.concurrent.locks中提供的一个可重入的互斥锁。使用时，需要先创建ReentrantLock对象，然后在需要同步的代码块前后分别调用lock()和unlock()方法。通常，为了确保锁一定能被释放，会将unlock()方法放在finally块中。示例如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; public class ReentrantLockExample { private int sharedResource; private Lock lock = new ReentrantLock(); public void performTask() { lock.lock(); try { // 需要同步的操作 sharedResource++; } finally { lock.unlock(); } } } ReentrantLock相较于synchronized，提供了更灵活的锁控制，如可中断的锁获取、公平锁与非公平锁的选择等。\n（三）优缺点及适用场景\n优点：悲观锁能够严格保证数据的一致性和线程安全性，因为它通过加锁机制避免了多个线程同时修改共享资源的情况。 缺点 性能开销大：在高并发场景下，由于大量线程竞争锁资源，会导致频繁的线程阻塞和上下文切换。线程阻塞意味着线程需要等待锁，这期间线程处于空闲状态，白白占用系统资源；而上下文切换则是指操作系统在不同线程之间切换执行环境，这一过程需要保存和恢复线程的状态信息，也会消耗一定的时间和资源。 死锁风险：如果多个线程获取锁的顺序不当，就可能出现死锁现象。例如，线程 A 持有锁 1，等待获取锁 2，而线程 B 持有锁 2，等待获取锁 1，此时两个线程都无法继续执行，形成死锁，严重影响系统的正常运行。 适用场景：适用于写操作频繁、数据一致性要求极高且并发冲突可能性较大的场景。比如，在银行转账系统中，对账户余额的修改操作就需要使用悲观锁，以确保在同一时刻只有一个转账操作能对账户余额进行修改，避免出现数据不一致的情况。 二、乐观锁\n（一）定义与原理\n乐观锁秉持乐观的态度，总是假设共享资源在每次被访问时不会出现问题，线程可以自由地执行操作，无需加锁等待。它仅在提交修改时，才会去验证对应的共享资源是否被其他线程修改过。若未被修改，则提交成功；若已被修改，则重试操作，直至成功提交。这种策略就像是在一个相对和谐的环境中，大家都先假设自己的操作不会与他人冲突，各自先进行工作，最后再检查是否有冲突发生。\n（二）Java 中的实现方式\n原子变量类（基于 CAS 算法）\n在 Java 的java.util.concurrent.atomic包中，提供了一系列原子变量类，如AtomicInteger、AtomicLong、LongAdder等，它们都是通过 CAS（Compare And Swap，比较与交换）算法实现了乐观锁机制。\nCAS 算法原理：CAS 操作涉及三个操作数，分别是要更新的变量值（Var）、预期值（Expected）和拟写入的新值（New）。当且仅当 Var 的值等于 Expected 时，CAS 才会通过原子方式用 New 值来更新 Var 的值。若两者不相等，说明在当前线程操作期间，已经有其他线程修改了 Var 的值，当前线程则放弃更新。例如，假设有一个AtomicInteger对象atomicInt，其初始值为 5，线程 A 希望将其值更新为 10，此时线程 A 会先获取atomicInt的当前值（即 5）作为预期值，然后尝试将其更新为 10。若在更新前没有其他线程修改atomicInt的值，那么更新操作会成功；若有其他线程已经修改了atomicInt的值，更新操作则失败，线程 A 可以选择再次尝试更新。 LongAdder 类的特殊优化：在高并发场景下，LongAdder类相比AtomicInteger和AtomicLong具有更好的性能。LongAdder内部采用了分段累加的方式，将对一个变量的操作分散到多个不同的单元格（Cell）中，每个线程在对变量进行操作时，会先访问不同的单元格，减少了线程间的竞争。不过，这种方式会消耗更多的内存空间，因为它需要为每个单元格分配内存，本质上是用空间换取时间。例如： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import java.util.concurrent.atomic.LongAdder; public class LongAdderExample { private static LongAdder sum = new LongAdder(); public static void main(String[] args) { Thread[] threads = new Thread[10]; for (int i = 0; i \u0026lt; 10; i++) { threads[i] = new Thread(() -\u0026gt; { for (int j = 0; j \u0026lt; 1000; j++) { sum.increment(); } }); threads[i].start(); } for (Thread thread : threads) { try { thread.join(); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(\u0026#34;Sum: \u0026#34; + sum.sum()); } } 在上述代码中，通过多个线程同时对LongAdder对象sum进行累加操作，展示了LongAdder在高并发环境下的高效性。\n版本号机制 在数据库操作中，常使用版本号机制来实现乐观锁。一般在数据表中添加一个数据版本号version字段，用于表示数据被修改的次数。当数据被修改时，version值会自动加 1。线程在读取数据的同时，也会读取version值，在提交更新时，只有当读取到的version值与当前数据库中的version值相等时，才会执行更新操作；否则，会重试更新操作，直到更新成功。例如，假设有一个用户信息表user_info，其中包含id、name、balance和version字段，当前有一条记录id = 1，name = \u0026quot;张三\u0026quot;，balance = 1000，version = 1。\n线程 A 读取该记录，此时version = 1，并对balance进行修改，将其值减少 100，即balance = 900。 在线程 A 操作过程中，线程 B 也读取了该记录，version同样为 1，并对balance进行修改，将其值减少 200，即balance = 800。 线程 A 完成修改后，提交更新操作，此时会比对读取的version值（1）和数据库中当前记录的version值（假设此时数据库中该记录的version值仍为 1，因为线程 B 还未提交更新），两者相等，更新操作成功，数据库中该记录的version值更新为 2，balance值更新为 900。 线程 B 完成操作后，提交更新操作，此时比对读取的version值（1）和数据库中当前记录的version值（2），两者不相等，更新操作失败，线程 B 需要重新读取数据并进行修改和提交，直到更新成功。 （三）优缺点及适用场景\n优点 高并发性能优越：由于乐观锁在大多数情况下不需要加锁，线程可以自由执行，避免了线程阻塞和上下文切换带来的性能开销，因此在高并发且读操作远多于写操作的场景中，性能表现出色。 无死锁问题：乐观锁不需要显式地获取和释放锁，不存在因获取锁顺序不当而导致的死锁问题，提高了系统的稳定性。 缺点 频繁重试开销：在写操作频繁的场景下，由于数据被其他线程修改的概率较高，可能会导致大量的更新操作失败并需要重试，这会消耗大量的 CPU 资源，严重影响系统性能，甚至可能导致 CPU 使用率飙升。 适用范围有限：乐观锁主要适用于对单个共享变量的操作，对于涉及多个共享变量的复杂操作，使用乐观锁可能无法保证数据的一致性。 适用场景：适用于读多写少的场景，例如在一些缓存系统中，数据的读取操作远远多于写入操作，使用乐观锁可以在保证数据一致性的前提下，极大地提高系统的并发性能。 三、MySQL篇 1. MySQL中，如何定位慢查询？ 在 MySQL 中，我们可以通过开启慢查询日志来定位执行缓慢的查询。具体操作是在 MySQL 的配置文件中设置 slow_query_log=1 来开启慢查询日志，并通过 long_query_time=2 参数设置慢查询的阈值，例如超过 2 秒。日志默认存放在 MySQL 数据目录下，文件名为 slow.log。通过分析这个日志文件，我们可以识别出哪些查询操作的性能不佳，进而进行优化。\u0026quot;\n除了慢查询日志，我们还可以使用 SQL Profile 工具来获取 SQL 语句的详细执行信息。在会话中开启 profiling 功能（SET profiling = 1;），执行 SQL 语句后，通过 SHOW PROFILES; 命令可以查看该语句的执行细节，包括消耗的时间、扫描的行数等，这有助于我们分析和优化慢查询。\n2. 那这个SQL语句执行很慢，如何分析呢？ 如果一条SQL执行很慢，我们通常会使用MySQL的EXPLAIN命令来分析这条SQL的执行情况。通过key和key_len可以检查是否命中了索引，如果已经添加了索引，也可以判断索引是否有效。通过type字段可以查看SQL是否有优化空间，比如是否存在全索引扫描或全表扫描。通过extra建议可以判断是否出现回表情况，如果出现，可以尝试添加索引或修改返回字段来优化。\n3. 了解过索引吗 嗯，索引在项目中非常常见，它是一种帮助MySQL高效获取数据的数据结构，主要用来提高数据检索效率，降低数据库的I/O成本。同时，索引列可以对数据进行排序，降低数据排序的成本，也能减少CPU的消耗。\n4. 索引的底层数据结构了解过吗？ MySQL的默认存储引擎InnoDB使用的是B+树作为索引的存储结构。选择B+树的原因包括：节点可以有更多子节点，路径更短；磁盘读写代价更低，非叶子节点只存储键值和指针，叶子节点存储数据；B+树适合范围查询和扫描，因为叶子节点形成了一个双向链表。\n5. B树和B+树的区别是什么呢？ B树的非叶子节点和叶子节点都存放数据，而B+树的所有数据只出现在叶子节点，这使得B+树在查询时效率更稳定。 B+树在进行范围查询时效率更高，因为所有数据都在叶子节点，并且叶子节点之间形成了双向链表。 6. 什么是聚簇索引什么是非聚簇索引？ 在数据库中，索引分为聚簇索引和非聚簇索引两种类型。聚簇索引，B+树的叶子节点保存了整行数据，通常只有一个聚簇索引，一般是由主键构成。非聚簇索引则存储的是数据的引用，B+树的叶子节点保存的是主键值，可以有多个非聚簇索引，通常我们自定义的索引都是非聚簇索引。\n7. 知道什么是回表查询吗？ 回表查询是指通过二级索引找到对应的主键值，然后再通过主键值查询聚簇索引中对应的整行数据的过程。\n8. 知道什么叫覆盖索引吗？ 覆盖索引是指在SELECT查询中，返回的列全部能在索引中找到，避免了回表查询，提高了性能。使用覆盖索引可以减少对主键索引的查询次数，提高查询效率。\n9. MySQL超大分页怎么处理？ 超大分页通常发生在数据量大的情况下，使用LIMIT分页查询且需要排序时效率较低。可以通过覆盖索引和子查询来解决。首先查询数据的ID字段进行分页，然后根据ID列表用子查询来过滤只查询这些ID的数据，因为查询ID时使用的是覆盖索引，所以效率可以提升。\n10. 索引创建原则有哪些？ 创建索引的原则包括：\n表中的数据量超过10万以上时考虑创建索引。 选择查询频繁的字段作为索引，如查询条件、排序字段或分组字段。 尽量使用复合索引，覆盖SQL的返回值。 如果字段区分度不高，可以将其放在组合索引的后面。 对于内容较长的字段，考虑使用前缀索引。 控制索引数量，因为索引虽然可以提高查询速度，但也会影响插入、更新的速度。 11. 什么情况下索引会失效？ 索引失效可能发生在几种情况下。首先，如果查询条件没有遵循最左前缀法则，即没有从复合索引的最左边列开始，索引可能不会生效。其次，如果对字符串类型的索引列使用了以 % 开头的 LIKE 查询，索引也会失效。此外，如果在索引列上进行了运算或类型转换，比如数学运算或函数操作，索引同样会失效。对于复合索引，如果在索引的中间列使用了范围查询，那么该列右边的所有列索引都将失效。\n12. SQL的优化经验有哪些？ 建表时选择合适的字段类型。 使用索引，遵循创建索引的原则。 编写高效的SQL语句，比如避免使用SELECT *，尽量使用UNION ALL代替UNION，以及在表关联时使用INNER JOIN。 查询优化：使用 EXPLAIN 分析查询执行计划，确保查询有效使用索引。 避免返回过多数据：使用覆盖索引，减少全表扫描。 在数据量大时考虑分库分表。 13. 创建表的时候，你们是如何优化的呢？ 选择合适的数据类型：根据数据的特性选择最合适的数据类型，例如，对于小整数使用 TINYINT，大整数使用 INT 或 BIGINT，字符串根据长度选择 CHAR、、VARCHAR或TEXT`。 定义字符集：选择适当的字符集，如 utf8，以支持多语言文本。 定义长度：对于可变长字段，定义合理的长度，避免不必要的空间浪费。 创建索引：为查询、排序或连接操作频繁的列创建索引，包括主键和外键。 规范化设计：应用数据库规范化原则，减少数据冗余，提高数据一致性。 分区：对于大数据量的表，考虑分区来提高查询和管理效率。 外键：使用外键来维护表之间的关系，确保引用完整性。 默认值：为具有默认值的列定义默认值。 避免冗余：设计表结构时避免重复存储相同数据。 性能测试：在开发环境中对表结构进行性能测试，确保查询和更新操作的效率。 14. 在使用索引的时候，是如何优化呢？ 在使用索引时，我们遵循索引创建原则，确保索引字段是查询频繁的，使用复合索引覆盖SQL返回值，尽量避免导致回表查询的查询条件，以减少额外的 I/O 开销。\n15. 你平时对SQL语句做了哪些优化呢？ 首先，我会避免使用 SELECT *，而是明确指定所需的字段，这有助于减少数据传输并提高查询效率。其次，我会确保在查询条件列上建立索引，并在查询中尽量使用这些索引。此外，我会优化 JOIN 操作，尽量使用 INNER JOIN，因为它在内连接时会过滤不符合条件的记录，减少需要处理的数据量。如果必须使用 LEFT JOIN 或 RIGHT JOIN，确保小表作为驱动表，即左连接的小表在左边，右连接的小表在右边。\n我还会使用 LIMIT 进行分页查询，并尽量减少 OFFSET 的使用，以提高查询性能。同时，我会创建覆盖索引以减少回表查询的需要。此外，我会重写复杂的子查询为连接查询，减少查询的复杂度和执行时间。\n16. 事务的特性是什么？可以详细说一下吗？ 事务的特性是ACID，即原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。例如，A向B转账500元，这个操作要么都成功，要么都失败，体现了原子性。转账过程中数据要保持一致，A扣除了500元，B必须增加500元。隔离性体现在A向B转账时，不受其他事务干扰。持久性体现在事务提交后，数据要被持久化存储。\n17. 并发事务带来哪些问题？ 脏读（Dirty Read）： 脏读发生在一个事务读取了另一个未提交事务修改的数据。如果该修改被回滚，那么读取的数据就是无效的。 不可重复读（Non-Repeatable Read）： 这发生在一个事务中多次读取同一数据，由于其他事务的修改，导致读取的数据在事务内出现不一致。 幻读（Phantom Read）： 幻读是指在一个事务中，两次读取同一范围的数据，第二次读取的结果包含了另一个并发事务提交的新数据。 丢失更新（Lost Update）： 当两个或多个事务同时更新同一行数据时，一个事务的更新可能被另一个事务的更新覆盖，导致数据丢失。 死锁（Deadlock）： 当两个或多个事务相互等待对方释放资源时，可能导致死锁，这会阻塞事务的进一步执行。 长时间运行的事务： 长时间运行的事务可能会锁定大量资源，这会阻塞其他事务，影响数据库性能。 系统资源竞争： 并发事务可能导致数据库的资源（如锁、日志空间、内存等）竞争，影响系统稳定性。 18. 怎么解决这些问题呢？MySQL的默认隔离级别是？ MySQL 的 InnoDB 存储引擎默认隔离级别是 可重复读（REPEATABLE READ）\n脏读：可将事务隔离级别设置为 “读已提交（READ COMMITTED）” 及以上级别来防止，这样事务只能读取到已提交的数据，避免读取到未提交的脏数据。 不可重复读：通常需要将事务隔离级别设置为 “可重复读（REPEATABLE READ）” 或更高的 “串行化（SERIALIZABLE）” 级别。在 “可重复读” 隔离级别下，事务在第一次读取数据时会对数据加锁，在事务提交前，其他事务无法修改该数据，从而保证在同一事务内多次读取同一数据的结果是一致的。 幻读：通常将事务隔离级别设置为 “串行化（SERIALIZABLE）” 可避免，此级别会对事务操作的范围数据加锁，阻止其他事务在该范围内插入或删除数据，但会降低并发性能。 丢失更新：可使用排他锁或乐观锁机制，确保在更新数据时只有一个事务能成功执行更新操作，防止更新被覆盖。 死锁：合理设计事务逻辑，避免事务之间循环等待资源；也可设置死锁检测机制，当检测到死锁时自动回滚其中一个事务来打破死锁。 长时间运行的事务：优化事务逻辑，减少不必要的操作和资源占用；对于确实需要长时间运行的事务，可适当调整资源配置和数据库参数。 系统资源竞争：合理配置数据库资源，根据并发量等情况调整缓存大小、连接数等参数；优化事务执行顺序，减少资源竞争。 19. 事务是怎么实现的，主要通过哪些机制来确保其 ACID 特性？ 事务的实现主要依赖于几个关键机制：锁机制、日志系统（包括 redo log 和 undo log），以及 MVCC（多版本并发控制）。这些机制共同协作以确保事务的 ACID 特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。\n锁机制： 锁机制用于控制数据的并发访问，确保在事务进行中数据不会被其他事务修改，从而满足隔离性。根据锁的粒度不同，可以是行级锁、表级锁等。 redo log（重做日志）： redo log 记录了事务对数据所做的修改。在系统崩溃后，可以通过重放这些日志来恢复数据，从而满足持久性要求。 undo log（回滚日志）： undo log 记录了事务执行前的数据状态，用于在事务回滚时恢复到事务开始前的状态，满足原子性和隔离性。 MVCC（多版本并发控制）： MVCC 是一种并发控制机制，它通过为每个事务提供一个数据的一致性视图来满足隔离性。这样，即使数据被其他事务修改，当前事务也看不到这些修改，从而避免了脏读和不可重复读的问题。 20. undo log和redo log的区别是什么？ undo log（撤销日志）和redo log（重做日志）\nundo log 主要用于事务回滚时恢复数据，它记录了事务更改之前的数据状态，确保我们可以撤销事务对数据的修改，保持数据的一致性。这有助于维护事务的原子性，即事务要么完全执行，要么完全不执行。\n相反，redo log 用于在系统故障后恢复数据，它记录了事务提交时的数据更改，确保这些更改在系统重启后可以被重新应用，从而保障事务的持久性。这有助于确保即使在系统崩溃的情况下，已提交的事务更改也不会丢失。\n21. 那事务的隔离级别有哪些，它们有什么区别？ 事务的隔离级别通常有四种，由低到高分别是：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）、串行化（Serializable）。\n读未提交：允许事务读取其他事务未提交的数据，可能会导致脏读。 读已提交：只能读取已经提交的数据，避免了脏读，但可能会出现不可重复读。 可重复读：保证了在同一事务中多次读取同样的数据结果是一致的，避免了不可重复读，但可能会出现幻读。 串行化：最高的隔离级别，通过强制事务串行执行来避免脏读、不可重复读和幻读，但性能开销也最大。 22. 事务中的隔离性是如何保证的呢？（你解释一下MVCC） 事务的隔离性是指当多个事务并发执行时，数据库如何管理这些事务对数据的访问，以防止数据不一致性的问题。MySQL 使用多版本并发控制（MVCC）来提供事务隔离。\n事务的隔离性通过锁和多版本并发控制（MVCC）来保证。MVCC通过维护数据的多个版本来避免读写冲突。底层实现包括隐藏字段、undo log和read view。隐藏字段包括trx_id和roll_pointer。trx_id，它是一个事务ID，用于标识对数据行进行更改的事务。undo log记录了不同版本的数据，通过roll_pointer形成版本链。read view定义了不同隔离级别下的快照读，决定了事务访问哪个版本的数据。\n23. MySQL主从同步原理是什么？ MySQL的主从同步是一种数据库复制技术，它允许将一个数据库服务器（主库）的数据变更复制到另一个或多个服务器（从库）上。\nMySQL主从复制的核心是二进制日志（Binlog）。步骤如下：\n主库在事务提交时记录数据变更到Binlog。 从库连接到主库并读取 Binlog，将 Binlog 中的事件写入到自己的中继日志（Relay Log）。 从库根据中继日志中的事件，重放（Replay）这些事件到自己的数据中，从而实现数据的同步。 24. 你们项目用过MySQL的分库分表吗？ 垂直分库：垂直分库是按照业务将不同的表拆分到不同的数据库中\n我们采用微服务架构，每个微服务对应一个数据库，是根据业务进行拆分的，这个其实就是垂直拆分。\n水平分库：水平分库是将数据按照一定的规则（如用户 ID 取模、哈希等）分布到不同的数据库中。比如，根据用户 ID 对 10 取模，将用户数据分布到 10 个不同的数据库中，每个数据库都保存着完整的数据表结构\n垂直分表：垂直分表是将一张表按照列的相关性拆分成多张表。例如，将一个包含大量字段的用户表，拆分为用户基本信息表和用户扩展信息表。垂直分表适合表中存在不常用并且占用了大量空间的表拆分出去。\n水平分表：水平分表是将一张表的数据按照行进行拆分。例如按照用户 ID 的范围或者哈希值将数据拆分到不同的表中。水平分表就适合用户表行数很多的情况下，一般单表行数超过5000万就得分表，如果单表的数据比较复杂那可能2000万甚至1000万就得分了。\n四、Redis 一、Redis 基础数据结构(五种) （一）String（字符串）\n结构特点：Redis 中最基础的数据类型，可存储字符串、整数或浮点数，最大能存储 512MB 的数据。在内存中以简单动态字符串（SDS）形式存储，相比传统 C 字符串，SDS 具有获取长度时间复杂度为 O (1)、杜绝缓冲区溢出等优势。 应用场景 数据缓存：常用来缓存各类常规数据，像用户的 session 信息、用于身份验证的 token、序列化后的对象等。通过将这些数据缓存于 Redis，可显著减少数据库查询次数，提升系统响应速度。例如在 Web 应用中，频繁访问的用户信息可先从 Redis 的 String 类型缓存中获取，若不存在再查询数据库并写入缓存。 计数场景：借助 INCR、INCRBY 等命令，可方便地对数值进行原子递增操作，用于统计用户单位时间内的请求数、页面单位时间的访问数等。比如在一个简单限流系统中，可设定用户每分钟最多请求 100 次，通过对用户请求计数（存储在 String 类型键中）与阈值比较，超过则限流。 分布式锁：利用 SETNX（SET if Not eXists）命令可实现简易分布式锁。当一个线程执行 SETNX key value 时，若键不存在，会设置成功并获取锁；若键已存在，获取锁失败。释放锁时可通过删除该键实现。但这种简单实现未处理锁超时等复杂情况，实际应用中需完善。 共享配置信息：存储应用程序的配置参数，如数据库连接字符串、系统开关配置等。应用启动时从 Redis 读取配置，配置变更时可实时更新 Redis 中的值，应用通过监听机制获取变更，实现配置动态更新。 与 Hash 存储对象对比 存储方式：String 存储的是序列化后的整个对象，比如将一个包含多个字段的 Java 对象序列化为 JSON 字符串后存储。Hash 则是将对象的每个字段作为一个键值对单独存储，例如存储一个用户对象，Hash 可将用户的姓名、年龄、邮箱等字段分别以 “field:value” 形式存储。 内存占用：一般情况下，缓存相同数量的对象数据，String 消耗内存约为 Hash 的一半。因为 Hash 结构本身会有额外开销用于存储字段名等信息。但如果对象字段较少，这种内存差异可能不明显。 查询与修改灵活性：String 适合对象整体读取和更新场景。若要获取或修改对象部分字段，需先反序列化整个对象，操作后再序列化存储，性能开销大。Hash 可直接对单个字段进行查询、修改、添加操作，无需处理整个对象，在购物车这类商品频繁变动场景中优势明显。例如购物车中商品数量、添加新商品等操作，使用 Hash 可高效完成。 建议：大多数场景下，若对对象操作以整体为主，或系统对内存资源敏感，优先使用 String 存储对象数据。但对对象部分字段频繁操作的场景，Hash 更合适。 （二）List（列表）\n结构特点：按插入顺序排序的字符串链表，可在链表头部（LPUSH）或尾部（RPUSH）插入元素，也可从头部（LPOP）或尾部（RPOP）删除元素。支持获取指定范围元素（LRANGE），时间复杂度为 O (S + N)，S 为起始位置偏移量，N 为返回元素数量。 应用场景 消息队列：可作为简单消息队列使用。生产者通过 RPUSH 向列表尾部发送消息，消费者使用 LPOP 从列表头部读取消息，实现消息的有序处理。但 Redis 原生 List 作为消息队列缺乏一些高级特性，如消息持久化、ACK 机制等，适用于对消息可靠性要求不高的简单场景。 最新消息展示：用于存储最新发布的消息、文章等内容。例如在社交媒体应用中，用户发布的动态可按时间顺序通过 RPUSH 存入 List，展示时使用 LRANGE 获取最新的若干条动态。 操作日志记录：记录系统操作日志，每次操作相关信息（如操作时间、操作人、操作内容）作为一个元素，通过 RPUSH 追加到 List 中。后续可根据需要查询特定时间段或特定类型的操作记录。 相关命令 LPUSH key value [value\u0026hellip;]：将一个或多个值插入到列表头部。 RPUSH key value [value\u0026hellip;]：将一个或多个值插入到列表尾部。 LPOP key：移除并返回列表的头元素。 RPOP key：移除并返回列表的尾元素。 LRANGE key start stop：返回列表中指定区间内的元素，start 和 stop 为元素索引，0 表示第一个元素， - 1 表示最后一个元素。 （三）Set（集合）\n结构特点：无序的字符串集合，集合中元素具有唯一性，重复添加相同元素不会产生新副本。内部实现基于哈希表，添加、删除、查找元素的时间复杂度平均为 O (1)。 应用场景 去重场景：在数据处理中，可利用 Set 的唯一性对数据进行去重。例如统计网站访问用户的唯一 IP 地址，每次将访问 IP 通过 SADD 命令添加到 Set 中，最终 Set 的元素数量即为唯一 IP 数量。 标签管理：用于管理对象标签。如在一个商品管理系统中，每个商品可关联多个标签（如电子产品、打折商品等），将商品 ID 作为 key，标签作为 Set 的元素存储。通过 SISMEMBER 命令可判断商品是否具有某个标签，通过 SMEMBERS 可获取商品所有标签。 交集、并集、差集运算：在社交应用中，可通过集合运算实现共同关注、共同爱好等功能。例如，用户 A 的关注列表和用户 B 的关注列表作为两个 Set，通过 SINTER 命令求交集，可得到 A 和 B 共同关注的人。 相关命令 SADD key member [member\u0026hellip;]：向集合中添加一个或多个成员。 SREM key member [member\u0026hellip;]：移除集合中的一个或多个成员。 SISMEMBER key member：判断成员是否在集合中，存在返回 1，不存在返回 0。 SMEMBERS key：返回集合中的所有成员。 SINTER key [key\u0026hellip;]：返回多个集合的交集。 SUNION key [key\u0026hellip;]：返回多个集合的并集。 SDIFF key [key\u0026hellip;]：返回多个集合的差集（第一个集合减去其他集合的元素）。 （四）Hash（散列）\n结构特点：由键值对组成的集合，适合存储对象。每个键值对的键和值都是字符串类型。内部采用哈希表存储，在查找、插入、删除单个字段时，时间复杂度平均为 O (1)。 应用场景 存储对象：如用户信息、商品信息等对象的存储。以用户 ID 为 key，用户的各个属性（姓名、年龄、地址等）为 field，对应的值为 value。与 String 存储对象相比，Hash 可灵活操作单个字段，无需序列化和反序列化整个对象。 购物车系统：购物车场景中，以用户 ID 为 key，商品 ID 为 field，商品数量为 value。方便对购物车中商品进行添加、修改数量、删除等操作。例如，用户添加商品时，若商品已存在，使用 HINCRBY 命令增加商品数量；删除商品时，使用 HDEL 命令。 配置管理：存储应用程序的配置信息，与 String 存储配置相比，Hash 可对单个配置项进行修改，无需重新设置整个配置字符串。如数据库连接配置，可将主机、端口、用户名、密码等字段分别存储在 Hash 中。 相关命令 HSET key field value：为哈希表中的字段赋值。 HGET key field：获取哈希表中指定字段的值。 HDEL key field [field\u0026hellip;]：删除哈希表中的一个或多个字段。 HINCRBY key field increment：为哈希表中的字段值增加指定的整数。 HGETALL key：获取哈希表中的所有字段和值。 （五）Zset（有序集合）\n结构特点：每个成员都关联一个分数的字符串集合，通过分数对成员进行从小到大排序，成员唯一但分数可重复。内部通过跳跃表和哈希表两种数据结构实现，在插入、删除、查找元素以及范围查询方面性能较好，时间复杂度为 O (log N)，N 为集合元素数量。 应用场景 排行榜系统：常用于各种排行榜场景，如直播间送礼物排行榜、游戏玩家积分排行榜、商品销量排行榜等。将用户 ID 或商品 ID 作为成员，对应的分数（如礼物价值、玩家积分、商品销量）作为排序依据。通过 ZRANGE（从小到大）或 ZREVRANGE（从大到小）命令获取排行榜数据。 带权重的任务队列：在任务调度系统中，每个任务可关联一个权重（分数），根据权重决定任务执行顺序。通过 ZADD 命令添加任务及权重，通过 ZPOPMIN 或 ZPOPMAX 命令获取并移除权重最小或最大的任务。 时间序列数据：如股票价格随时间变化数据，时间作为分数，股票价格作为成员。可方便地查询某个时间段内的股票价格数据，通过 ZRANGEBYSCORE 命令实现。 相关命令 ZADD key score member [score member\u0026hellip;]：向有序集合中添加一个或多个成员，或更新已存在成员的分数。 ZRANGE key start stop [WITHSCORES]：返回有序集合中指定范围内的成员，按分数从小到大排序，WITHSCORES 选项可同时返回成员的分数。 ZREVRANGE key start stop [WITHSCORES]：返回有序集合中指定范围内的成员，按分数从大到小排序，WITHSCORES 选项可同时返回成员的分数。 ZREVRANK key member：返回有序集合中指定成员的排名（从大到小排序）。 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]：返回有序集合中分数在指定区间内的成员，可通过 LIMIT 选项指定返回结果的偏移量和数量。 二、Redis 特殊数据结构(八种) （一）HyperLogLogs（基数统计）\n结构特点：用于近似统计集合中唯一元素的数量，通过概率算法实现，在内存使用上极为高效。只需少量内存（12KB）即可统计大量数据，误差率约为 0.81%。内部通过稀疏矩阵和稠密矩阵存储数据。 应用场景 网页 UV 统计：统计网页的独立访客数。每次用户访问网页时，将用户 ID 通过 PFADD 命令添加到对应的 HyperLogLog 中，最后使用 PFCOUNT 命令获取 UV 数量。相比传统存储每个用户 ID 再统计唯一值的方式，HyperLogLog 大大节省内存。 广告曝光统计：在广告投放系统中，统计广告的独立曝光次数。将每次广告曝光的设备 ID 或用户 ID 记录到 HyperLogLog 中，可高效获取独立曝光数，为广告效果评估提供数据支持。 相关命令 PFADD key element [element\u0026hellip;]：向 HyperLogLog 中添加一个或多个元素。 PFCOUNT key [key\u0026hellip;]：返回一个或多个 HyperLogLog 的近似基数。 PFMERGE destkey sourcekey [sourcekey\u0026hellip;]：将多个 HyperLogLog 合并为一个。 （二）Bitmap（位存储）\n结构特点：基于字符串类型实现，通过位操作来存储和处理数据。每个字符串可存储 2^32 - 1 个位，即 512MB 大小的位数据。通过 SETBIT 命令设置某位的值（0 或 1），GETBIT 命令获取某位的值。 应用场景 用户活跃状态统计：以日期（精确到天）作为 key，用户 ID 为 offset。当用户在当天活跃过时，使用 SETBIT 命令将对应位置设置为 1。例如，统计 2023 年 10 月 1 日活跃用户，SETBIT 20231001 user_id 1。通过 BITCOUNT 命令可统计当天活跃用户数量，通过 BITOP 命令可进行位运算，如统计连续多日活跃用户。 权限管理：用 Bitmap 表示用户权限，每个权限对应一位。例如，用户具有读取权限，对应位设为 1；无写入权限，对应位设为 0。通过位运算可快速判断用户是否具有某组权限，如判断用户是否同时具有读取和执行权限。 相关命令 SETBIT key offset value：设置或清除指定 key 偏移量上的位值（0 或 1）。 GETBIT key offset：获取指定 key 偏移量上的位值。 BITCOUNT key [start end]：统计指定 key 中位值为 1 的数量，start 和 end 可选，用于指定字节范围。 BITOP operation destkey key [key\u0026hellip;]：对一个或多个 key 进行位运算（AND、OR、XOR、NOT），结果存储在 destkey 中。 （三）Geospatial（地理位置）\n结构特点：用于存储地理位置信息，并支持基于地理位置的查询。本质上是通过 Zset 实现，将地理位置的经纬度信息编码后作为分数存储，地理位置标识（如城市名、店铺名）作为成员。支持查询附近位置、计算距离等操作。 应用场景 附近位置查询：在地图应用、外卖配送、共享单车等场景中，用于查找用户附近的商家、配送员、共享单车等。例如，外卖应用中，用户可通过该功能查找附近可配送的餐厅，餐厅位置信息事先存储在 Redis 的 Geospatial 结构中，通过 GEORADIUS 命令查询。 距离计算：计算两个地理位置之间的距离。如在物流配送中，计算仓库与配送地址的距离，为配送路线规划提供数据支持。通过 GEODIST 命令实现距离计算。 相关命令 GEOADD key longitude latitude member [longitude latitude member\u0026hellip;]：将一个或多个地理位置信息添加到指定 key 中。 GEODIST key member1 member2 [m|km|ft|mi]：计算两个地理位置之间的距离，单位可指定为米（m）、千米（km）、英尺（ft）、英里（mi）。 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count]：以指定经纬度为中心，返回指定半径内的地理位置，可选择返回地理位置的坐标、距离、哈希值，COUNT 选项可限制返回结果数量。 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count]：与 GEORADIUS 类似，只是以已存在的地理位置成员为中心进行查询。 1. 什么是缓存穿透？怎么解决？ 嗯，我想一下。缓存穿透是指查询一个一定不存在的数据，由于存储层查不到数据因此不写入缓存，这将导致这个不存在的数据每次请求都要到 DB 去查询，可能导致 DB 挂掉。这种情况大概率是遭到了攻击。解决方案的话，我们通常都会用布隆过滤器来解决它。\n解决方法1：缓存空对象\n当查询数据库中不存在的数据时，我们不是直接返回空，而是将一个空对象（比如一个空的JSON对象或者一个标记值）存入缓存，并设置一个较短的TTL（存活时间）。这样，当下次再查询相同的数据时，可以直接从缓存中获取到这个空对象，而无需再次查询数据库，从而避免了对数据库的无效访问。\n**缺点：**虽然实现简单，但它会带来一些额外的内存消耗，因为即使是空对象，也需要占用一定的缓存空间。此外，由于设置了TTL，在缓存过期之前，即使数据库中已经添加了相应的数据，查询时仍然会返回空对象，这可能会导致短期内的数据不一致。\n解决方法2：布隆过滤器\n布隆过滤器是一种概率型数据结构，它可以用来判断一个元素是否在一个集合中。我们当时使用的是Redisson实现的布隆过滤器。它的底层原理是，先初始化一个比较大的数组，里面存放的是二进制0或1。一开始都是0，当一个key来了之后，经过3次hash计算，模数组长度找到数据的下标，然后把数组中原来的0改为1。这样，三个数组的位置就能标明一个key的存在。当需要判断一个元素是否存在时，通过相同的哈希函数计算出该元素在位数组上的位置，如果这些位置上的值都为1，则认为该元素可能存在；如果任何一个位置上的值为0，则该元素一定不存在。在缓存穿透的场景中，我们可以将数据库中存在的数据对应的key存入布隆过滤器，当查询一个key时，先通过布隆过滤器判断该key是否存在，如果不存在，直接返回，避免查询数据库。\n优点：布隆过滤器的内存占用相对较少，因为它只需要一个位数组来存储信息，相比直接缓存大量的key值，节省了很多空间。而且它没有多余的key，不会像缓存空对象那样占用额外的缓存资源。\n缺点：布隆过滤器的实现相对复杂，需要理解和使用多个哈希函数以及位数组等数据结构。而且它存在误判的可能，即当布隆过滤器判断一个key存在时，实际上该key可能并不存在，这就增加了穿透的风险。另外，布隆过滤器无法删除数据，一旦一个key被添加到布隆过滤器中，就无法单独将其移除，这在某些场景下可能会带来一些问题。\n2. 什么是缓存击穿？怎么解决？ 缓存击穿问题，也被称为热点 Key 问题。它是指在高并发场景下，一个被频繁访问且缓存重建业务较复杂的 Key 突然失效了，导致大量的请求直接穿透到数据库，给数据库带来巨大的冲击，从而引发系统性能下降甚至崩溃的情况。\n解决方法1：基于互斥锁的解决方案\n就是当线程查询缓存未命中时，尝试去获取互斥锁，然后在重建缓存数据，在这段时间里，其他线程也会去尝试获取互斥锁，如果失败就休眠一段时间，并继续，不断重试，等到数据重建成功，其他线程就可以命中数据了。这样就不会导致缓存击穿。\n这里我们获取互斥锁可以使用redis中string类型中的setnx方法 ，因为setnx方法是在key不存在的情况下才可以创建成功的，所以我们重建缓存时，使用setnx来将锁的数据加入到redis中，并且通过判断这个锁的key是否存在，如果存在就是获取锁成功，失败就是获取失败，这样刚好可以实现互斥锁的效果。释放锁就更简单了，直接删除我们存入的锁的 Key 来释放锁。\n优点：\n​\t**内存占用小：**由于不需要额外存储过期时间等信息，内存占用相对较少。\n​\t**一致性高：**所有线程最终都能获取到最新的缓存数据，数据一致性得到了保证。\n​\t**实现简单：**通过 Redis 的 SETNX 方法和简单的逻辑控制，就可以实现互斥锁，实现起来相对容易。\n缺点：\n​\t**性能较低：**多个线程在获取锁失败后会不断重试，导致线程阻塞，降低了系统的并发性能。\n​\t**容易出现死锁：**如果在获取锁后，线程出现异常或长时间未释放锁，可能会导致其他线程一直等待，从而出现死锁的情况。\n解决方法2：基于逻辑过期解决缓存击穿问题\n原理：给 Redis 缓存字段中添加一个过期时间字段，而不是直接设置缓存的过期时间。当线程查询缓存的时候，先判断是否已经过期。如果过期，就获取互斥锁，并开启一个子线程进行缓存重建任务，直到子线程完成任务后，释放锁。在这段时间内，其他线程获取互斥锁失败后，并不是继续等待重试，而是直接返回旧数据。\n实现方式：所谓的逻辑过期，类似于逻辑删除，并不是真正意义上的过期，而是新增一个字段，用来标记 Key 的过期时间。这样能够避免 Key 过期而被自动删除，从而使数据永不过期，从根本上解决因为热点 Key 过期导致的缓存击穿。一般在搞活动时，比如抢优惠券、秒杀等场景，请求量比较大就可以使用逻辑过期，等活动一过就手动删除逻辑过期的数据。逻辑过期一定要先进行数据预热，将热点数据加载到缓存中。逻辑过期时间根据具体业务而定，逻辑过期过长，会造成缓存数据的堆积，浪费内存；过短会造成频繁缓存重建，降低性能。所以设置逻辑过期时间时需要实际测试和评估不同参数下的性能和资源消耗情况，可以通过观察系统的表现，在业务需求和性能要求之间找到一个平衡点。\n优点：\n​\t性能高：通过开启子线程重建缓存，使原来的同步阻塞变成异步，提高了系统的响应速度，能够更好地应对高并发场景。\n缺点：\n​\t内存占用较大：需要额外存储过期时间字段，增加了内存的占用。\n​\t容易出现脏读：在缓存重建期间，其他线程可能会获取到旧数据，从而导致脏读的情况。\n3. 什么是缓存雪崩？怎么解决？ 缓存雪崩是指在缓存系统中，大量缓存 Key 同时失效，或者缓存服务（如 Redis）整体宕机，导致原本应该从缓存获取的数据全部穿透到数据库，从而给数据库带来巨大的压力，甚至可能导致数据库崩溃。这种情况通常会引发整个系统的性能问题，甚至导致服务不可用。\n具体来说，缓存雪崩可以分为两种情况：\n大量 Key 同时失效：如果缓存中的 Key 都设置了相同的过期时间，那么在某一时刻，这些 Key 会同时失效。此时，大量请求会同时查询数据库，导致数据库负载瞬间增加。 缓存服务宕机：如果整个缓存服务（如 Redis 集群）宕机，所有原本依赖缓存的请求都会直接查询数据库，同样会给数据库带来巨大压力。 解决方案1： 给不同的 Key 设置随机的 TTL\n为了避免大量 Key 同时失效，可以在设置缓存时，为每个 Key 的 TTL（过期时间）添加一个随机值。例如，原本的过期时间是 10 分钟，可以在此基础上随机增加 1-5 分钟。这样，每个 Key 的过期时间就会分散开来，很难同时失效。\n解决方案2： 利用 Redis 集群提高服务的可用性\n通过部署 Redis 集群，将缓存数据分散到多个 Redis 实例中。即使某个 Redis 实例宕机，其他实例仍然可以正常工作，从而提高缓存服务的整体可用性。\n解决方案3： 添加降级限流策略\n在缓存层和数据库层之间添加降级限流策略，当请求量过高时，通过快速失败机制直接返回错误或默认值，避免请求穿透到数据库。\n解决方案4： 添加多级缓存\n在应用层添加多级缓存，除了本地缓存（如 Ehcache）和分布式缓存（如 Redis）外，还可以在数据库层添加缓存（如数据库的查询缓存）。这样，即使分布式缓存失效，还可以通过本地缓存或数据库缓存来缓解压力。\n实现方式：\n在应用层使用本地缓存（如 Ehcache）作为第一级缓存。 在分布式缓存（如 Redis）作为第二级缓存。 在数据库层使用查询缓存作为第三级缓存。 4. redis作为缓存，mysql的数据如何与redis进行同步呢？（双写一致性） 我们当时采用了读写锁来保证强一致性。具体来说，我们使用了 Redisson 实现的读写锁。\n在读取数据时，我们添加共享锁，这样可以保证读读不互斥，但读写互斥。这意味着多个线程可以同时读取数据，但如果有线程正在写入数据，其他线程就不能读取，从而避免了脏读。而在更新数据时，我们添加排他锁，这样无论是读操作还是写操作，都会被互斥，确保在写数据的同时，不会有其他线程读取数据，进一步避免了脏数据。\n这里面需要注意的是，读方法和写方法上需要使用同一把锁才行。这样可以确保在任何时刻，数据的一致性都能得到保证。\n5. 那这个排他锁是如何保证读写、读读互斥的呢？ 其实排他锁底层使用的是 SETNX，它保证了同时只能有一个线程操作锁住的方法。当一个线程获取了排他锁后，其他线程无论是尝试读取还是写入，都会被阻塞，直到锁被释放。\n5. 你听说过延时双删吗？为什么不用它呢？ 延迟双删，如果是写操作，我们先把缓存中的数据删除，然后更新数据库，最后再延时删除缓存中的数据。其中，这个延时多久不太好确定。在延时的过程中，可能会出现脏数据，并不能保证强一致性，所以没有采用它。\n6. Redis 数据的持久化是怎么做的？ 候选人：在 Redis 中提供了两种主要的数据持久化方式：RDB（Redis Database）和 AOF（Append Only File）。\n7. 这两种持久化方式有什么区别呢？ 候选人：RDB 是一个快照文件，它会定期将 Redis 内存中的数据写入到磁盘上的一个 RDB 文件中。当 Redis 实例宕机后，可以从 RDB 文件中恢复数据。RDB 文件是二进制格式的，体积相对较小，恢复速度较快，但可能会丢失最后一次快照之后的数据。\nAOF 是追加文件，它会将 Redis 执行的每个写命令追加到 AOF 文件中。当 Redis 实例宕机后，可以从 AOF 文件中重新执行这些命令来恢复数据。AOF 文件是文本格式的，体积相对较大，恢复速度较慢，但数据丢失的风险更小。\n8. 这两种方式，哪种恢复的比较快呢？ 候选人：RDB 恢复速度更快，因为它是一个二进制文件，体积较小，加载和恢复数据的速度通常比 AOF 快。然而，RDB 的缺点是可能会丢失最后一次快照之后的数据。AOF 恢复速度较慢，因为它需要重新执行文件中的所有命令来恢复数据，但它的数据完整性更高，丢失数据的风险更小。\n在实际项目中，我们通常会结合使用 RDB 和 AOF 来实现数据的持久化。例如，我们会在项目中设置 RDB 每隔一定时间（如每小时）生成一次快照，同时开启 AOF 并设置为每秒批量写入一次命令。这样可以在保证数据恢复速度的同时，最大限度地减少数据丢失的风险。\n9. 你们项目中具体是如何设置的呢？ 候选人：在我们的项目中，我们采用了以下配置：\nRDB 配置：\n我们设置了 RDB 每小时生成一次快照，这样可以在 Redis 实例宕机时，从最近的快照文件中恢复大部分数据。\n配置示例：\n1 save 3600 1 AOF 配置：\n我们开启了 AOF，并设置了每秒批量写入一次命令，这样可以在 Redis 实例宕机时，从 AOF 文件中恢复最近的写操作。\n配置示例：\n1 2 appendonly yes appendfsync everysec 通过这种配置，我们既保证了数据恢复的速度，又最大限度地减少了数据丢失的风险。在实际运行中，这种组合方式表现出了良好的性能和可靠性。\n10. Redis的数据过期策略有哪些？ 候选人：嗯~，在redis中提供了两种数据过期删除策略。第一种是惰性删除。在设置该key过期时间后，我们不去管它。当需要该key时，我们检查其是否过期。如果过期，我们就删掉它；反之，返回该key。第二种是定期删除。就是说，每隔一段时间，我们就对一些key进行检查，并删除里面过期的key。定期清理的两种模式是：1) SLOW模式，是定时任务，执行频率默认为10hz，每次不超过25ms，可以通过修改配置文件redis.conf的hz选项来调整这个次数；2) FAST模式，执行频率不固定，每次事件循环会尝试执行，但两次间隔不低于2ms，每次耗时不超过1ms。Redis的过期删除策略是：惰性删除 + 定期删除两种策略配合使用。\n11. Redis的数据淘汰策略有哪些？ 候选人：嗯，这个在redis中提供了很多种，默认是noeviction，不删除任何数据，内部不足时直接报错。这个可以在redis的配置文件中进行设置。里面有两个非常重要的概念：一个是LRU，另外一个是LFU。LRU的意思就是最少最近使用。它会用当前时间减去最后一次访问时间。这个值越大，则淘汰优先级越高。LFU的意思是最少频率使用。它会统计每个key的访问频率。值越小，淘汰优先级越高。我们在项目中设置的是allkeys-lru，它会挑选最近最少使用的数据进行淘汰，把一些经常访问的key留在redis中。\n12. 数据库有1000万数据，Redis只能缓存20w数据。如何保证Redis中的数据都是热点数据？ 候选人：嗯，我想一下()。可以使用allkeys-lru（挑选最近最少使用的数据淘汰）淘汰策略。那留下来的都是经常访问的热点数据。\n13. Redis的内存用完了会发生什么？ 候选人：嗯~，这个要看redis的数据淘汰策略是什么。如果是默认的配置，redis内存用完以后则直接报错。我们当时设置的是allkeys-lru策略，把最近最常访问的数据留在缓存中。\n14. Redis分布式锁如何实现 在Redis中提供了一个命令SETNX（SET if not exists）。由于Redis是单线程的，使用这个命令之后，只能有一个客户端对某一个key设置值。在没有过期或删除key的时候，其他客户端是不能设置这个key的。\n15. 那你如何控制Redis实现分布式锁的有效时长呢？ SETNX指令不好控制有效时长。我们采用的是Redisson框架实现的。在Redisson中需要手动加锁，并且可以控制锁的失效时间和等待时间。当锁住的一个业务还没有执行完成时，Redisson会引入一个看门狗机制，每隔一段时间检查当前业务是否还持有锁。如果持有，就增加加锁的持有时间。当业务执行完成之后，需要使用释放锁。在高并发下，一个业务执行很快时，客户1持有锁，客户2来了以后并不会马上被拒绝，而是自旋不断尝试获取锁。如果客户1释放之后，客户2就可以马上持有锁，性能也得到了提升。\n16. Redisson实现的分布式锁是可重入的吗？它是怎么实现的？ 是的，Redisson 实现的分布式锁是可重入的。可重入锁允许同一个线程多次获取同一把锁而不会被阻塞，这可以有效避免死锁问题，同时让代码逻辑更清晰。\nRedisson 如何实现可重入锁\nRedisson 是基于 Redis 的哈希结构来存储锁信息的。打个比方，我们有个叫 “myLock” 的锁，这就是锁的唯一标识，相当于哈希结构里的 Key。而每个尝试获取锁的线程都有自己唯一的标识，像线程 ID 或者 UUID，这就是哈希结构里的 Field。线程获取锁的次数，也就是重入次数，就是哈希结构里的 Value。\n加锁过程\n当一个线程想去获取锁的时候，Redisson 首先会检查这个锁对应的 Key 存不存在。要是不存在，那就说明当前没有线程持有这把锁，Redisson 就会创建这个锁，把 Field 设成当前线程的标识，Value 设为 1，同时给锁设置一个过期时间。要是锁已经存在，Redisson 就会去检查 Field 是不是和当前线程的标识一样。如果一样，那就意味着当前线程已经持有这把锁了，Redisson 就把 Value 加 1，并且刷新锁的过期时间。要是不一样，那就表示锁被其他线程占着，当前线程就得等着锁被释放。\n释放锁过程\n释放锁的时候，Redisson 会先看看锁的 Field 和当前线程标识是不是一致。如果一致，就把 Value 减 1。要是减完之后 Value 变成 0 了，那就说明当前线程已经完全释放了这把锁，Redisson 就把锁对应的 Key 删除。要是 Value 还大于 0，说明当前线程还有重入的情况，还持有锁，Redisson 就刷新一下锁的过期时间。\n防止死锁\nRedisson 在防止死锁方面也有很实用的机制。一方面，加锁的时候会给锁设置过期时间，就算某个线程出问题了，一直不释放锁，到时间了锁也会自动被删除。另一方面，它的可重入机制也能避免因为线程嵌套调用导致的死锁。\n可重入锁\nRedisson 的可重入锁优势也很明显。从线程安全角度看，只有持有锁的线程才能释放锁，这就保证了不会出现线程安全问题。在性能上，它通过 Lua 脚本确保加锁和释放锁的操作是原子性的，避免了竞态条件，效率很高。\n17. Redisson实现的分布式锁能解决主从一致性的问题吗？ 不能完全解决。在Redis的主从复制模式下，主节点和从节点之间存在数据同步的延迟。当主节点宕机时，从节点可能会被提升为新的主节点，但此时锁数据可能尚未完全同步，导致锁失效。\n例如，当线程1在主节点上加锁成功后，主节点数据还未完全同步到从节点。如果此时主节点宕机，从节点被提升为新的主节点，线程2可能会在新的主节点上加锁成功，导致两个线程同时持有一把锁。\n18. 如果业务非要保证数据的强一致性，这个该怎么解决呢？ 使用Redisson的multiLock Redisson提供了multiLock方案来解决主从一致性问题。其核心思想是：\n在多个独立的Redis节点上分别创建锁。 只有当在所有指定的Redis节点上都成功获取锁时，才算获取锁成功。 这种方式的优点是可以避免主从同步不一致时锁失效的问题。但缺点是运维成本高，实现复杂，且至少需要三台Redis服务器。\n使用其他分布式锁实现 如果业务对数据一致性要求极高，建议使用其他分布式锁实现，如基于ZooKeeper的分布式锁。ZooKeeper的分布式锁可以保证强一致性，适用于对一致性要求严格的应用场景。\n19. Redis集群有哪些方案，知道吗？ 在Redis中提供的集群方案总共有三种：主从复制、哨兵模式、Redis分片集群。\n20. 那你来介绍一下主从同步。 单节点Redis的并发能力有上限，可以通过搭建主从集群实现读写分离，一般是一主多从，主节点负责写数据，从节点负责读数据。主节点写入数据后，需要把数据同步到从节点中。\n21. 能说一下主从同步数据的流程吗？ 嗯~~，好！主从同步分为了两个阶段，一个是全量同步，一个是增量同步。\n全量同步是指从节点第一次与主节点建立连接的时候使用全量同步，流程是这样的：\n第一：从节点请求主节点同步数据，其中从节点会携带自己的replication id和offset偏移量。\n第二：主节点判断是否是第一次请求，主要判断的依据就是，主节点与从节点是否是同一个replication id，如果不是，就说明是第一次同步，那主节点就会把自己的replication id和offset发送给从节点，让从节点与主节点的信息保持一致。\n第三：在同时主节点会执行BGSAVE，生成RDB文件后，发送给从节点去执行，从节点先把自己的数据清空，然后执行主节点发送过来的RDB文件，这样就保持了一致。\n当然，如果在RDB生成执行期间，依然有请求到了主节点，而主节点会以命令的方式记录到缓冲区，缓冲区是一个日志文件，最后把这个日志文件发送给从节点，这样就能保证主节点与从节点完全一致了，后期再同步数据的时候，都是依赖于这个日志文件，这个就是全量同步。\n增量同步指的是，当从节点服务重启之后，数据就不一致了，所以这个时候，从节点会请求主节点同步数据，主节点还是判断不是第一次请求，不是第一次就获取从节点的offset值，然后主节点从命令日志中获取offset值之后的数据，发送给从节点进行数据同步。\n22. 怎么保证Redis的高并发高可用？ 为了保证Redis的高并发和高可用性，通常会采用以下几种策略：\n主从复制与哨兵模式\n主从复制：通过主从复制，可以将数据从主节点同步到多个从节点。主节点负责写操作，从节点负责读操作，这样可以分散读取压力，提高系统的并发能力。 哨兵模式：哨兵（Sentinel）是Redis的高可用性解决方案，可以监控主从复制的运行状态。如果主节点发生故障，哨兵会自动将一个从节点提升为新的主节点，并通知客户端更新连接信息。哨兵模式通过以下机制实现高可用性： 监控：哨兵会持续监控主节点和从节点的状态。 自动故障转移：当主节点故障时，哨兵会自动选择一个从节点作为新的主节点。 通知客户端：哨兵会通过发布/订阅机制通知客户端新的主节点信息。 Redis集群\n分片：Redis集群通过将数据分片到多个节点上，可以有效分散负载，提高并发处理能力。 自动故障转移：集群模式下，当主节点故障时，集群会自动进行故障转移，确保服务的可用性。 23. 你们使用Redis是单点还是集群，哪种集群？ 24. Redis集群脑裂该怎么解决呢？ 25. Redis的分片集群有什么作用？ 26. Redis分片集群中数据是怎么存储和读取的？ 27. Redis是单线程的，但是为什么还那么快？ 完全基于内存的，C语言编写。 采用单线程，避免不必要的上下文切换和竞争条件。 使用多路I/O复用模型，非阻塞IO。 28. 能解释一下I/O多路复用模型？ I/O 多路复用模型呢，简单来说，就是让一个线程能同时盯着好多网络连接（Socket）。就像餐厅里一个厉害的服务员，能同时照顾好多桌客人。以前传统的方式是来一个客人就安排一个服务员专门盯着，太浪费人力了。而 I/O 多路复用就是让一个服务员（线程）看着所有桌子（Socket），哪个桌子上的客人需要点菜（可读）或者要结账（可写）了，这个服务员就过去服务。\nRedis 就是用了 I/O 多路复用结合事件处理器来处理好多 Socket 请求，像有专门处理连接的、处理命令回复的、处理命令请求的。不过在 Redis 6.0 之后，为了让处理速度更快，在命令回复和命令请求处理的某些环节用了多线程，但核心的命令执行还是单线程，保证命令执行的准确性和稳定性。\n五、设计模式篇 1. 单点登录这块怎么实现的？ 单点登录（SSO）：用户只需登录一次，即可访问所有信任的应用系统。\n基于 Cookie 和 Session 的实现\n用户登录：用户访问应用系统，输入用户名和密码进行登录，登录请求被发送到认证中心。 认证与 Session 创建：认证中心验证用户身份信息，若成功则在服务器端创建一个 Session，将用户信息存储在 Session 中，并生成一个唯一的 Session ID。 Cookie 设置：认证中心将 Session ID 通过 Cookie 发送给客户端浏览器，浏览器会在后续请求中自动携带该 Cookie。 访问其他应用系统：当用户访问其他应用系统时，浏览器会携带包含 Session ID 的 Cookie，应用系统收到请求后，从 Cookie 中获取 Session ID，并根据 Session ID 到服务器端查找对应的 Session，以此来验证用户身份，若 Session 存在且有效，则允许用户访问。 基于 Token 的实现\n用户登录：用户在登录页面输入账号和密码，系统验证身份后生成一个JWT令牌，然后将这个令牌返回给浏览器，浏览器会将其保存到Cookie中。 访问其他服务：当用户访问其他系统或服务时，浏览器会自动携带这个JWT令牌。网关会拦截请求，检查令牌是否有效。 令牌验证： 如果令牌有效，网关会将请求路由到目标服务，用户可以正常访问。 如果令牌无效（比如过期或被篡改），网关会返回401状态码，前端接收到后会跳转到登录页面，提示用户重新登录。 服务间通信：在微服务架构中，各服务之间也可能需要进行身份验证。这时，服务之间会通过传递JWT令牌来确认彼此的身份，确保通信的安全性。 2. 上传数据的安全性你们怎么控制？ 在我们项目中，对于浏览器访问后台时上传数据的安全性问题，主要是通过加密技术来控制的，常用的是对称加密和非对称加密这两种方式。\n先来说说对称加密吧。它的原理是加密和解密都使用同一个密钥。数据发送方会把要传输的明文和这个加密密钥一起，通过特定的加密算法进行处理，将其转化为复杂的密文，然后再发送出去。接收方收到密文后，要想解读出原文，就需要使用和加密时相同的密钥，以及这个加密算法的逆算法来对密文进行解密，这样才能恢复成原来可读的明文。对称加密的优点很明显，它的算法是公开的，计算量比较小，加密的速度快，效率也高，能够快速处理大量数据。不过它也有缺点，就是安全性相对非对称加密来说要低一些。所以我们一般会用它来保存像用户手机号、身份证这类虽然敏感，但后续需要解密使用的信息。常见的对称加密算法有 AES、DES、3DES 等等。\n再就是非对称加密。这种加密方式有两个密钥，一个是公开密钥，另一个是私有密钥。我们会同时生成这两把密钥，私钥由服务器端隐秘保存好，公钥则可以下发给信任的客户端。在加密和解密方面，私钥加密的内容，只有持有公钥的一方才能解密；反过来，公钥加密的内容，就只有持有私钥的一方可以解密。而且，还可以用私钥进行签名，通过公钥来验证数据有没有被篡改过。非对称加密的优势在于它的安全性更好，但是它也有不足的地方，那就是加密和解密花费的时间比较长，速度慢，所以只适合对少量数据进行加密。我们一般会把它用在签名和认证这些场景里，比如说私钥由服务器保存用来加密数据，公钥交给客户端，让客户端用来对令牌或者签名进行解密或者校验。像 RSA、DSA 这些都是常见的非对称加密算法。\n在实际应用中，我们会根据具体的情况来选择合适的加密方式。如果传输的数据量很大，为了保证效率，我们会建议使用对称加密，不过这种情况下，我们不会用它来保存特别敏感的信息。而如果传输的数据量比较小，同时对安全性要求又很高，那我们就会采用非对称加密，以确保数据的安全。通过这样的方式，我们能够有效地保障上传数据在网络传输过程中的安全性。\n3. 你负责项目的时候遇到了哪些比较棘手的问题？ 在我们的在线判题系统（OJ）项目中，不同编程语言的判题逻辑存在显著差异。例如，Java语言的内存和时间限制通常需要适当增加，而C++语言可能需要更严格的限制。此外，代码沙箱的调用也需要支持多种方式，如本地沙箱、远程沙箱和第三方沙箱。如果将所有这些逻辑都写在一个Service类中，通过大量的if...else语句来区分，代码的可读性和可维护性会变得很差，圈复杂度也会很高。因此，我选择了策略模式、工厂模式和代理模式来解决这些问题。\n(1) 首先，我们用策略模式封装不同语言的判题算法\n定义判题策略接口\n这个接口就像是一个规范，规定了所有具体判题策略类都必须实现的方法。在这个接口里，有一个 doJudge 方法，它接收一个 JudgeContext 类型的参数，这个参数可以携带判题所需的各种信息。\n1 2 3 public interface JudgeStrategy { void doJudge(JudgeContext context); } 实现具体的策略类\n以 Java 和 C++ 语言为例。对于 Java 语言，我创建了 JavaLanguageJudgeStrategy 类，它实现了 JudgeStrategy 接口，在 doJudge 方法里编写了 Java 语言特有的判题逻辑。同样地，对于 C++ 语言，我创建了 CppLanguageJudgeStrategy 类，也在其 doJudge 方法中实现了 C++ 语言的判题逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 public class JavaLanguageJudgeStrategy implements JudgeStrategy { @Override public void doJudge(JudgeContext context) { // Java语言的判题逻辑 } } public class CppLanguageJudgeStrategy implements JudgeStrategy { @Override public void doJudge(JudgeContext context) { // C++语言的判题逻辑 } } 定义上下文类\n为了方便传递判题所需的信息，我定义了一个上下文类 JudgeContext。在这个类中，包含了提交的代码、输入用例以及预期输出等信息，这些信息会在判题过程中被使用。\n1 2 3 4 5 6 public class JudgeContext { private String submissionCode; private String input; private String expectedOutput; // 其他需要的信息 } 定义策略管理类\n最后，为了管理这些不同的判题策略，我定义了一个策略管理类 JudgeManager。在这个类中，使用一个 Map 来存储不同语言对应的判题策略，键是语言的名称，值是对应的策略类实例。在构造函数中，我初始化了这个 Map，将 Java 和 C++ 等语言对应的策略类实例添加进去。同时，提供了一个 executeStrategy 方法，根据传入的语言名称从 Map 中获取对应的策略实例，如果存在就调用其 doJudge 方法进行判题，如果不存在则抛出异常。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class JudgeManager { private Map\u0026lt;String, JudgeStrategy\u0026gt; strategyMap; public JudgeManager() { strategyMap = new HashMap\u0026lt;\u0026gt;(); strategyMap.put(\u0026#34;Java\u0026#34;, new JavaLanguageJudgeStrategy()); strategyMap.put(\u0026#34;Cpp\u0026#34;, new CppLanguageJudgeStrategy()); // 添加其他语言的策略 } public void executeStrategy(String language, JudgeContext context) { JudgeStrategy strategy = strategyMap.get(language); if (strategy != null) { strategy.doJudge(context); } else { throw new IllegalArgumentException(\u0026#34;Unsupported language: \u0026#34; + language); } } } 通过这种策略模式的实现，我们可以很方便地添加新的编程语言判题策略，同时也使得代码的可维护性和可扩展性得到了显著提升。如果后续需要增加新的语言判题逻辑，只需要创建一个新的策略类并实现 JudgeStrategy 接口，然后在 JudgeManager 的 Map 中添加对应的映射关系即可，不会对现有的代码造成影响。\n(2) 然后，我们使用工厂模式简化代码沙箱调用实例的获取\n定义代码沙箱接口\n首先，我定义了一个代码沙箱接口 CodeSandbox。这个接口规定了代码沙箱必须具备的核心功能，即 execute 方法，该方法接收代码和输入作为参数，返回执行结果。通过这个接口，我们可以为不同类型的代码沙箱提供统一的调用方式。\n1 2 3 public interface CodeSandbox { ExecutionResult execute(String code, String input); } 实现具体的代码沙箱类\n我实现了具体的代码沙箱类。分别有 LocalCodeSandbox 用于本地代码沙箱执行，RemoteCodeSandbox 用于远程代码沙箱执行，以及 ThirdPartyCodeSandbox 用于调用第三方代码沙箱服务。每个类都实现了 CodeSandbox 接口，并在 execute 方法中编写了各自对应的执行逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 public class LocalCodeSandbox implements CodeSandbox { @Override public ExecutionResult execute(String code, String input) { // 本地沙箱执行逻辑 } } public class RemoteCodeSandbox implements CodeSandbox { @Override public ExecutionResult execute(String code, String input) { // 远程沙箱执行逻辑 } } public class ThirdPartyCodeSandbox implements CodeSandbox { @Override public ExecutionResult execute(String code, String input) { // 第三方沙箱执行逻辑 } } 定义工厂类\n最后，为了方便获取不同类型的代码沙箱实例，我定义了一个工厂类CodeSandboxFactory。在这个工厂类中，有一个静态方法 getCodeSandbox，它接收一个表示沙箱类型的字符串作为参数。根据传入的类型，使用 switch 语句进行判断，然后返回相应的代码沙箱实例。如果传入的类型不被支持，会抛出一个 IllegalArgumentException 异常.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class CodeSandboxFactory { public static CodeSandbox getCodeSandbox(String type) { switch (type) { case \u0026#34;local\u0026#34;: return new LocalCodeSandbox(); case \u0026#34;remote\u0026#34;: return new RemoteCodeSandbox(); case \u0026#34;thirdParty\u0026#34;: return new ThirdPartyCodeSandbox(); default: throw new IllegalArgumentException(\u0026#34;Unsupported sandbox type: \u0026#34; + type); } } } 通过这种工厂模式的实现，我们在需要获取代码沙箱实例时，只需要调用 CodeSandboxFactory 的 getCodeSandbox 方法并传入相应的类型即可，无需关心具体的实例创建过程。而且，如果后续需要添加新的代码沙箱类型，只需要在工厂类的 switch 语句中添加相应的分支，同时实现对应的代码沙箱类，不会对其他部分的代码造成影响，大大提高了代码的可扩展性和可维护性。\n(3) 为了进一步优化判题机模块中代码沙箱调用的灵活性与可管理性，我采取了配置化代码沙箱类型的方案。\n实现动态切换代码沙箱的实现，全程无需修改代码，大大提高了系统的灵活性与稳定性。\n在 application.yml 配置文件中，我们添加了代码沙箱类型的配置项。\n1 2 sandbox: type: remote 代码读取配置\n在 Java 代码中，利用 Spring 框架提供的 @Value 注解，我们可以方便地读取配置文件中的 sandbox.type 值。示例代码如下：\n1 2 @Value(\u0026#34;${sandbox.type}\u0026#34;) private String sandboxType; 然后，在获取代码沙箱实例的方法中，我们利用前面提到的工厂类来获取相应类型的代码沙箱实例：\n1 2 3 public CodeSandbox getCodeSandbox() { return CodeSandboxFactory.getCodeSandbox(sandboxType); } 这里通过将读取到的 sandboxType 作为参数传递给 CodeSandboxFactory 的 getCodeSandbox 方法，就能获取到对应的代码沙箱实例。\n(4) 最后 我们使用代理模式增强代码沙箱的能力\n在调用代码沙箱前后进行日志记录等操作时，如果直接在代码沙箱调用实现类中编写，会导致代码耦合度高，后续修改和扩展困难。\n定义代理类\n我定义了一个代理类 CodeSandboxProxy，它实现了 CodeSandbox 接口。在这个代理类中，有一个成员变量 target，它是被代理的代码沙箱对象。通过构造函数将被代理的对象传入并赋值给 target。 在 execute 方法中，我在调用被代理对象的 execute 方法前后添加了日志记录的操作。具体来说，在调用之前，会调用 log 方法记录 “Executing code in sandbox\u0026hellip;”，表示代码沙箱开始执行代码；在调用之后，再次调用 log 方法记录 “Execution completed.”，表示代码执行完成。最后返回执行结果。log 方法中封装了具体的日志记录逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 public class CodeSandboxProxy implements CodeSandbox { private CodeSandbox target; public CodeSandboxProxy(CodeSandbox target) { this.target = target; } @Override public ExecutionResult execute(String code, String input) { log(\u0026#34;Executing code in sandbox...\u0026#34;); ExecutionResult result = target.execute(code, input); log(\u0026#34;Execution completed.\u0026#34;); return result; } private void log(String message) { // 日志记录逻辑 } } 使用代理类\n在获取代码沙箱实例的方法 getCodeSandbox 中，首先通过 CodeSandboxFactory 工厂类根据配置的沙箱类型获取到具体的代码沙箱实例 target，然后将这个实例作为参数传递给 CodeSandboxProxy 代理类的构造函数，创建一个代理对象并返回。这样，后续调用代码沙箱的 execute 方法时，实际上调用的是代理对象的 execute 方法，从而实现了在代码沙箱执行前后添加日志记录等额外功能。\n1 2 3 4 public CodeSandbox getCodeSandbox() { CodeSandbox target = CodeSandboxFactory.getCodeSandbox(sandboxType); return new CodeSandboxProxy(target); } 通过代理模式，我们可以在不改变代码沙箱实现类的前提下，集中地为代码沙箱添加新的功能，比如日志记录、性能监控等。这样，代码沙箱的原有功能不会受到影响，同时又能轻松地扩展其能力。\n代理模式将日志记录逻辑与代码沙箱的执行逻辑分离开来。代码沙箱的实现类只需要专注于代码执行的核心逻辑，而日志记录等额外功能由代理类负责。这使得每个类的职责更加清晰明确，提高了代码的可维护性和可扩展性，也让代码结构更加合理。\n六、Java基础 1. JVM vs JDK vs JRE JVM（Java Virtual Machine） 是运⾏ Java 字节码的虚拟机。JVM 有针对不同系统的特定实现（Windows， Linux，macOS），⽬的是使⽤相同的字节码，它们都会给出相同的结果。字节码和不同系统的 JVM 实现是 Java 语⾔“⼀次编译，随处可以运⾏”的关键所在。JVM有多种实现，比如我们常用的HotSpot VM，还有J9 VM、Zing VM等。这些不同的实现提供了不同的性能优化和特性。\nJDK（Java Development Kit） 是Java开发工具包，它包含了JRE的所有内容，同时还包括编译器（javac）、调试器（jdb）、文档生成器（javadoc）等开发工具。JDK是Java开发者的必备工具，用于编写、编译和调试Java程序。如果你需要进行Java开发，那么安装JDK是必须的。\nJRE（Java Runtime Environment） 是Java运行时环境，它包含了运行已编译Java程序所需的所有内容，包括JVM、Java类库和一些基础工具。JRE主要用于运行Java程序，而不包含开发工具。如果你只需要运行Java程序，而不需要进行开发，那么安装JRE就足够了。\n在实际工作中，虽然JRE可以运行Java程序，但很多时候我们也会安装JDK，因为一些工具（如JSP容器）在运行时需要编译功能，这就需要JDK的支持。总的来说，JVM是运行Java程序的核心，JDK是开发Java程序的工具集，而JRE是运行Java程序的环境。\n2. 什么是字节码?采用字节码的好处是什么? 在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的⽂件），它不⾯向任何特定 的处理器，只⾯向虚拟机。Java 语⾔通过字节码的⽅式，在⼀定程度上解决了传统解释型语⾔执⾏ 效率低的问题，同时⼜保留了解释型语⾔可移植的特点。所以， Java 程序运⾏时相对来说还是⾼效 的（不过，和 C++，Rust，Go 等语⾔还是有⼀定差距的），⽽且，由于字节码并不针对⼀种特定的 机器，因此，Java 程序⽆须重新编译便可在多种不同操作系统的计算机上运⾏。\n3. Java 程序从源代码到运⾏的过程如下图所示 我们需要格外注意的是 .class-\u0026gt;机器码 这⼀步。在这⼀步 JVM 类加载器⾸先加载字节码⽂件，然后 通过解释器逐⾏解释执⾏，这种⽅式的执⾏速度会相对比较慢。⽽且，有些⽅法和代码块是经常需要 被调⽤的(也就是所谓的热点代码)，所以后⾯引进了 JIT（just-in-time compilation） 编译器，⽽ JIT 属于运⾏时编译。当 JIT 编译器完成第⼀次编译后，其会将字节码对应的机器码保存下来，下次可以 直接使⽤。⽽我们知道，机器码的运⾏效率肯定是⾼于 Java 解释器的。这也解释了我们为什么经常 会说 Java 是编译与解释共存的语⾔ 。\n3. 成员变量与局部变量的区别？ 语法形式 ：从语法形式上看，成员变量是属于类的，⽽局部变量是在代码块或⽅法中定义的变量或是⽅法的参数；成员变量可以被 public , private , static 等修饰符所修饰，⽽局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰。 存储⽅式 ：从变量在内存中的存储⽅式来看,如果成员变量是使⽤ static 修饰的，那么这个成员变量是属于类的，如果没有使⽤ static 修饰，这个成员变量是属于实例的。⽽对象存在于堆内存，局部变量则存在于栈内存。 ⽣存时间 ：从变量在内存中的⽣存时间上看，成员变量是对象的⼀部分，它随着对象的创建⽽ 存在，⽽局部变量随着⽅法的调⽤⽽⾃动⽣成，随着⽅法的调⽤结束⽽消亡。 默认值 ：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会⾃动以类型的默认值⽽赋值（⼀种情况例外:被 final 修饰的成员变量也必须显式地赋值），⽽局部变量则不会⾃动 赋值。 5. 静态变量有什么作用？ 共享数据： 静态变量属于类本身，而不是类的某个具体实例。因此，无论一个类创建了多少个对象，所有这些对象都共享同一份静态变量。这使得静态变量非常适合用于存储类级别的数据，这些数据对所有实例都是相同的。 节省内存： 由于静态变量是类级别的，它在内存中只有一份副本，无论创建多少个类的实例，都不会重复占用内存。这在处理大量对象时可以显著节省内存资源。 常量定义： 静态变量通常会被final关键字修饰，成为常量。这样可以确保这些变量的值在运行时不会被修改，从而提高代码的安全性和可维护性。 6. 静态方法和实例方法有何不同？ 调用方式： 静态方法：可以通过类名.方法名直接调用，无需创建类的实例。虽然也可以通过对象调用，但这种方式容易造成混淆，因此建议使用类名.方法名来调用静态方法。 实例方法：必须通过类的实例调用，即对象.方法名。实例方法依赖于具体的对象实例。 访问类成员的限制： 静态方法：只能访问类的静态成员（静态变量和静态方法），不能访问实例成员（实例变量和实例方法）。这是因为静态方法属于类本身，而不是某个具体实例。 实例方法：可以访问类的所有成员，包括静态成员和实例成员。实例方法依赖于具体的对象实例，因此可以访问该实例的所有成员。 与对象的关系： 静态方法：属于类本身，与具体的对象实例无关。无论创建多少个对象，静态方法都只有一份。 实例方法：属于具体的对象实例，每个对象实例都有自己的实例方法。 7. 重载和重写有什么区别？ 重载（Overloading）\n重载是指在同一个类中，允许定义多个同名方法，但这些方法的参数列表必须不同\n重载发生在编译期，编译器通过参数列表来区分不同的方法。\n重载就是同⼀个类中多个同名方法根据不同的传参来执行不同的逻辑处理。\n重写\n重写发生在运行期，是⼦类对父类的允许访问的方法的实现过程进行重新编写。\n方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的 异常范围小于等于父类，访问修饰符范围大于等于父类。如果方法的返回 类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写 时是可以返回该引⽤类型的子类的。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 **口诀：**方法的重写要遵循“两同两小一大”\n“两同”即方法名相同、形参列表相同；\n“两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等；\n“一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。\n8. Java 中的几种基本数据类型了解么？ Java 中有 8 种基本数据类型，分别为：\n4 种整数型： byte 、 short 、 int 、 long\n2 种浮点型： float （4）、 double（8）\n1 种字符类型：char（2）\n1 种布尔型： boolean\nbyte、short、int、long能表示的最大正数都减 1 了。这是为什么呢？这是因为在二进制补码表示法中，最高位是用来表示符号的（0 表示正数，1 表示负数），其余位表示数值部分。所以，如果我们要表示最大的正数，我们需要把除了最高位之外的所有位都设为 1。如果我们再加 1，就会导致溢出，变成一个负数。\n9. 基本类型和包装类型的区别？ 成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null 。 包装类型可用于泛型，而基本类型不可以。 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。 相比于对象类型，基本数据类型占用的空间非常小。 ⚠ 注意：基本数据类型存放在栈中是一个常见的误区！基本数据类型的成员变量如果没有被 static 修饰的话（不建议这么使用，应该要使用基本数据类型对应的包装类型），就存放在堆中。\n10. 自动装箱与拆箱了解吗？原理是什么？ 装箱：将基本类型用它们对应的引用类型包装起来；\n拆箱：将包装类型转换为基本数据类型；\n装箱其实就是调用了 包装类的 valueOf() 方法，拆箱其实就是调用了 xxxValue() ⽅法。\nInteger i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue() ; 11. 面向对象三大特征 1. 封装 封装是将对象的状态信息（即属性）隐藏在对象内部，不允许外部直接访问这些内部信息。但可以通过一些公开的方法来操作这些属性。例如，我们无法看到挂在墙上的空调内部的零件，但可以通过遥控器来控制空调。如果某些属性不想被外界访问，可以选择不提供相应的方法。但如果一个类没有任何可访问的方法，那么这个类就失去了意义。\n2. 继承 不同类型的对象通常有一些共同点。例如，小明、小红和小李都是学生，共享学生的一些特性（如班级、学号等）。同时，每个对象也有其独特的特性。继承是一种使用已存在的类定义作为基础来创建新类的技术。新类可以增加新的数据和功能，也可以使用父类的功能，但不能选择性地继承父类。继承可以快速创建新类，提高代码复用性，增强程序的可维护性，节省开发时间。\n关于继承的三个要点：\n子类拥有父类的所有属性和方法（包括私有属性和方法），但无法访问父类中的私有属性和方法。 子类可以拥有自己的属性和方法，即可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 3. 多态 多态表示一个对象可以具有多种状态，具体表现为父类的引用指向子类的实例。多态的特点包括：\n对象类型和引用类型之间必须存在继承（类）或实现（接口）的关系。 引用类型变量调用的方法在运行时才能确定。 多态不能调用“只在子类中存在而在父类中不存在”的方法。 如果子类重写了父类的方法，调用的是子类覆盖的方法；如果没有覆盖，则调用父类的方法。 12. 接口和抽象类有什么共同点和区别？ 共同点\n都不能被实例化。 都可以包含抽象方法。 都可以有默认实现的方法（Java 8 可以用default关键字在接口中定义默认方法）。 区别\n接口： 主要用于对类的行为进行约束，实现接口的类具有对应的行为。 一个类可以实现多个接口。 接口中的成员变量只能是public static final类型的，不能被修改且必须有初始值。 抽象类： 主要用于代码复用，强调的是所属关系。 一个类只能继承一个抽象类。 抽象类的成员变量默认是default，可以在子类中被重新定义或赋值。 13. 深拷贝、浅拷贝和引用拷贝的区别 引用拷贝是指两个不同的引用指向同一个对象。这种方式不会创建新的对象，只是将一个引用赋值给另一个引用。\n浅拷贝会在堆上创建一个新的对象，但不会复制对象内部的引用类型属性。也就是说，浅拷贝的对象和原对象共享内部的引用类型属性。\n深拷贝会完全复制整个对象，包括对象内部的所有引用类型属性。深拷贝的对象和原对象完全独立，不共享任何引用类型属性。\n14. == 和 equals() 的区别 == 的作用： 基本数据类型：比较两个值是否相等。 引用数据类型：比较两个引用是否指向同一个对象，即比较内存地址。 注意：Java中只有值传递，无论是比较基本数据类型还是引用数据类型，== 比较的都是值。对于引用类型，值是对象的内存地址。 equals() 的作用： 基本数据类型：equals() 方法不能用于基本数据类型，只能用于对象。 引用数据类型：equals() 方法用于比较两个对象的内容是否相等。equals() 方法存在于 Object 类中，所有类都继承自 Object 类，因此所有类都有 equals() 方法。 默认行为：Object 类中的 equals() 方法默认比较对象的内存地址，等同于 ==。 重写行为：通常我们会重写 equals() 方法来比较对象的属性是否相等。如果两个对象的属性相等，则返回 true，表示这两个对象相等。 15. hashCode() 有什么用？ hashCode() 的作用是获取哈希码（一个整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode() 定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 方法。另外需要注意的是：Object 的 hashCode() 方法是本地方法，也就是用 C 语言或 C++ 实现的，该方法通常用来将对象的内存地址转换为整数之后返回。\n散列表存储的是键值对（key-value），它的特点是：能根据“键”快速检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象）\n为什么要有 hashCode()？\n我们以“HashSet 如何检查重复”为例来说明为什么要有 hashCode()。当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们大大减少了 equals 的次数，相应就大大提高了执行速度。\nhashCode() 和 equals() 都是用来比较两个对象是否相等的。那为什么 JDK 还要同时提供这两个方法呢？\n这是因为在一些容器（比如 HashMap、HashSet）中，有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高（参考添加元素进 HashSet 的过程）！我们在前面也提到了添加元素进 HashSet 的过程，如果 HashSet 在对比的时候，同样的 hashCode 有多个对象，它会继续使用 equals() 来判断是否真的相同。也就是说 hashCode 帮助我们大大缩小了查找成本。\n那为什么不只提供 hashCode() 方法呢？这是因为两个对象的 hashCode 值相等并不代表两个对象就相等。那为什么两个对象有相同的 hashCode 值，它们也不一定相等呢？\n因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 hashCode）。\n为什么重写 equals() 时必须重写 hashCode() 方法？\n因为两个相等的对象的 hashCode 值必须是相等。也就是说如果 equals() 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。如果重写 equals() 时没有重写 hashCode() 方法的话就可能会导致 equals() 方法判断是相等的两个对象，hashCode 值却不相等。\n16. String、StringBuffer、StringBuilder 的区别？ 线程安全性\nString 中的对象是不可变的，也就可以理解为常量，线程安全。 AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。 StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。 StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能\n每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。 StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 String 为什么是不可变的？\n保存字符串的数组被 final 修饰且为私有的，并且 String 类没有提供/暴露修改这个字符串的方法。 String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。 字符串拼接用“+” 还是 StringBuilder?\n字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。如果直接使用 StringBuilder 对象进行字符串拼接的话，就不会存在这个问题了。 String#equals() 和 Object#equals() 有何区别？\nString 中的 equals 方法是被重写过的，比较的是 String 字符串的值是否相等。 Object 的 equals 方法比较的是对象的内存地址。 字符串常量池的作用了解吗？\n字符串常量池是 JVM 为了提升性能和减少内存消耗针对字符串（String 类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建。 intern 方法有什么作用？\nString.intern() 是一个 native（本地）方法，其作用是将指定的字符串对象的引用保存在字符串常量池中，可以简单分为两种情况： 如果字符串常量池中保存了对应的字符串对象的引用，就直接返回该引用。 如果字符串常量池中没有保存了对应的字符串对象的引用，那就在常量池中创建一个指向该字符串对象的引用并返回。 17. Exception 和 Error 的区别 在 Java 里，所有异常都有一个共同的祖先，即java.lang包中的Throwable类。Throwable类有两个重要的子类：\nException：是程序本身可以处理的异常，能够通过catch语句来捕获。它又可细分为Checked Exception（受检查异常，必须处理）和Unchecked Exception（不受检查异常，可以不处理）。 Error：属于程序无法处理的错误，不建议通过catch捕获。例如Java虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误（OutOfMemoryError）、类定义错误（NoClassDefFoundError）等。当这些异常发生时，Java虚拟机（JVM）通常会选择终止线程。 18. Checked Exception 和 Unchecked Exception 的区别 Checked Exception（受检查异常）：在 Java 代码编译过程中，如果受检查异常没有被catch或者throws关键字处理，代码就无法通过编译。除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常。常见的受检查异常有：与IO相关的异常、ClassNotFoundException、SQLException等。\nUnchecked Exception（不受检查异常）：在 Java 代码编译过程中，即使不处理不受检查异常，代码也能正常通过编译。\nRuntimeException及其子类都被统称为非受检查异常，常见的有：\nNullPointerException（空指针错误） IllegalArgumentException（参数错误，比如方法入参类型错误） NumberFormatException（字符串转换为数字格式错误，是IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） ArithmeticException（算术错误） SecurityException（安全错误，比如权限不够） UnsupportedOperationException（不支持的操作错误，比如重复创建同一用户 19. Throwable 类常用方法 String getMessage()：返回异常发生时的简要描述。 String toString()：返回异常发生时的详细信息。 String getLocalizedMessage()：返回异常对象的本地化信息。Throwable的子类覆盖这个方法，可以生成本地化信息。若子类没有覆盖该方法，则该方法返回的信息与getMessage()返回的结果相同。 void printStackTrace()：在控制台上打印Throwable对象封装的异常信息。 20. try-catch-finally 的使用 try 块：用于捕获异常。其后可以接零个或多个catch块，如果没有catch块，则必须跟一个finally块。 catch 块：用于处理try捕获到的异常。 finally 块：无论是否捕获或处理异常，finally块里的语句都会被执行。当在try块或catch块中遇到return语句时，finally语句块将在方法返回之前被执行。 注意：不要在finally语句块中使用return！当try语句和finally语句中都有return语句时，try语句块中的return语句会被忽略。这是因为try语句中的return返回值会先被暂存在一个本地变量中，当执行到finally语句中的return之后，这个本地变量的值就变为了finally语句中的return返回值。\n21. finally 中的代码一定会执行吗？ 不一定。在某些情况下，finally中的代码不会被执行，比如：\nfinally之前虚拟机被终止运行，例如使用System.exit(1)终止当前正在运行的 Java 虚拟机。 程序所在的线程死亡。 关闭 CPU。 22. 异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常，比如字符串转换为数字格式错误的时候应该抛出 NumberFormatException 而不是其父类 IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在于一段代码逻辑中）。 23. 反射 定义：反射赋予了 Java 程序在运行时分析类以及执行类中方法的能力。借助反射，能够获取任意一个类的所有属性和方法，并且可以调用这些方法和属性。若深入研究框架底层原理或自行编写框架，对反射概念会较为熟悉，因其被称为框架的灵魂。 优缺点 优点：使代码更灵活，为各种框架实现开箱即用的功能提供便利。 缺点：增加安全问题，例如可无视泛型参数在编译时的安全检查；性能相对稍差，但对框架实际影响不大。 应用场景 在业务开发中，直接使用反射机制的场景较少。然而，众多框架如 Spring/Spring Boot、MyBatis 等大量运用了反射机制。 动态代理在这些框架中广泛使用，而动态代理的实现依赖反射。例如 JDK 实现动态代理的示例代码，通过反射类Method调用指定方法。 注解的实现也用到反射。以 Spring 为例，使用@Component注解声明一个类为 Spring Bean，通过@Value注解读取配置文件中的值，其原理是基于反射分析类，获取类、属性、方法及方法参数上的注解，进而进行后续处理。 24. 序列化与反序列化 如果我们需要持久化 Java 对象比如将 Java 对象保存在文件中，或者在网络传输 Java 对象，这些场景都需要用到序列化。\n定义 序列化：将数据结构或对象转换为二进制字节流的过程。当需要持久化 Java 对象，如保存在文件中或在网络传输 Java 对象时，会用到序列化。 反序列化：将序列化过程中生成的二进制字节流转换回数据结构或对象的过程。 不想序列化某些字段的处理方式：对于不想进行序列化的变量，使用transient关键字修饰。transient关键字作用为阻止实例中被其修饰的变量序列化；对象反序列化时，被transient修饰的变量值不会被持久化和恢复。需注意：transient只能修饰变量，不能修饰类和方法；被其修饰的变量，反序列化后值将被置为类型默认值（如int类型为 0）；static变量不属于任何对象，无论是否有transient修饰，均不会被序列化。 26. Java IO 流了解吗？ Java IO流概述：Java IO流是Java中用于处理输入输出的核心机制，数据输入到计算机内存的过程称为输入，从内存输出到外部存储（如文件、数据库、远程主机等）的过程称为输出。由于数据传输过程类似于水流，因此形象地称为IO流。Java IO流分为输入流和输出流，根据数据处理方式又分为字节流和字符流。 抽象基类：Java IO流的40多个类都是从4个抽象类基类中派生出来的，分别是InputStream和Reader（输入流的基类，前者是字节输入流，后者是字符输入流），以及OutputStream和Writer（输出流的基类，前者是字节输出流，后者是字符输出流）。 Buffered类的作用：为了提高IO操作的效率，Java提供了BufferedInputStream、BufferedOutputStream、BufferedReader和BufferedWriter这几个类。它们分别继承自对应的输入输出流类，并在内部使用缓冲区来减少物理读写次数。 对于输入流：BufferedInputStream和BufferedReader会在第一次读取数据时，将数据块一次性读入缓冲区，之后的读取操作先从缓冲区中获取数据，只有当缓冲区中的数据被读完后，才会再次从底层数据源读取数据填充缓冲区。这种方式减少了对底层数据源的频繁访问，提高了读取效率。例如，当读取大文件时，使用BufferedReader可以显著提升读取速度。 对于输出流：BufferedOutputStream和BufferedWriter会先将要写入的数据暂存到缓冲区中，当缓冲区满或者调用flush()方法时，才将缓冲区中的数据一次性写入目标存储。这样可以减少对目标存储的写入次数，提高写入效率，同时也能避免频繁写入导致的性能问题。比如在写入大量文本数据到文件时，使用BufferedWriter可以更高效地完成写入操作。 27. 为什么Java的I/O流要分为字节流和字符流？ 主要有以下两个原因：\n字符流涉及编码转换： 字符流是由Java虚拟机将字节转换得到的。这个转换过程涉及到字符编码（如UTF-8、ISO-8859-1等），而不同的编码方式会导致相同的字节序列被解释为不同的字符。因此，字符流的处理相对复杂，需要考虑编码问题，这个过程比较耗时。 例如，一个字节序列在UTF-8编码下可能表示一个字符，而在ISO-8859-1编码下可能表示另一个字符。字符流通过指定编码方式，确保字符的正确读取和写入。 字节流和字符流的适用场景不同： 字节流：适用于处理二进制数据，如文件的读写、网络通信等。字节流直接操作字节，不涉及编码转换，因此效率较高，适合处理二进制文件（如图片、视频等）。 字符流：适用于处理文本数据。字符流在处理文本时会自动处理编码问题，确保文本内容的正确性，适合处理文本文件（如.txt、.java等）。 28. 泛型 指定参数，编译器可以对泛型参数进行检测，并且通过泛型参数可以指定传入的对象类型。\n比如 ArrayList\u0026lt;Person\u0026gt; persons = new ArrayList\u0026lt;Person\u0026gt;() 这行代码就指明了该 ArrayList 对象只能传入 Person 对象，如果传入其他类型的对象就会报错。\n29. 为什么重写 equals () 就一定要重写 hashCode () 方法 在 Java 中，若仅重写 equals() 方法而不重写 hashCode() 方法，可能会导致 a.equals(b) 表达式为 true，但 a 和 b 的 hashCode 值却不同。这在使用散列集合（如 HashMap、HashSet 等）存储对象时会引发问题。因为散列集合利用 hashCode 计算键的存储位置，若存储两个相等的对象却有不同的 hashCode，它们会被存于哈希表的不同位置。当依据对象获取数据时，就会出现矛盾，即相同对象存于哈希表的不同位置，进而引发不可预期的错误。所以，为保证对象在散列集合中的正确存储和查找，重写 equals() 方法时必须重写 hashCode() 方法。\n30. Integer 和 int 的区别以及 Java 设计封装类的原因 区别\n初始值不同：Integer 作为引用类型，初始值为 null；int 作为基本数据类型，初始值为 0。 存储位置不同：Integer 对象存储在堆内存，int 类型的变量直接存储在栈空间。 类型及使用灵活性不同：Integer 是对象类型，封装了众多方法和属性，使用起来更加灵活；int 是基本数据类型，功能相对单一。 设计封装类的原因\nJava 是面向对象的编程语言，其操作大多基于对象。例如，集合类（如 List、Set 等）只能存储 Object 类型的元素，基本数据类型无法直接存储在集合中，必须使用对应的封装类。因此，Java 设计封装类的主要目的是让基本数据类型也能参与面向对象的操作，增强代码的灵活性和可扩展性。\n31. 如何理解Java对象的创建过程 类加载检查：当程序尝试实例化一个对象时，JVM 首先会检查该对象所属的类是否已经被加载到内存中。如果该类尚未被加载，JVM 会启动类加载机制，通过类加载器（ClassLoader）查找并加载对应的字节码文件（.class 文件）。在类加载过程中，会进行一系列的验证、准备和解析操作，为类的使用做好准备。例如，验证字节码文件的格式是否正确，为类的静态变量分配内存并设置初始值等。 内存分配：一旦类被成功加载，JVM 就会为新对象分配内存空间。对象所需的内存大小在类加载完成后就已经确定，JVM 会在堆内存中为其分配一块合适的内存区域。内存分配的方式通常有两种：指针碰撞和空闲列表。指针碰撞是指 JVM 维护一个指向堆内存中可用内存的指针，当需要分配内存时，就将指针移动相应的距离来为新对象分配空间；空闲列表则是 JVM 维护一个记录堆内存中可用内存块的列表，每次分配内存时，从列表中选择合适的内存块分配给对象。 初始化零值：在为对象分配完内存后，JVM 会将分配的内存空间初始化为零值（对于基本数据类型）或 null（对于引用数据类型）。这一步确保了对象的实例变量在被程序员显式赋值之前有一个默认的初始状态。例如，int 类型的实例变量会被初始化为 0，Object 类型的实例变量会被初始化为 null。 设置对象头：对象头是对象在内存中的一个重要组成部分，它包含了对象的一些元数据信息，如对象的哈希码（HashCode）、对象的分代年龄、锁状态标志等。JVM 会在这一步设置对象头中的这些信息，以便后续对对象的管理和操作。 执行构造方法：完成上述步骤后，JVM 会调用对象的构造方法（constructor），执行程序员在构造方法中编写的代码逻辑，对对象进行进一步的初始化。在构造方法中，程序员可以为对象的实例变量赋予特定的值，进行一些必要的资源初始化或其他操作，使对象处于可用的状态。 七、JVM基础 1. JVM 是什么？ 定义：JVM 即 Java Virtual Machine，是 Java 程序的运行环境，也是运行 Java 二进制字节码的环境。 好处：实现了 “一次编写，到处运行” 的特性，因为 JVM 屏蔽了底层操作系统的差异，使得 Java 程序在不同的操作系统上都能以相同的方式运行；具备自动内存管理和垃圾回收机制，减轻了开发者对内存管理的负担，提高了开发效率，并且降低了因内存管理不当导致的错误风险。 2. JVM 由哪些部分组成，运行流程是什么？ 组成部分 ClassLoader（类加载器）：负责加载 Java 类文件，将 Java 代码转换为字节码，使 JVM 能够识别和处理。 Runtime Data Area（运行时数据区，内存分区）：是 JVM 运行时管理内存的区域，包括堆、方法区、栈、本地方法栈和程序计数器等。 Execution Engine（执行引擎）：执行字节码指令，将字节码翻译为底层系统能够理解和执行的指令，最终交由 CPU 执行。 Native Method Library（本地库接口）：当 Java 程序需要调用其他语言（如 C、C++）编写的本地代码时，通过该接口实现调用，以完成一些 Java 语言本身无法直接实现的功能。 运行流程 类加载器把 Java 代码编译后的字节码文件加载到 JVM 中。 运行时数据区将字节码加载到内存中进行存储和管理。由于字节码是 JVM 的指令集规范，不能直接被底层系统执行，所以需要进一步处理。 执行引擎将字节码翻译为底层系统指令，在这个过程中可能会调用本地库接口来实现一些功能，最终由 CPU 执行这些指令，完成程序的运行。 3. 什么是程序计数器？ 定义：程序计数器是线程私有的内存区域，内部保存着字节码的行号，用于记录当前线程正在执行的字节码指令的地址。 作用：在多线程环境下，JVM 通过线程轮流切换并分配执行时间来实现多线程的并发执行。当一个线程的执行时间用完被挂起，处理器切换到其他线程执行后，再次切换回该线程时，就可以从程序计数器中获取上次执行的行号，从而继续向下执行，保证了线程执行的连续性和正确性。 特点：它是 JVM 规范中唯一一个没有规定会出现 OutOfMemoryError（OOM）的区域，并且该区域也不会进行垃圾回收（GC）。 查看工具：使用 javap -verbose xx.class 命令可以打印堆栈大小、局部变量的数量和方法的参数等信息，这对于了解程序的字节码结构和分析问题有一定帮助。 4. Java 堆是什么样的？ 概述：Java 堆是线程共享的内存区域，主要用于保存对象实例和数组等数据。当堆中没有足够的内存空间分配给新的对象实例，且无法再进行扩展时，就会抛出 OutOfMemoryError 异常。 分区 年轻代：被划分为三部分，分别是 Eden 区和两个大小相同的 Survivor 区（一般称为 From Survivor 和 To Survivor）。在垃圾收集过程中，新创建的对象通常会首先分配到 Eden 区，经过几次垃圾收集后，仍然存活在 Survivor 区的对象会根据 JVM 的策略被移动到老年代。 老年代：主要保存生命周期较长的对象，比如一些长时间存在的对象实例，这些对象在年轻代经过多次垃圾回收后依然存活，就会被移动到老年代。 元空间（Java 8 及以后）：在 Java 8 中，为了避免方法区（之前的永久代）出现 OOM 问题，将其移动到了本地内存，重新开辟了一块空间称为元空间。元空间主要保存**类信息、静态变量、常量、编译后的代码等数据。**它的大小默认仅受本地内存的限制，相比之前的永久代，更加灵活，减少了因永久代内存不足导致的 OOM 情况。 5. 什么是虚拟机栈？ 定义：每个线程运行时所需要的内存空间称为虚拟机栈，它遵循先进后出（FILO）的原则。每个栈由多个栈帧（frame）组成，每个栈帧对应着每次方法调用时所占用的内存。 栈帧相关：每个线程在任何时刻只能有一个活动栈帧，这个活动栈帧对应着当前正在执行的方法。当一个方法被调用时，就会创建一个新的栈帧并压入栈中；当方法执行完毕时，栈帧会从栈中弹出，释放相应的内存。 与垃圾回收的关系：垃圾回收主要针对堆内存，当栈帧弹栈后，其所占用的内存会自动释放，因此栈内存的管理相对简单，一般不需要垃圾回收机制来处理。 栈内存大小影响：栈内存并非越大越好。默认情况下，栈内存通常为 1024k。如果栈内存设置过大，会导致线程数变少。例如，假设机器总内存为 512m，按照默认栈内存 1024k 计算，能活动的线程数为 512 个；如果将栈内存改为 2048k，那么能活动的线程数就会减半。 局部变量线程安全性 如果方法内的局部变量没有逃离方法的作用范围，即该变量只在方法内部使用，那么它是线程安全的，因为不同线程不会同时访问到这个局部变量。 如果局部变量引用了对象，并且该引用逃离了方法的作用范围，例如作为方法的返回值被其他线程获取到，此时就需要考虑线程安全问题，因为多个线程可能会同时访问和修改这个对象，从而导致数据不一致或其他错误。 栈内存溢出情况 栈帧过多会导致栈内存溢出，典型的情况是递归调用。如果递归调用没有正确设置终止条件，会不断创建新的栈帧，最终导致栈内存耗尽，抛出 java.lang.StackOverFlowError 异常。 栈帧过大也可能导致栈内存溢出，因为每个栈帧占用的空间过大，会使得栈能够容纳的栈帧数量减少，从而更容易出现内存不足的情况。 栈帧的压入与弹出：方法调用的数据通过栈传递。每次方法调用，会对应一个栈帧被压入栈中；方法调用结束，对应的栈帧被弹出。\n栈帧的组成\n局部变量表：存放编译期可知的多种数据类型，包括基本数据类型（如 boolean、byte、char、short、int、float、long、double）以及对象引用（reference 类型，可能是指向对象起始地址的引用指针，也可能是指向代表对象的句柄或其他相关位置）。 操作数栈：作为方法调用的中转站，用于存储方法执行过程中产生的中间计算结果，计算时的临时变量也存放于此。 动态链接：主要用于当一个方法需要调用其他方法的场景。Class 文件常量池存有大量符号引用（如方法引用的符号引用），动态链接负责将常量池中指向方法的符号引用转换为内存地址中的直接引用，这一过程也称为动态连接。 方法返回地址：方法执行结束时，用于确定返回的位置。 6. JVM 运行时数据区有哪些组成部分，各自的作用是什么？ 堆：主要用于存储对象实例，是垃圾回收器管理的主要区域。对象的创建、存储和销毁等操作都与堆内存密切相关。 方法区：可以看作是堆的一部分，用于存储已被虚拟机加载的类信息、常量、静态变量以及即时编译器编译后的代码等。在 Java 8 之前，方法区的实现是永久代；Java 8 及以后，方法区由元空间实现，元空间使用本地内存，避免了永久代常见的 OOM 问题。 栈：解决程序运行时的方法调用和局部变量存储问题。栈中存储的是栈帧，每个栈帧包含局部变量表、操作数栈、动态链接、方法出口等信息。 本地方法栈：与栈的功能类似，主要用于执行本地方法，即 Java 调用非 Java 代码（如 C、C++ 代码）的接口。当 Java 程序需要调用本地方法时，会在本地方法栈中创建相应的栈帧来执行这些方法。 程序计数器：如前面所述，它是线程私有的，保存当前线程正在执行的字节码指令的地址，用于线程切换后恢复执行位置。 7. 解释一下方法区？ 概述 方法区是各个线程共享的内存区域，在虚拟机启动时创建，关闭虚拟机时释放。 主要存储类的信息（如类的结构、字段、方法等）、运行时常量池等数据。如果方法区域中的内存无法满足分配请求，就会抛出 OutOfMemoryError: Metaspace 异常（在 Java 8 及以后使用元空间实现方法区）。 常量池：可以看作是一张表，虚拟机指令根据这张表找到要执行的类名、方法名、参数类型、字面量等信息。通过 javap -v xx.class 命令可以查看字节码结构，包括类的基本信息、常量池和方法定义等。例如，对于一个包含 main 方法的 Application 类，执行该命令后，可以看到常量池中存储了类名、方法名、字段引用、字符串常量等信息，main 方法在执行时，需要到常量池中查找具体的类和方法地址进行调用。 运行时常量池：常量池是存在于 .class 文件中的，当该类被加载到 JVM 中时，它的常量池信息会被放入运行时常量池，并将其中的符号地址转换为真实地址，以便 JVM 能够正确执行字节码指令。 8. 什么是直接内存，它有什么特点和应用场景？ 定义：直接内存不受 JVM 内存回收管理，它是虚拟机使用的系统内存。 特点：常见于 NIO（New IO）操作中，用于数据缓冲区。直接内存的分配和回收成本相对较高，因为它需要与操作系统进行交互来申请和释放内存。但是，它的读写性能很高，这是因为它不需要在堆内存和系统内存之间进行数据拷贝，JVM 可以直接操作直接内存。 应用场景：以将本地电脑中一个较大的文件（如超过 100m 的视频文件）从一个磁盘挪到另一个磁盘为例，使用传统的 IO 操作和 NIO 操作进行对比。通过代码测试可以发现，使用传统 IO 操作的时间要比 NIO 操作的时间长很多，这是因为 NIO 操作利用了直接内存的特性，减少了数据拷贝的开销，从而提高了数据传输的效率。在传统阻塞 IO 中，数据需要先从磁盘读取到操作系统的缓冲区，再从操作系统缓冲区拷贝到 JVM 的堆内存，然后再进行后续处理；而在 NIO 中，数据可以直接读取到直接内存中，JVM 可以直接对其进行操作，大大提高了读写性能。 9. 堆栈的区别是什么？ 存储内容不同：栈内存一般用于存储局部变量和方法调用相关的信息，例如方法的参数、局部变量等；而堆内存主要用于存储 Java 对象和数组等数据。 内存管理方式不同：堆内存会进行 GC 垃圾回收，当对象不再被引用时，垃圾回收器会自动回收其占用的内存；而栈内存的管理相对简单，当栈帧弹栈时，其所占用的内存会自动释放，不需要垃圾回收机制。 线程相关性不同：栈内存是线程私有的，每个线程都有自己独立的栈，线程之间的栈内存相互隔离；而堆内存是线程共有的，多个线程可以同时访问和操作堆中的对象。 异常类型不同：如果栈内存不足，会抛出 java.lang.StackOverFlowError 异常，通常是由于递归调用过深或栈帧过大导致栈空间耗尽；如果堆内存不足，会抛出 java.lang.OutOfMemoryError 异常，例如创建过多的对象导致堆内存被耗尽。嘻嘻 10. 什么是类加载器，类加载器有哪些? 类加载器\nJVM只会运行二进制文件，而类加载器（ClassLoader）的主要作用就是将字节码文件加载到JVM中，从而让Java程序能够启动起来。现有的类加载器基本上都是java.lang.ClassLoader的子类，该类的主要职责就是用于将指定的类找到或生成对应的字节码文件，同时类加载器还会负责加载程序所需要的资源。\n类加载器种类\n类加载器根据各自加载范围的不同，划分为四种类加载器：\n启动类加载器(BootStrap ClassLoader)： 该类并不继承ClassLoader类，其是由C++编写实现。用于加载JAVA_HOME/jre/lib目录下的类库。 扩展类加载器(ExtClassLoader)： 该类是ClassLoader的子类，主要加载JAVA_HOME/jre/lib/ext目录中的类库。 应用类加载器(AppClassLoader)： 该类是ClassLoader的子类，主要用于加载classPath下的类，也就是加载开发者自己编写的Java类。 自定义类加载器： 开发者自定义类继承ClassLoader，实现自定义类加载规则。 类加载器的体系并不是“继承”体系，而是委派体系，类加载器首先会到自己的parent中查找类或者资源，如果找不到才会到自己本地查找。类加载器的委托行为动机是为了避免相同的类被加载多次。\n11. 什么是双亲委派模型？ 如果一个类加载器在接到加载类的请求时，它首先不会自己尝试去加载这个类，而是把这个请求任务委托给父类加载器去完成，依次递归，如果父类加载器可以完成类加载任务，就返回成功；只有父类加载器无法完成此加载任务时，才由下一级去加载。\n12. JVM为什么采用双亲委派机制 （1）通过双亲委派机制可以避免某一个类被重复加载，当父类已经加载后则无需重复加载，保证唯一性。\n（2）为了安全，保证类库API不会被修改\n13. 说一下类装载的执行过程？ 类从加载到虚拟机中开始，直到卸载为止，它的整个生命周期包括了：加载、验证、准备、解析、初始化、使用和卸载这7个阶段。其中，验证、准备和解析这三个部分统称为连接（linking）。\n加载: 查找和导入class文件 验证: 保证加载类的准确性 准备: 为类变量分配内存并设置类变量初始值 解析: 把类中的符号引用转换为直接引用 初始化: 对类的静态变量，静态代码块执行初始化操作 使用: JVM 开始从入口方法开始执行用户的程序代码 卸载: 当用户程序代码执行完毕后，IVM便开始销毁创建的Class对象。\n一、加载\n核心任务：通过类的全限定名（类的全名），获取该类的二进制数据流。这通常涉及到从文件系统、网络或其他存储位置读取类的字节码文件（.class文件）。接着，将类的二进制数据流解析为方法区内特定的数据结构，即构建起符合 Java 类模型的数据表示。最后，创建java.lang.Class类的实例，这个实例作为后续对该类各种数据（如字段、方法等）进行访问的入口。 示例说明：假设存在一个User类，类加载器会根据User类的全限定名找到对应的.class文件，读取其字节码数据，将其转换为 JVM 内部能够理解的类模型结构，并创建Class\u0026lt;User\u0026gt;实例，后续可以通过这个实例获取User类的相关信息。 二、连接\n验证 目的：确保被加载的类符合 JVM 规范，是安全可靠的，防止恶意或错误的字节码对 JVM 运行造成危害。 具体内容 文件格式验证：检查字节码文件是否符合Class文件的规范，例如文件的魔数、版本号、常量池格式等是否正确。 元数据验证：验证类的元数据信息，如类是否有父类（除Object外应都有父类）、是否继承了被final修饰的类（不允许继承）、类中的字段和方法是否与父类存在矛盾（如覆盖final方法或字段）等。 字节码验证：对字节码进行数据流和控制流分析，确保程序语义合法、逻辑正确，例如检查跳转指令是否指向合法的位置、操作数栈的操作是否正确等。 符号引用验证：验证符号引用的有效性，如类、字段、方法等的符号引用是否能正确解析到目标。 准备 内存分配：为类变量（被static修饰的变量）分配内存空间。此时，类变量会被赋予默认初始值，例如int类型变量初始化为0，boolean类型变量初始化为false等。 特殊情况：对于static且为final的基本类型变量，以及字符串常量，其值在编译期就已确定，会在准备阶段直接赋值。而对于static且为final的引用类型变量，赋值操作则在初始化阶段完成。 解析 任务：将类中的符号引用转换为直接引用。符号引用是一组用于描述所引用目标的符号，与虚拟机实现的内存布局无关；而直接引用是可以直接指向目标的指针、相对偏移量或能间接定位目标的句柄，与虚拟机内存布局直接相关。 示例：当一个方法中调用了其他方法时，方法名在编译期是符号引用，在解析阶段会将其转换为指向实际方法内存地址的直接引用，使得 JVM 能够直接调用该方法。 三、初始化\n**静态成员初始化：**对类的静态变量和静态代码块执行初始化操作。 父类优先：如果初始化一个类时，其父类尚未初始化，则优先初始化其父类。这保证了类继承体系中，父类的静态资源先被初始化，子类可以基于父类已初始化的状态进行自身的初始化。 顺序执行：当类中同时包含多个静态变量和静态代码块时，按照自上而下的顺序依次执行。例如，先定义的静态变量会先于后面的静态代码块以及后续定义的静态变量被初始化。 触发时机：初始化阶段通常在以下几种情况下触发：创建类的实例（使用new关键字）、调用类的静态方法、访问类的静态字段（除final常量外，因为其在准备阶段已赋值）、反射调用类等。 四、使用\n执行入口方法：JVM 从程序的入口方法（如main方法）开始执行用户的程序代码。这是程序运行的起点，后续的代码执行和逻辑处理都基于此展开。 调用静态成员：在程序执行过程中，可以调用类的静态成员信息，包括静态字段和静态方法。静态成员是类级别的资源，可被类的所有实例共享访问，常用于实现与类相关的通用功能或存储类级别的数据。 创建对象实例：使用new关键字为类创建对象实例。创建对象时，会在堆内存中分配空间，初始化对象的成员变量，并调用对象的构造函数完成对象的初始化。创建的对象实例可以访问类的非静态成员，执行对象特定的行为和操作。 五、卸载\n当用户程序代码执行完毕后，JVM 会开始销毁之前创建的Class对象。这包括释放与类相关的所有内存资源，如类的元数据信息（存储在方法区）、静态变量所占用的内存等。随着所有相关Class对象被销毁，负责运行程序的 JVM 也会退出内存，整个类加载的生命周期结束。\n通过对类加载各个阶段的详细了解，可以更好地理解 Java 程序在运行时的行为和机制，对于排查问题、优化性能等方面具有重要意义。\n14. 简述Java垃圾回收机制？（GC是什么？为什么要GC） 为了让程序员更专注于代码的实现，而不用过多的考虑内存释放的问题，所以，在Java语言中，有了自动的垃圾回收机制，也就是我们熟悉的GC(Garbage Collection)。\n有了垃圾回收机制后，程序员只需要关心内存的申请即可，内存的释放由系统自动识别完成。\n在进行垃圾回收时，不同的对象引用类型，GC会采用不同的回收时机\n换句话说，自动的垃圾回收的算法就会变得非常重要了，如果因为算法的不合理，导致内存资源一直没有释放，同样也可能会导致内存溢出的。\n15. 对象什么时候可以被垃圾器回收 简单一句就是：如果一个或多个对象没有任何的引用指向它了，那么这个对象现在就是垃圾，如果定位了垃圾，则有可能会被垃圾回收器回收。\n如果要定位什么是垃圾，有两种方式来确定，第一个是引用计数法，第二个是可达性分析算法。\n16. 定位垃圾的方法 1. 引用计数法\n原理：对象每被引用一次，其引用计数就递增一次；当引用计数为 0 时，该对象可被回收。例如String demo = new String(\u0026quot;123\u0026quot;);，此时demo对象引用计数为 1，当demo = null;时，demo对象引用计数降为 0，可被回收。 缺点：对象间存在循环引用时会失效。例如两个对象相互引用，即使它们不再被外部引用，引用计数也不会为 0，导致无法回收，引发内存泄漏。同时，每次对象引用状态改变都要更新计数器，有时间开销且浪费 CPU 资源，即便内存充足也持续统计。 2. 可达性分析算法\n原理：以 GC Roots 为根节点，通过引用关系向下遍历所有对象。若对象与根节点无直接或间接引用，则可被判定为垃圾回收对象。GC Roots 包括虚拟机栈（栈帧中的本地变量表）中引用的对象、方法区中类静态属性引用的对象、方法区中常量引用的对象以及本地方法栈中 JNI 引用的对象。 对象回收特殊情况：对象被标记为可回收后，若其finalize方法未执行，GC 时会先执行该方法。在finalize方法中，若设置对象与 GC ROOTS 产生关联，则 GC 再次判断时该对象可达，不会被回收；若仍不可达，则进行回收。且finalize方法对每个对象仅执行一次。 17. 垃圾回收算法 （一）标记清除算法\n流程：分标记和清除两个阶段。先根据可达性分析算法标记所有可回收对象，然后对标记对象进行回收。 优缺点：能解决引用计数法的循环引用问题，但效率低，标记和清除都需遍历所有对象，且 GC 时需停止应用程序，影响交互性。清理出的内存碎片化严重，因为被回收对象分散在内存各处，导致内存不连贯。 （二）复制算法\n流程：将内存空间一分为二，每次只使用其中一块。垃圾回收时，将正在使用块中的存活对象复制到另一块，然后清空当前使用块，两块内存角色交换，重复此过程。 适用场景及优缺点：当内存中垃圾对象较多，需复制的存活对象较少时效率高，且清理后内存无碎片。但同一时刻只能使用一半内存，内存使用率低。 （三）标记整理算法\n流程：在标记清除算法基础上改进。同样先标记可回收对象，清理阶段将存活对象向内存一端移动，然后清理边界以外的垃圾。 优缺点：解决了标记清除算法的内存碎片化问题，但多了对象移动步骤，效率会受一定影响。与复制算法相比，复制算法标记完就复制对象，而标记整理算法需等所有存活对象标记完毕后再整理。 （四）分代收集算法\n堆内存划分：Java 8 时，堆分为新生代和老年代，比例为 1:2。新生代又细分为 Eden 区、S0 区和 S1 区，比例为 8:1:1。Java 7 时还存在永久代。 不同 GC 类型 Minor GC（Young GC）：发生在新生代的垃圾回收，暂停时间短（STW, Stop-The-World）。新创建对象先分配到 Eden 区，Eden 区内存不足时，标记 Eden 区和 from 区（初始时无）的存活对象，用复制算法将其复制到 to 区，Eden 区和 from 区内存释放。经过多次回收，幸存区对象若熬过一定次数（最多 15 次）或因幸存区内存不足、大对象等原因，会晋升到老年代。 Major GC：发生在老年代的垃圾回收。 Full GC：新生代和老年代完整垃圾回收，暂停时间长，应尽量避免。 Mixed GC：新生代和老年代部分区域的垃圾回收，是 G1 收集器特有的。 18. 垃圾回收器 （一）串行垃圾收集器\n特点：使用单线程进行垃圾回收，适用于堆内存较小的个人电脑。 应用区域及算法：Serial 用于新生代，采用复制算法；Serial Old 用于老年代，采用标记 - 整理算法。垃圾回收时，Java 应用所有线程都要暂停（STW）等待回收完成。 （二）并行垃圾收集器\n特点：多线程进行垃圾回收，JDK8 默认使用。 应用区域及算法：Parallel New 作用于新生代，采用复制算法；Parallel Old 作用于老年代，采用标记 - 整理算法。同样在垃圾回收时，所有应用线程需暂停（STW）。 （三）CMS（并发）垃圾收集器\n特点：针对老年代，采用标记 - 清除算法，以获取最短回收停顿时间为目标，垃圾回收时应用仍能正常运行，停顿时间短，用户体验好。 （四）G1 垃圾收集器\n概述：应用于新生代和老年代，JDK9 之后默认使用。将内存划分为多个区域，每个区域可充当 eden、survivor、old、humongous（专为大对象准备）。采用复制算法，兼顾响应时间与吞吐量。分为新生代回收、并发标记、混合收集三个阶段，若并发失败（回收速度赶不上创建新对象速度），会触发 Full GC。 工作阶段 年轻代垃圾回收（Young Collection）：初始时所有区域空闲，创建对象存于伊甸园区。伊甸园需回收时，选一个空闲区域做幸存区，用复制算法复制存活对象，需暂停用户线程。后续伊甸园内存再不足时，将伊甸园及之前幸存区存活对象复制到新幸存区，较老对象晋升到老年代。 年轻代垃圾回收 + 并发标记（Young Collection + Concurrent Mark）：老年代占用内存超阈值（默认 45%）后触发并发标记，此时无需暂停用户线程。并发标记后有重新标记阶段解决漏标问题，需暂停用户线程。完成后确定老年代存活对象，进入混合收集阶段，根据暂停时间目标优先回收存活对象少的区域。 混合收集（Mixed Collection）：完成对象复制，释放内存，进入下一轮回收循环。 19、对象引用类型 （一）强引用\n只有所有 GC Roots 对象都不通过强引用引用该对象时，该对象才能被垃圾回收。例如User user = new User();，只要user引用存在，User对象就不会被回收。\n（二）软引用\n仅有软引用引用对象时，垃圾回收后若内存仍不足，会再次触发对该对象的回收。如User user = new User(); SoftReference softReference = new SoftReference(user);，在内存紧张时，软引用对象可能被回收。\n（三）弱引用\n仅有弱引用引用对象时，垃圾回收时无论内存是否充足，都会回收该对象。例如User user = new User(); WeakReference weakReference = new WeakReference(user);。ThreadLocal 使用弱引用，其内部Entry类的key为弱引用，value为强引用。当key被回收而value未清理时可能导致内存泄漏，因此使用 ThreadLocal 后建议调用清理方法。\n（四）虚引用\n必须配合引用队列使用，被引用对象回收时，虚引用会入队，由 Reference Handler 线程调用相关方法释放直接内存。\nJVM 调优实践 1. JVM 调优参数设置位置 Tomcat：要在 Tomcat 里设置 JVM 调优参数，得去修改TOMCAT_HOME/bin/catalina.sh文件。比如，你想设置堆的初始大小为 512MB，最大大小为 1024MB，就在文件里加上JAVA_OPTS=\u0026quot;-Xms512m -Xmx1024m\u0026quot; 。这样，Tomcat 启动时就会按照你设置的参数来分配堆内存。 Spring Boot 项目：在 Linux 系统下启动 Spring Boot 项目时，能直接在启动命令里加参数。像 nohup java -Xms512m -Xmx1024m -jar xxxx.jar --spring.profiles.active=prod \u0026amp; ，nohup 能让命令在后台一直运行，关了终端也不受影响，\u0026amp; 也是让命令在后台执行。这里同样设置了堆的初始和最大大小，还指定了项目运行环境是prod 。 2. 常用 JVM 调优参数 堆大小设置 -Xms ：用来设置堆的初始化大小。比如说 -Xms512m ，意思就是堆刚开始就分配 512MB 内存。通常为了不让垃圾收集器在初始大小和最大大小之间来回调整堆大小，浪费时间，会把 -Xms 和 -Xmx 设置成一样的值。 -Xmx ：这个是设置堆能达到的最大大小。例如 -Xmx1024m ，就是说堆最多可以使用 1024MB 内存。 新生代内存分配比例设置 -XX:SurvivorRatio ：它决定年轻代里 Eden 区和两个 Survivor 区的大小比例。默认是 8:1:1 。要是设置 -XXSurvivorRatio=3 ，那就表示年轻代里 survivor 区和 eden 区的比例是 2:3 。Java 官方一般会增大 Eden 区来减少 YGC（新生代垃圾回收）的次数，但 Eden 区太大，满了之后释放内存会变慢，程序暂停（STW）的时间就长，所以要根据实际程序情况来调整这个比例。 -XX:newSize ：设置年轻代的初始大小。 -XX:MaxNewSize ：设置年轻代能达到的最大大小。一般把这两个值设成一样，默认年轻代和老年代的空间比例是 1:2 ，通过这两个参数可以调整它们的大小。 线程堆栈设置 -Xss ：每个线程默认有 1M 的堆栈空间，用来放栈帧、调用参数、局部变量这些东西。但一般 256K 就够了。如果减少每个线程的堆栈大小，理论上能产生更多线程，不过这也受操作系统限制。比如 -Xss128k ，就是把每个线程的堆栈大小设成 128KB 。 对象晋升老年代相关设置 -Xmn ：设置年轻代的大小。合理设置 eden 区、survivor 区以及它们的使用率，能让年轻对象留在年轻代，避免 Full GC（全量垃圾回收）。 -XX:PetenureSizeThreshold ：指定对象晋升到老年代的大小门槛，单位是字节。比如 -XX:PetenureSizeThreshold=1000000 ，就是说对象大小超过 1MB，就直接在老年代分配内存，防止在年轻代频繁移动大对象，导致 Full GC。 -XX:MaxTenuringThreshold ：设置对象在 Survivor 区存活的最大年龄。年轻对象在 Eden 区经过第一次 GC 后还活着，就会被移到 Survivor 区，之后每 GC 一次年龄加 1 ，当年龄达到这个阈值，就会被移到老年区。要是想让对象留在年轻代，就设个大一点的阈值。 垃圾回收器选择设置 -XX:+UseParallelGC ：让年轻代使用并行垃圾回收收集器，这个收集器注重吞吐量，能减少垃圾回收的时间。 -XX:+UseParallelOldGC ：设置老年代也用并行垃圾回收收集器。 -XX:+UseConcMarkSweepGC ：让老年代用 CMS（Concurrent Mark Sweep）收集器，这个收集器能降低垃圾回收时程序暂停的时间。 内存分页设置 -XX:+LargePageSizeInBytes ：设置内存页的大小。用大的内存分页可以让 CPU 内存寻址能力变强，提高系统性能。 3. JVM 调优工具 命令行工具 jps（Java Process Status） ：它能输出 JVM 里正在运行的进程状态信息，通过它可以看到当前运行的 Java 进程以及相关信息。 jstack ：用来查看 Java 进程里线程的堆栈信息。使用方法是 jstack [option] \u0026lt;pid\u0026gt; ，\u0026lt;pid\u0026gt; 就是进程 ID 。通过分析线程堆栈，能找到线程死锁、长时间阻塞这些问题。 jmap可以生成堆转存快照。常见命令有： jmap -heap pid ：能显示 Java 堆的信息，像各代的大小、比例，还有使用情况。 jmap -dump:format=b,file=heap.hprof pid ：把 Java 堆内存以 hprof 二进制格式转储，并且保存成你指定的文件 heap.hprof 。 jstat是 JVM 的统计监测工具，能显示垃圾回收信息、类加载信息、新生代统计信息等。常见参数有： jstat -gcutil pid ：总结垃圾回收的统计信息，包括各代的使用率、GC 次数、GC 花的时间等。 jstat -gc pid ：详细输出垃圾回收统计信息，比如对象在各代之间怎么移动、怎么回收的。 jhat ：可以分析 jmap 生成的堆转存快照，但一般不推荐用它，现在常用 Eclipse Memory Analyzer 这类工具替代。 可视化工具 jconsole ：这是基于 JMX（Java Management Extensions）的图形化性能监控工具，可以监控 JVM 的内存、线程、类等。在 Java 安装目录的bin目录下，直接启动 jconsole.exe （Windows 系统）或者 jconsole （Linux 系统），连上目标 Java 进程，就能很直观地看到各种运行指标。 VisualVM ：是故障处理工具，能监控线程、内存情况，查看方法耗费 CPU 的时间，内存里的对象，已经被 GC 的对象，还能反向查看对象分配的堆栈。在 Java 安装目录的bin目录下启动 jvisualvm.exe （Windows 系统），它能加载离线的堆转存快照（.hprof文件）进行分析，方便找出内存泄漏这类问题。 4. Java 内存泄露排查思路 生成内存快照可以用jmap命令指定生成内存快照 dump 文件。但要是程序因为内存溢出已经中断了，\njmap就没法用了。\n这时候可以设置 VM 参数让程序自动生成 dump 文件，设置方法是：\n-XX:+HeapDumpOnOutOfMemoryError ：打开内存溢出时自动生成堆转储快照的功能。 -XX:HeapDumpPath=/home/app/dumps/ ：指定生成文件保存的目录是 /home/app/dumps/ 。 分析 dump 文件：用工具来分析 dump 文件，比如 JDK 自带的 VisualVM （也可以用 Eclipse Memory Analyzer 等）。 VisualVM 能加载离线的 dump 文件，如果是 Linux 系统里程序生成的 dump 文件，得先下载到本地（比如 Windows 环境），再用 VisualVM 打开分析。\n定位问题代码：通过查看堆信息，像对象数量、大小、引用关系这些，大概就能找到内存溢出是哪段代码导致的。\n修复问题：找到对应的代码，看看上下文，分析对象的生命周期和引用关系，然后修改导致内存泄漏的代码逻辑，比如释放掉没用的引用，优化对象创建和销毁的方式等。\n5. 服务器 CPU 持续飙高排查方案与思路 使用top命令查看 CPU 占用情况：在终端输入 top 命令，就能实时看到系统里各个进程的 CPU 占用率等信息，这样就能找出哪个进程占用 CPU 比较高。 确定高 CPU 占用进程 ID：从 top 命令的结果里，记下占用 CPU 高的那个进程的 ID 。 查看进程内线程信息：用 ps 命令查看这个进程里的线程信息，找出 CPU 占用高的线程。比如 ps H -eo pid,tid,%CPU | grep \u0026lt;进程ID\u0026gt; ，pid 是进程 ID ，tid 是线程 ID ，%CPU 是 CPU 使用率，这个命令能筛选出指定进程里每个线程的 CPU 使用情况。 转换线程 ID 为十六进制：通常日志里显示的线程 ID 是十六进制的，所以要把前面得到的十进制线程 ID 转换成十六进制。在 Linux 里用 printf \u0026quot;%x\\n\u0026quot; \u0026lt;线程ID\u0026gt; 这个命令就能转换。 定位问题代码行号：用 jstack 命令打印进程的堆栈信息，根据转换后的十六进制线程 ID 找到有问题的线程，进而找到问题代码在源码里的行号。执行 jstack \u0026lt;进程ID\u0026gt; 命令，分析输出的堆栈信息，找到线程执行的方法调用栈，就能确定是哪段代码导致 CPU 飙高，然后再分析代码逻辑，看看是不是有死循环、复杂计算没优化这些问题，找到原因后进行修复。 面试现场 一、JVM组成 面试官：JVM由那些部分组成，运行流程是什么？\n候选人:\n嗯，好的~~\n在JVM中共有四大部分，分别是ClassLoader（类加载器）、Runtime Data Area（运行时数据区，内存分区）、Execution Engine（执行引擎）、Native Method Library（本地库接口）\n它们的运行流程是：\n第一，类加载器（ClassLoader）把Java代码转换为字节码\n第二，运行时数据区（Runtime Data Area）把字节码加载到内存中，而字节码文件只是JVM的一套指令集规范，并不能直接交给底层系统去执行，而是有执行引擎运行\n第三，执行引擎（Execution Engine）将字节码翻译为底层系统指令，再交由CPU执行去执行，此时需要调用其他语言的本地库接口（Native Method Library）来实现整个程序的功能。\n面试官：好的，你能详细说一下 JVM 运行时数据区吗？\n候选人:\n嗯，好~\n运行时数据区包含了堆、方法区、栈、本地方法栈、程序计数器这几部分，每个功能作用不一样。\n堆解决的是对象实例存储的问题，垃圾回收器管理的主要区域。 方法区可以认为是堆的一部分，用于存储已被虚拟机加载的信息，常量、静态变量、即时编译器编译后的代码。 栈解决的是程序运行的问题，栈里面存的是栈帧，栈帧里面存的是局部变量表、操作数栈、动态链接、方法出口等信息。 本地方法栈与栈功能相同，本地方法栈执行的是本地方法，一个Java调用非Java代码的接口。 程序计数器（PC寄存器）程序计数器中存放的是当前线程所执行的字节码的行数。JVM工作时就是通过改变这个计数器的值来选取下一个需要执行的字节码指令。 面试官：好的，你再详细介绍一下程序计数器的作用？\n候选人:\n嗯，是这样~~\njava虚拟机对于多线程是通过线程轮流切换并且分配线程执行时间。在任何的一个时间点上，一个处理器只会处理执行一个线程，如果当前被执行的这个线程它所分配的执行时间用完了【挂起】。处理器会切换到另外的一个线程上来进行执行。并且这个线程的执行时间用完了，接着处理器就会又来执行被挂起的这个线程。这时候程序计数器就起到了关键作用，程序计数器在来回切换的线程中记录他上一次执行的行号，然后接着继续向下执行。\n面试官：你能给我详细的介绍Java堆吗?\n候选人:\n好的~\nJava中的堆术语线程共享的区域。主要用来保存对象实例，数组等，当堆中没有内存空间可分配给实例，也无法再扩展时，则抛出OutOfMemoryError异常。\n​ 在JAVA8中堆内会存在年轻代、老年代\n​ 1）Young区被划分为三部分，Eden区和两个大小严格相同的Survivor区，其中，Survivor区间中，某一时刻只有其中一个是被使用的，另外一个留做垃圾收集时复制对象用。在Eden区变满的时候， GC就会将存活的对象移到空闲的Survivor区间中，根据JVM的策略，在经过几次垃圾收集后，任然存活于Survivor的对象将被移动到Tenured区间。\n​ 2）Tenured区主要保存生命周期长的对象，一般是一些老的对象，当一些对象在Young复制转移一定的次数以后，对象就会被转移到Tenured区。\n面试官：能不能解释一下方法区？\n候选人:\n方法区（Method Area）是 Java 虚拟机（JVM）内存结构中的一个区域，它主要用于存储已被虚拟机加载的类信息（版本、字段、方法、接口等）、常量、静态变量、即时编译器编译后的代码等数据。方法区是所有线程共享的内存区域，它在虚拟机启动时创建，并且在虚拟机的生命周期内一直存在。\n需要注意的是，在 JDK 1.8 之前，方法区被称为永久代（Permanent Generation），它与堆内存的其他区域（如新生代、老年代）类似，是 JVM 的一部分。从 JDK 1.8 开始，永久代被移除，方法区被重新设计为元空间（Metaspace），元空间使用本地内存（Native Memory），而不是堆内存。\n面试官：你听过直接内存吗？\n候选人:\n直接内存也被称为堆外内存，它是一种位于 Java 虚拟机堆外的内存，通常通过 Java Native Interface（JNI）或者java.nio.ByteBuffer.allocateDirect() 方法进行分配。这种内存直接使用系统的本地内存，因此它不受 JVM 堆大小的限制，也不会受到垃圾回收（GC）机制的管理。由于直接内存是线程共享的，所以所有线程都可以访问其中的数据。此外，直接内存特别适合用于高性能的 I/O 操作，因为它能够减少数据在 JVM 堆和本地内存之间的拷贝，从而提高 I/O 操作的效率。不过，由于直接内存不受 GC 管理，开发者需要手动管理内存的分配和释放，否则可能会导致内存泄漏。\n面试官：Java 8 中将方法区从永久代迁移到元空间的改动带来了哪些好处和潜在影响？\n候选人\n在 Java 8 之前，方法区是通过 HotSpot 虚拟机的永久代来实现的，它存储了类的信息、常量、静态变量以及即时编译器编译后的代码等。由于永久代是 JVM 堆的一部分，它受到垃圾回收（GC）的管理，并且有一个 -XX:MaxPermSize 的上限。这导致在大量动态生成类时，很容易出现 OutOfMemoryError。虽然可以通过增大 -XX:MaxPermSize 来缓解，但很难确定一个合适的大小，因为它受到类的数量和常量数量的影响很大。\n从 Java 8 开始，方法区的实现被迁移到了本地内存中的元空间。元空间使用本地内存而不是 JVM 堆内存，因此它不受 -XX:MaxPermSize 的限制，也不受 JVM 垃圾回收机制的直接管理。这种改动带来了以下好处和潜在影响：\n好处\n减少 OutOfMemoryError：由于元空间使用本地内存，不再受 JVM 堆大小的限制，因此减少了因永久代空间不足而导致的 OutOfMemoryError。 更灵活的内存管理：元空间的大小可以通过 -XX:MaxMetaspaceSize 参数进行设置，但默认情况下，元空间会自动扩展，避免了手动调整内存大小的麻烦。 性能提升：由于元空间不再受 GC 的频繁管理，减少了 GC 的负担，从而提升了整体性能。 潜在影响\n内存使用增加：元空间使用本地内存，可能会导致系统内存的使用量增加，尤其是在类加载频繁的应用中。 监控和管理复杂性：由于元空间使用本地内存，监控和管理元空间的内存使用情况可能比监控永久代更加复杂。 兼容性问题：对于一些依赖于永久代特性的旧代码或工具，可能需要进行调整以适应新的元空间机制。 面试官：什么是虚拟机栈\n候选人:\n虚拟机栈是描述的是方法执行时的内存模型,是线程私有的，生命周期与线程相同，每个方法被执行的同时会创建栈桢。保存执行方法时的局部变量、动态连接信息、方法返回地址信息等等。方法开始执行的时候会进栈，方法执行完会出栈【相当于清空了数据】，所以这块区域不需要进行 GC。\n面试官：能说一下堆栈的区别是什么吗？\n候选人:\n嗯，好的，有这几个区别\n第一，栈内存一般会用来存储局部变量和方法调用，但堆内存是用来存储Java对象和数组的的。堆会GC垃圾回收，而栈不会。\n第二、栈内存是线程私有的，而堆内存是线程共有的。\n第三、两者异常错误不同，但如果栈内存或者堆内存不足都会抛出异常。\n栈空间不足：java.lang.StackOverFlowError。\n堆空间不足：java.lang.OutOfMemoryError。\n二、类加载器 面试官：什么是类加载器，类加载器有哪些?\n候选人:\n嗯，是这样的\nJVM只会运行二进制文件，而类加载器（ClassLoader）的主要作用就是将字节码文件加载到JVM中，从而让Java程序能够启动起来。\n常见的类加载器有4个\n第一个是启动类加载器(BootStrap ClassLoader)：其是由C++编写实现。用于加载JAVA_HOME/jre/lib目录下的类库。\n第二个是扩展类加载器(ExtClassLoader)：该类是ClassLoader的子类，主要加载JAVA_HOME/jre/lib/ext目录中的类库。\n第三个是应用类加载器(AppClassLoader)：该类是ClassLoader的子类，主要用于加载classPath下的类，也就是加载开发者自己编写的Java类。\n第四个是自定义类加载器：开发者自定义类继承ClassLoader，实现自定义类加载规则。\n面试官：说一下类装载的执行过程？\n候选人:\n嗯，这个过程还是挺多的。\n类从加载到虚拟机中开始，直到卸载为止，它的整个生命周期包括了：加载、验证、准备、解析、初始化、使用和卸载这7个阶段。其中，验证、准备和解析这三个部分统称为连接（linking）\n1.加载：查找和导入class文件\n2.验证：保证加载类的准确性\n3.准备：为类变量分配内存并设置类变量初始值\n4.解析：把类中的符号引用转换为直接引用\n5.初始化：对类的静态变量，静态代码块执行初始化操作\n6.使用：JVM 开始从入口方法开始执行用户的程序代码\n7.卸载：当用户程序代码执行完毕后，JVM 便开始销毁创建的 Class 对象，最后负责运行的 JVM 也退出内存\n面试官：什么是双亲委派模型？\n候选人:\n嗯，它是是这样的。\n如果一个类加载器收到了类加载的请求，它首先不会自己尝试加载这个类，而是把这请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传输到顶层的启动类加载器中，只有当父类加载器返回自己无法完成这个加载请求（它的搜索返回中没有找到所需的类）时，子类加载器才会尝试自己去加载\n面试官：JVM为什么采用双亲委派机制\n候选人:\n主要有两个原因。\n第一、通过双亲委派机制可以避免某一个类被重复加载，当父类已经加载后则无需重复加载，保证唯一性。\n第二、为了安全，保证类库API不会被修改\n三、垃圾回收 面试官：简述Java垃圾回收机制？（GC是什么？为什么要GC）\n候选人:\n嗯，是这样~~\n为了让程序员更专注于代码的实现，而不用过多的考虑内存释放的问题，所以，在Java语言中，有了自动的垃圾回收机制，也就是我们熟悉的GC(Garbage Collection)。\n有了垃圾回收机制后，程序员只需要关心内存的申请即可，内存的释放由系统自动识别完成。\n在进行垃圾回收时，不同的对象引用类型，GC会采用不同的回收时机\n面试官：强引用、软引用、弱引用、虚引用的区别？\n候选人:\n嗯嗯~\n强引用最为普通的引用方式，表示一个对象处于有用且必须的状态，如果一个对象具有强引用，则GC并不会回收它。即便堆中内存不足了，宁可出现OOM，也不会对其进行回收\n软引用表示一个对象处于有用且非必须状态，如果一个对象处于软引用，在内存空间足够的情况下，GC机制并不会回收它，而在内存空间不足时，则会在OOM异常出现之间对其进行回收。但值得注意的是，因为GC线程优先级较低，软引用并不会立即被回收。\n弱引用表示一个对象处于可能有用且非必须的状态。在GC线程扫描内存区域时，一旦发现弱引用，就会回收到弱引用相关联的对象。对于弱引用的回收，无关内存区域是否足够，一旦发现则会被回收。同样的，因为GC线程优先级较低，所以弱引用也并不是会被立刻回收。\n虚引用表示一个对象处于无用的状态。在任何时候都有可能被垃圾回收。虚引用的使用必须和引用队列Reference Queue联合使用\n面试官：对象什么时候可以被垃圾器回收\n候选人:\n思考一会~~\n如果一个或多个对象没有任何的引用指向它了，那么这个对象现在就是垃圾，如果定位了垃圾，则有可能会被垃圾回收器回收。\n如果要定位什么是垃圾，有两种方式来确定，第一个是引用计数法，第二个是可达性分析算法\n通常都使用可达性分析算法来确定是不是垃圾\n面试官： JVM 垃圾回收算法有哪些？\n候选人:\n我记得一共有四种，分别是标记清除算法、复制算法、标记整理算法、分代回收\n面试官： 你能详细聊一下分代回收吗？\n候选人:\n关于分代回收是这样的\n在java8时，堆被分为了两份：新生代和老年代，它们默认空间占用比例是1:2\n对于新生代，内部又被分为了三个区域。Eden区，S0区，S1区默认空间占用比例是8:1:1\n具体的工作机制是有些情况：\n1）当创建一个对象的时候，那么这个对象会被分配在新生代的Eden区。当Eden区要满了时候，触发YoungGC。\n2）当进行YoungGC后，此时在Eden区存活的对象被移动到S0区，并且当前对象的年龄会加1，清空Eden区。\n3）当再一次触发YoungGC的时候，会把Eden区中存活下来的对象和S0中的对象，移动到S1区中，这些对象的年龄会加1，清空Eden区和S0区。\n4）当再一次触发YoungGC的时候，会把Eden区中存活下来的对象和S1中的对象，移动到S0区中，这些对象的年龄会加1，清空Eden区和S1区。\n5）对象的年龄达到了某一个限定的值（默认15岁 ），那么这个对象就会进入到老年代中。\n当然也有特殊情况，如果进入Eden区的是一个大对象，在触发YoungGC的时候，会直接存放到老年代\n当老年代满了之后，触发FullGC。FullGC同时回收新生代和老年代，当前只会存在一个FullGC的线程进行执行，其他的线程全部会被挂起。 我们在程序中要尽量避免FullGC的出现。\n面试官：讲一下新生代、老年代、永久代的区别？\n候选人:\n嗯！是这样的，简单说就是\n新生代主要用来存放新生的对象。\n老年代主要存放应用中生命周期长的内存对象。\n永久代指的是永久保存区域。主要存放Class和Meta（元数据）的信息。在Java8中，永久代已经被移除，取而代之的是一个称之为“元数据区”（元空间）的区域。元空间和永久代类似，不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存的限制。\n面试官：说一下 JVM 有哪些垃圾回收器？\n候选人:\n在jvm中，实现了多种垃圾收集器，包括：串行垃圾收集器、并行垃圾收集器（JDK8默认）、CMS（并发）垃圾收集器、G1垃圾收集器（JDK9默认）\n面试官：Minor GC、Major GC、Full GC是什么\n候选人:\n嗯，其实它们指的是不同代之间的垃圾回收\nMinor GC 发生在新生代的垃圾回收，暂停时间短\nMajor GC 老年代区域的垃圾回收，老年代空间不足时，会先尝试触发Minor GC。Minor GC之后空间还不足，则会触发Major GC，Major GC速度比较慢，暂停时间长\nFull GC 新生代 + 老年代完整垃圾回收，暂停时间长，应尽力避免\n四、JVM实践（调优） 面试官：JVM 调优的参数可以在哪里设置参数值？\n候选人:\n我们当时的项目是springboot项目，可以在项目启动的时候，java -jar中加入参数就行了\n面试官：用的 JVM 调优的参数都有哪些？\n候选人:\n嗯，这些参数是比较多的\n我记得当时我们设置过堆的大小，像-Xms和-Xmx\n还有就是可以设置年轻代中Eden区和两个Survivor区的大小比例\n还有就是可以设置使用哪种垃圾回收器等等。具体的指令还真记不太清楚。\n面试官：嗯，好的，你们平时调试 JVM都用了哪些工具呢？\n候选人:\n嗯，我们一般都是使用jdk自带的一些工具，比如\njps 输出JVM中运行的进程状态信息\njstack查看java进程内线程的堆栈信息。\njmap 用于生成堆转存快照\njstat用于JVM统计监测工具\n还有一些可视化工具，像jconsole和VisualVM等\n面试官：假如项目中产生了java内存泄露，你说一下你的排查思路？\n候选人:\n嗯，这个我在之前项目排查过\n第一呢可以通过jmap指定打印他的内存快照 dump文件，不过有的情况打印不了，我们会设置vm参数让程序自动生成dump文件\n第二，可以通过工具去分析 dump文件，jdk自带的VisualVM就可以分析\n第三，通过查看堆信息的情况，可以大概定位内存溢出是哪行代码出了问题\n第四，找到对应的代码，通过阅读上下文的情况，进行修复即可\n面试官：好的，那现在再来说一种情况，就是说服务器CPU持续飙高，你的排查方案与思路？\n候选人:\n嗯，我思考一下~~\n可以这么做~~\n第一可以使用top命令查看占用cpu的情况\n第二通过top命令查看后，可以查看是哪一个进程占用cpu较高，记录这个进程id\n第三可以用 ps 命令查看这个进程里的线程信息，找出 CPU 占用高的线程。\n通常日志里显示的线程 ID 是十六进制的，所以要把前面得到的十进制线程 ID 转换成十六进制。\n第四用 jstack 命令打印进程的堆栈信息，根据转换后的十六进制线程 ID 找到有问题的线程，就可以进一步定位问题代码的行号。\n","date":"2025-04-22T00:00:00Z","image":"https://nova-bryan.github.io/p/java%E9%9D%A2%E8%AF%95/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/java%E9%9D%A2%E8%AF%95/","title":"Java面试"},{"content":"请求响应-概述 在上一次的课程中，我们开发了springbootweb的入门程序。 基于SpringBoot的方式开发一个web应用，浏览器发起请求 /hello 后 ，给浏览器返回字符串 “Hello World ~”。\n其实呢，是我们在浏览器发起请求，请求了我们的后端web服务器(也就是内置的Tomcat)。而我们在开发web程序时呢，定义了一个控制器类Controller，请求会被部署在Tomcat中的Controller接收，然后Controller再给浏览器一个响应，响应一个字符串 “Hello World”。 而在请求响应的过程中是遵循HTTP协议的。\n但是呢，这里要告诉大家的时，其实在Tomcat这类Web服务器中，是不识别我们自己定义的Controller的。但是我们前面讲到过Tomcat是一个Servlet容器，是支持Serlvet规范的，所以呢，在tomcat中是可以识别 Servlet程序的。 那我们所编写的XxxController 是如何处理请求的，又与Servlet之间有什么联系呢？\n其实呢，在SpringBoot进行web程序开发时，它内置了一个核心的Servlet程序 DispatcherServlet，称之为 核心控制器。 DispatcherServlet 负责接收页面发送的请求，然后根据执行的规则，将请求再转发给后面的请求处理器Controller，请求处理器处理完请求之后，最终再由DispatcherServlet给浏览器响应数据。\n那将来浏览器发送请求，会携带请求数据，包括：请求行、请求头；请求到达tomcat之后，tomcat会负责解析这些请求数据，然后呢将解析后的请求数据会传递给Servlet程序的HttpServletRequest对象，那也就意味着 HttpServletRequest 对象就可以获取到请求数据。 而Tomcat，还给Servlet程序传递了一个参数 HttpServletResponse，通过这个对象，我们就可以给浏览器设置响应数据 。\n那上述所描述的这种浏览器/服务器的架构模式呢，我们称之为：BS架构。\n• BS架构：Browser/Server，浏览器/服务器架构模式。客户端只需要浏览器，应用程序的逻辑和数据都存储在服务端。\n那今天呢，我们的课程内容主要就围绕着：请求、响应进行。 今天课程内容，主要包含三个部分：\n请求 响应 分层解耦 请求响应-请求-postman工具 Postman是一款功能强大的网页调试与发送网页HTTP请求的Chrome插件。\nPostman原是Chrome浏览器的插件，可以模拟浏览器向后端服务器发起任何形式(如:get、post)的HTTP请求\n使用Postman还可以在发起请求时，携带一些请求参数、请求头等信息\n作用：常用于进行接口测试\n特征\n简单 实用 美观 大方 安装\n双击资料中提供的Postman-win64-8.3.1-Setup.exe即可自动安装。\n安装完成之后，进入页面中会提示有新版本可以升级（无需升级）\n界面介绍:\n如果我们需要将测试的请求信息保存下来，就需要创建一个postman的账号，然后登录之后才可以。\n登录完成之后，可以创建工作空间：\n创建请求：\n点击\u0026quot;Save\u0026quot;，保存当前请求\n请求响应-请求-简单参数\u0026amp;实体参数 简单参数\n简单参数：在向服务器发起请求时，向服务器传递的是一些普通的请求数据。\n那么在后端程序中，如何接收传递过来的普通参数数据呢？\n我们在这里讲解两种方式：\n原始方式 SpringBoot方式 原始方式\n在原始的Web程序当中，需要通过Servlet中提供的API：HttpServletRequest（请求对象），获取请求的相关信息。比如获取请求参数：\nTomcat接收到http请求时：把请求的相关信息封装到HttpServletRequest对象中\n在Controller中，我们要想获取Request对象，可以直接在方法的形参中声明 HttpServletRequest 对象。然后就可以通过该对象来获取请求信息：\n1 2 //根据指定的参数名获取请求参数的数据值 String request.getParameter(\u0026#34;参数名\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @RestController public class RequestController { //原始方式 @RequestMapping(\u0026#34;/simpleParam\u0026#34;) public String simpleParam(HttpServletRequest request){ // http://localhost:8080/simpleParam?name=Tom\u0026amp;age=10 // 请求参数： name=Tom\u0026amp;age=10 （有2个请求参数） // 第1个请求参数： name=Tom 参数名:name，参数值:Tom // 第2个请求参数： age=10 参数名:age , 参数值:10 String name = request.getParameter(\u0026#34;name\u0026#34;);//name就是请求参数名 String ageStr = request.getParameter(\u0026#34;age\u0026#34;);//age就是请求参数名 int age = Integer.parseInt(ageStr);//需要手动进行类型转换 System.out.println(name+\u0026#34; : \u0026#34;+age); return \u0026#34;OK\u0026#34;; } } 以上这种方式，我们仅做了解。（在以后的开发中不会使用到）\nSpringBoot方式\n在Springboot的环境中，对原始的API进行了封装，接收参数的形式更加简单。 如果是简单参数，参数名与形参变量名相同，定义同名的形参即可接收参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 @RestController public class RequestController { // http://localhost:8080/simpleParam?name=Tom\u0026amp;age=10 // 第1个请求参数： name=Tom 参数名:name，参数值:Tom // 第2个请求参数： age=10 参数名:age , 参数值:10 //springboot方式 @RequestMapping(\u0026#34;/simpleParam\u0026#34;) public String simpleParam(String name , Integer age ){//形参名和请求参数名保持一致 System.out.println(name+\u0026#34; : \u0026#34;+age); return \u0026#34;OK\u0026#34;; } } postman测试( GET 请求)：\npostman测试( POST请求 )：\n结论：不论是GET请求还是POST请求，对于简单参数来讲，只要保证==请求参数名和Controller方法中的形参名保持一致==，就可以获取到请求参数中的数据值。\n参数名不一致\n如果方法形参名称与请求参数名称不一致，controller方法中的形参还能接收到请求参数值吗？\n1 2 3 4 5 6 7 8 9 10 11 12 @RestController public class RequestController { // http://localhost:8080/simpleParam?name=Tom\u0026amp;age=20 // 请求参数名：name //springboot方式 @RequestMapping(\u0026#34;/simpleParam\u0026#34;) public String simpleParam(String username , Integer age ){//请求参数名和形参名不相同 System.out.println(username+\u0026#34; : \u0026#34;+age); return \u0026#34;OK\u0026#34;; } } 答案：运行没有报错。 controller方法中的username值为：null，age值为20\n结论：对于简单参数来讲，请求参数名和controller方法中的形参名不一致时，无法接收到请求数据 那么如果我们开发中，遇到了这种请求参数名和controller方法中的形参名不相同，怎么办？\n解决方案：可以使用Spring提供的@RequestParam注解完成映射\n在方法形参前面加上 @RequestParam 然后通过value属性执行请求参数名，从而完成映射。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 @RestController public class RequestController { // http://localhost:8080/simpleParam?name=Tom\u0026amp;age=20 // 请求参数名：name //springboot方式 @RequestMapping(\u0026#34;/simpleParam\u0026#34;) public String simpleParam(@RequestParam(\u0026#34;name\u0026#34;) String username , Integer age ){ System.out.println(username+\u0026#34; : \u0026#34;+age); return \u0026#34;OK\u0026#34;; } } 注意事项：\n@RequestParam中的required属性默认为true（默认值也是true），代表该请求参数必须传递，如果不传递将报错\n如果该参数是可选的，可以将required属性设置为false\n1 2 3 4 5 @RequestMapping(\u0026#34;/simpleParam\u0026#34;) public String simpleParam(@RequestParam(name = \u0026#34;name\u0026#34;, required = false) String username, Integer age){ System.out.println(username+ \u0026#34;:\u0026#34; + age); return \u0026#34;OK\u0026#34;; } 小结\n\\1. 原始方式获取请求参数\nController方法形参中声明HttpServletRequest对象 调用对象的getParameter(参数名) \\2. SpringBoot中接受简单参数\n请求参数名与方法形参变量名相同 会自动进行类型转换 \\3. @RequestParam注解\n方法形参名称与请求参数名称不匹配，通过该注解完成映射 该注解的required属性默认是true，代表请求参数必须传递 实体参数\n在使用简单参数做为数据传递方式时，前端传递了多少个请求参数，后端controller方法中的形参就要书写多少个。如果请求参数比较多，通过上述的方式一个参数一个参数的接收，会比较繁琐。\n此时，我们可以考虑将请求参数封装到一个实体类对象中。 要想完成数据封装，需要遵守如下规则：请求参数名与实体类的属性名相同\n简单实体对象\n定义POJO实体类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class User { private String name; private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } @Override public String toString() { return \u0026#34;User{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#39;}\u0026#39;; } } Controller方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //实体参数：简单实体对象 @RequestMapping(\u0026#34;/simplePojo\u0026#34;) public String simplePojo(User user){ System.out.println(user); return \u0026#34;OK\u0026#34;; } } Postman测试：\n参数名和实体类属性名一致时 参数名和实体类属性名不一致时 复杂实体对象\n复杂实体对象指的是，在实体类中有一个或多个属性，也是实体对象类型的。如下：\nUser类中有一个Address类型的属性（Address是一个实体类） 复杂实体对象的封装，需要遵守如下规则：\n请求参数名与形参对象属性名相同，按照对象层次结构关系即可接收嵌套实体类属性参数。 定义POJO实体类：\nAddress实体类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public class Address { private String province; private String city; public String getProvince() { return province; } public void setProvince(String province) { this.province = province; } public String getCity() { return city; } public void setCity(String city) { this.city = city; } @Override public String toString() { return \u0026#34;Address{\u0026#34; + \u0026#34;province=\u0026#39;\u0026#34; + province + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, city=\u0026#39;\u0026#34; + city + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } User实体类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class User { private String name; private Integer age; private Address address; //地址对象 public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } public Address getAddress() { return address; } public void setAddress(Address address) { this.address = address; } @Override public String toString() { return \u0026#34;User{\u0026#34; + \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, age=\u0026#34; + age + \u0026#34;, address=\u0026#34; + address + \u0026#39;}\u0026#39;; } } Controller方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //实体参数：复杂实体对象 @RequestMapping(\u0026#34;/complexPojo\u0026#34;) public String complexPojo(User user){ System.out.println(user); return \u0026#34;OK\u0026#34;; } } Postman测试：\n请求响应-请求-数组集合参数 数组\n数组参数：请求参数名与形参数组名称相同且请求参数为多个，定义数组类型形参即可接收参数\nController方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //数组集合参数 @RequestMapping(\u0026#34;/arrayParam\u0026#34;) public String arrayParam(String[] hobby){ System.out.println(Arrays.toString(hobby)); return \u0026#34;OK\u0026#34;; } } Postman测试：\n在前端请求时，有两种传递形式：\n方式一： xxxxxxxxxx?hobby=game\u0026amp;hobby=java\n方式二：xxxxxxxxxxxxx?hobby=game,java\n集合\n集合参数：请求参数名与形参集合对象名相同且请求参数为多个，@RequestParam 绑定参数关系\n默认情况下，请求中参数名相同的多个值，是封装到数组。如果要封装到集合，要使用@RequestParam绑定参数关系\nController方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //数组集合参数 @RequestMapping(\u0026#34;/listParam\u0026#34;) public String listParam(@RequestParam List\u0026lt;String\u0026gt; hobby){ System.out.println(hobby); return \u0026#34;OK\u0026#34;; } } Postman测试：\n方式一： xxxxxxxxxx?hobby=game\u0026amp;hobby=java\n方式二：xxxxxxxxxxxxx?hobby=game,java\n请求响应-请求-日期参数\u0026amp;json参数 日期参数\n上述演示的都是一些普通的参数，在一些特殊的需求中，可能会涉及到日期类型数据的封装。比如，如下需求：\n因为日期的格式多种多样（如：2022-12-12 10:05:45 、2022/12/12 10:05:45），那么对于日期类型的参数在进行封装的时候，需要通过@DateTimeFormat注解，以及其pattern属性来设置日期的格式。\n@DateTimeFormat注解的pattern属性中指定了哪种日期格式，前端的日期参数就必须按照指定的格式传递。 后端controller方法中，需要使用Date类型或LocalDateTime类型，来封装传递的参数。 Controller方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //日期时间参数 @RequestMapping(\u0026#34;/dateParam\u0026#34;) public String dateParam(@DateTimeFormat(pattern = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) LocalDateTime updateTime){ System.out.println(updateTime); return \u0026#34;OK\u0026#34;; } } Postman测试：\nJSON参数\n我们学习JSON格式参数，主要从以下两个方面着手：\nPostman在发送请求时，如何传递json格式的请求参数 在服务端的controller方法中，如何接收json格式的请求参数 Postman发送JSON格式数据：\n服务端Controller方法接收JSON格式数据：\n传递json格式的参数，在Controller中会使用实体类进行封装。 封装规则：JSON数据键名与形参对象属性名相同，定义POJO类型形参即可接收参数。需要使用 @RequestBody标识。 @RequestBody注解：将JSON数据映射到形参的实体类对象中（JSON中的key和实体类中的属性名保持一致） 实体类：Address\n1 2 3 4 5 6 public class Address { private String province; private String city; //省略GET , SET 方法 } 实体类：User\n1 2 3 4 5 6 7 public class User { private String name; private Integer age; private Address address; //省略GET , SET 方法 } Controller方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //JSON参数 @RequestMapping(\u0026#34;/jsonParam\u0026#34;) public String jsonParam(@RequestBody User user){ System.out.println(user); return \u0026#34;OK\u0026#34;; } } Postman测试：\n请求响应-请求-路径参数 在现在的开发中，经常还会直接在请求的URL中传递参数。例如：\n1 2 http://localhost:8080/user/1 http://localhost:880/user/1/0 上述的这种传递请求参数的形式呢，我们称之为：路径参数。\n学习路径参数呢，主要掌握在后端的controller方法中，如何接收路径参数。\n路径参数：\n前端：通过请求URL直接传递参数， 后端：使用{\u0026hellip;}来标识该路径参数，需要使用@PathVariable获取路径参数 Controller方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //路径参数 @RequestMapping(\u0026#34;/path/{id}\u0026#34;) public String pathParam(@PathVariable Integer id){ System.out.println(id); return \u0026#34;OK\u0026#34;; } } Postman测试：\n传递多个路径参数：\nPostman：\nController方法：\n1 2 3 4 5 6 7 8 9 @RestController public class RequestController { //路径参数 @RequestMapping(\u0026#34;/path/{id}/{name}\u0026#34;) public String pathParam2(@PathVariable Integer id, @PathVariable String name){ System.out.println(id+ \u0026#34; : \u0026#34; +name); return \u0026#34;OK\u0026#34;; } } 总结\n请求响应-响应-@ResponseBody\u0026amp;统一响应结果 响应\n前面我们学习过HTTL协议的交互方式：请求响应模式（有请求就有响应）\n那么Controller程序呢，除了接收请求外，还可以进行响应。\n@ResponseBody\n在我们前面所编写的controller方法中，都已经设置了响应数据。\ncontroller方法中的return的结果，怎么就可以响应给浏览器呢？\n答案：使用@ResponseBody注解\n@ResponseBody注解：\n类型：方法注解、类注解 位置：书写在Controller方法上或类上 作用：将方法返回值直接响应给浏览器 如果返回值类型是实体对象/集合，将会转换为JSON格式后在响应给浏览器 但是在我们所书写的Controller中，只在类上添加了@RestController注解、方法添加了@RequestMapping注解，并没有使用@ResponseBody注解，怎么给浏览器响应呢？\n1 2 3 4 5 6 7 8 @RestController public class HelloController { @RequestMapping(\u0026#34;/hello\u0026#34;) public String hello(){ System.out.println(\u0026#34;Hello World ~\u0026#34;); return \u0026#34;Hello World ~\u0026#34;; } } 原因：在类上添加的@RestController注解，是一个组合注解。\n@RestController = @Controller + @ResponseBody @RestController源码：\n1 2 3 4 5 6 7 8 9 10 11 @Target({ElementType.TYPE}) //元注解（修饰注解的注解） @Retention(RetentionPolicy.RUNTIME) //元注解 @Documented //元注解 @Controller @ResponseBody public @interface RestController { @AliasFor( annotation = Controller.class ) String value() default \u0026#34;\u0026#34;; } 结论：在类上添加@RestController就相当于添加了@ResponseBody注解。\n类上有@RestController注解或@ResponseBody注解时：表示当前类下所有的方法返回值做为响应数据 方法的返回值，如果是一个POJO对象或集合时，会先转换为JSON格式，在响应给浏览器 下面我们来测试下响应数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @RestController public class ResponseController { //响应字符串 @RequestMapping(\u0026#34;/hello\u0026#34;) public String hello(){ System.out.println(\u0026#34;Hello World ~\u0026#34;); return \u0026#34;Hello World ~\u0026#34;; } //响应实体对象 @RequestMapping(\u0026#34;/getAddr\u0026#34;) public Address getAddr(){ Address addr = new Address();//创建实体类对象 addr.setProvince(\u0026#34;广东\u0026#34;); addr.setCity(\u0026#34;深圳\u0026#34;); return addr; } //响应集合数据 @RequestMapping(\u0026#34;/listAddr\u0026#34;) public List\u0026lt;Address\u0026gt; listAddr(){ List\u0026lt;Address\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();//集合对象 Address addr = new Address(); addr.setProvince(\u0026#34;广东\u0026#34;); addr.setCity(\u0026#34;深圳\u0026#34;); Address addr2 = new Address(); addr2.setProvince(\u0026#34;陕西\u0026#34;); addr2.setCity(\u0026#34;西安\u0026#34;); list.add(addr); list.add(addr2); return list; } } 在服务端响应了一个对象或者集合，那私前端获取到的数据是什么样子的呢？我们使用postman发送请求来测试下。测试效果如下：\n统一响应结果\n大家有没有发现一个问题，我们在前面所编写的这些Controller方法中，返回值各种各样，没有任何的规范。\n如果我们开发一个大型项目，项目中controller方法将成千上万，使用上述方式将造成整个项目难以维护。那在真实的项目开发中是什么样子的呢？\n在真实的项目开发中，无论是哪种方法，我们都会定义一个统一的返回结果。方案如下：\n前端：只需要按照统一格式的返回结果进行解析(仅一种解析方案)，就可以拿到数据。\n统一的返回结果使用类来描述，在这个结果中包含：\n响应状态码：当前请求是成功，还是失败 状态码信息：给页面的提示信息 返回的数据：给前端响应的数据（字符串、对象、集合） 定义在一个实体类Result来包含以上信息。代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 public class Result { private Integer code;//响应码，1 代表成功; 0 代表失败 private String msg; //响应码 描述字符串 private Object data; //返回的数据 public Result() { } public Result(Integer code, String msg, Object data) { this.code = code; this.msg = msg; this.data = data; } public Integer getCode() { return code; } public void setCode(Integer code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Object getData() { return data; } public void setData(Object data) { this.data = data; } //增删改 成功响应(不需要给前端返回数据) public static Result success(){ return new Result(1,\u0026#34;success\u0026#34;,null); } //查询 成功响应(把查询结果做为返回数据响应给前端) public static Result success(Object data){ return new Result(1,\u0026#34;success\u0026#34;,data); } //失败响应 public static Result error(String msg){ return new Result(0,msg,null); } } 改造Controller：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @RestController public class ResponseController { //响应统一格式的结果 @RequestMapping(\u0026#34;/hello\u0026#34;) public Result hello(){ System.out.println(\u0026#34;Hello World ~\u0026#34;); //return new Result(1,\u0026#34;success\u0026#34;,\u0026#34;Hello World ~\u0026#34;); return Result.success(\u0026#34;Hello World ~\u0026#34;); } //响应统一格式的结果 @RequestMapping(\u0026#34;/getAddr\u0026#34;) public Result getAddr(){ Address addr = new Address(); addr.setProvince(\u0026#34;广东\u0026#34;); addr.setCity(\u0026#34;深圳\u0026#34;); return Result.success(addr); } //响应统一格式的结果 @RequestMapping(\u0026#34;/listAddr\u0026#34;) public Result listAddr(){ List\u0026lt;Address\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); Address addr = new Address(); addr.setProvince(\u0026#34;广东\u0026#34;); addr.setCity(\u0026#34;深圳\u0026#34;); Address addr2 = new Address(); addr2.setProvince(\u0026#34;陕西\u0026#34;); addr2.setCity(\u0026#34;西安\u0026#34;); list.add(addr); list.add(addr2); return Result.success(list); } } 使用Postman测试：\n请求响应-响应-案例 案例 获取员工数据，返回统一响应结果，在页面渲染展示\n加载并解析emp.xml文件中的数据，完成数据处理，并在页面展示。 获取员工数据，返回统一响应结果，在页面渲染展示 步骤 获取员工数据，返回统一响应结果，在页面渲染展示\n在pom.xml文件中引入dom4j的依赖，用于解析XML文件 引入资料中提供的解析XML的工具类XMLParserUtils、对应的实体类Emp、XML文件 emp.xml 引入资料中提供的静态页面文件，放在resources下的static目录下 编写Controller程序，处理请求，响应数据 实现步骤\n\\1. 在pom.xml文件中引入dom4j的依赖，用于解析XML文件\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dom4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dom4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \\2. 引入资料中提供的：解析XML的工具类XMLParserUtils、实体类Emp、XML文件emp.xml\n\\3. 引入资料中提供的静态页面文件，放在resources下的static目录下\n\\4. 创建EmpController类，编写Controller程序，处理请求，响应数据\n代码实现\nContriller代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @RestController public class EmpController { @RequestMapping(\u0026#34;/listEmp\u0026#34;) public Result list(){ //1. 加载并解析emp.xml String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); //System.out.println(file); List\u0026lt;Emp\u0026gt; empList = XmlParserUtils.parse(file, Emp.class); //2. 对数据进行转换处理 - gender, job empList.stream().forEach(emp -\u0026gt; { //处理 gender 1: 男, 2: 女 String gender = emp.getGender(); if(\u0026#34;1\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;男\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;女\u0026#34;); } //处理job - 1: 讲师, 2: 班主任 , 3: 就业指导 String job = emp.getJob(); if(\u0026#34;1\u0026#34;.equals(job)){ emp.setJob(\u0026#34;讲师\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(job)){ emp.setJob(\u0026#34;班主任\u0026#34;); }else if(\u0026#34;3\u0026#34;.equals(job)){ emp.setJob(\u0026#34;就业指导\u0026#34;); } }); //3. 响应数据 return Result.success(empList); } } 统一返回结果实体类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 public class Result { private Integer code ;//1 成功 , 0 失败 private String msg; //提示信息 private Object data; //数据 date public Result() { } public Result(Integer code, String msg, Object data) { this.code = code; this.msg = msg; this.data = data; } public Integer getCode() { return code; } public void setCode(Integer code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Object getData() { return data; } public void setData(Object data) { this.data = data; } public static Result success(Object data){ return new Result(1, \u0026#34;success\u0026#34;, data); } public static Result success(){ return new Result(1, \u0026#34;success\u0026#34;, null); } public static Result error(String msg){ return new Result(0, msg, null); } } 测试\n代码编写完毕之后，我们就可以运行引导类，启动服务进行测试了。\n使用Postman测试：\n打开浏览器，在浏览器地址栏输入： http://localhost:8080/emp.html\n问题分析\n上述案例的功能，我们虽然已经实现，但是呢，我们会发现案例中：解析XML数据，获取数据的代码，处理数据的逻辑的代码，给页面响应的代码全部都堆积在一起了，全部都写在controller方法中了。\n当前程序的这个业务逻辑还是比较简单的，如果业务逻辑再稍微复杂一点，我们会看到Controller方法的代码量就很大了。\n当我们要修改操作数据部分的代码，需要改动Controller 当我们要完善逻辑处理部分的代码，需要改动Controller 当我们需要修改数据响应的代码，还是需要改动Controller 这样呢，就会造成我们整个工程代码的复用性比较差，而且代码难以维护。 那如何解决这个问题呢？其实在现在的开发中，有非常成熟的解决思路，那就是分层开发。\n分层解耦-三层架构 三层架构\n在我们进行程序设计以及程序开发时，尽可能让每一个接口、类、方法的职责更单一些（单一职责原则）。\n单一职责原则：一个类或一个方法，就只做一件事情，只管一块功能。\n这样就可以让类、接口、方法的复杂度更低，可读性更强，扩展性更好，也更利用后期的维护。\n我们之前开发的程序呢，并不满足单一职责原则。下面我们来分析下之前的程序：\n那其实我们上述案例的处理逻辑呢，从组成上看可以分为三个部分：\n数据访问：负责业务数据的维护操作，包括增、删、改、查等操作。 逻辑处理：负责业务逻辑处理的代码。 请求处理、响应数据：负责，接收页面的请求，给页面响应数据。 按照上述的三个组成部分，在我们项目开发中呢，可以将代码分为三层：\nController：控制层。接收前端发送的请求，对请求进行处理，并响应数据。 Service：业务逻辑层。处理具体的业务逻辑。 Dao：数据访问层(Data Access Object)，也称为持久层。负责数据访问操作，包括数据的增、删、改、查。 基于三层架构的程序执行流程：\n前端发起的请求，由Controller层接收（Controller响应数据给前端） Controller层调用Service层来进行逻辑处理（Service层处理完后，把处理结果返回给Controller层） Serivce层调用Dao层（逻辑处理过程中需要用到的一些数据要从Dao层获取） Dao层操作文件中的数据（Dao拿到的数据会返回给Service层） 思考：按照三层架构的思想，如何要对业务逻辑(Service层)进行变更，会影响到Controller层和Dao层吗？\n答案：不会影响。 （程序的扩展性、维护性变得更好了）\n代码拆分\n我们使用三层架构思想，来改造下之前的程序：\n控制层包名：xxxx.controller 业务逻辑层包名：xxxx.service 数据访问层包名：xxxx.dao **控制层：**接收前端发送的请求，对请求进行处理，并响应数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RestController public class EmpController { //业务层对象 private EmpService empService = new EmpServiceA(); @RequestMapping(\u0026#34;/listEmp\u0026#34;) public Result list(){ //1. 调用service层, 获取数据 List\u0026lt;Emp\u0026gt; empList = empService.listEmp(); //3. 响应数据 return Result.success(empList); } } **业务逻辑层：**处理具体的业务逻辑\n业务接口 1 2 3 4 5 //业务逻辑接口（制定业务标准） public interface EmpService { //获取员工列表 public List\u0026lt;Emp\u0026gt; listEmp(); } 业务实现类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 //业务逻辑实现类（按照业务标准实现） public class EmpServiceA implements EmpService { //dao层对象 private EmpDao empDao = new EmpDaoA(); @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 调用dao, 获取数据 List\u0026lt;Emp\u0026gt; empList = empDao.listEmp(); //2. 对数据进行转换处理 - gender, job empList.stream().forEach(emp -\u0026gt; { //处理 gender 1: 男, 2: 女 String gender = emp.getGender(); if(\u0026#34;1\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;男\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;女\u0026#34;); } //处理job - 1: 讲师, 2: 班主任 , 3: 就业指导 String job = emp.getJob(); if(\u0026#34;1\u0026#34;.equals(job)){ emp.setJob(\u0026#34;讲师\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(job)){ emp.setJob(\u0026#34;班主任\u0026#34;); }else if(\u0026#34;3\u0026#34;.equals(job)){ emp.setJob(\u0026#34;就业指导\u0026#34;); } }); return empList; } } **数据访问层：**负责数据的访问操作，包含数据的增、删、改、查\n数据访问接口 1 2 3 4 5 //数据访问层接口（制定标准） public interface EmpDao { //获取员工列表数据 public List\u0026lt;Emp\u0026gt; listEmp(); } 数据访问实现类 1 2 3 4 5 6 7 8 9 10 11 //数据访问实现类 public class EmpDaoA implements EmpDao { @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 加载并解析emp.xml String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); System.out.println(file); List\u0026lt;Emp\u0026gt; empList = XmlParserUtils.parse(file, Emp.class); return empList; } } 三层架构的好处：\n复用性强 便于维护 利用扩展 分层解耦=分层解耦（IOC-DI引入） 分层解耦\n内聚：软件中各个功能模块内部的功能联系。 耦合：衡量软件中各个层/模块之间的依赖、关联的程度。 软件设计原则：高内聚低耦合。\n高内聚指的是：一个模块中各个元素之间的联系的紧密程度，如果各个元素(语句、程序段)之间的联系程度越高，则内聚性越高，即 \u0026ldquo;高内聚\u0026rdquo;。\n低耦合指的是：软件中各个层、模块之间的依赖关联程序越低越好。\n程序中高内聚的体现：\nEmpServiceA类中只编写了和员工相关的逻辑处理代码 程序中耦合代码的体现：\n把业务类变为EmpServiceB时，需要修改controller层中的代码 高内聚、低耦合的目的是使程序模块的可重用性、移植性大大增强。\n解耦思路\n之前我们在编写代码时，需要什么对象，就直接new一个就可以了。 这种做法呢，层与层之间代码就耦合了，当service层的实现变了之后， 我们还需要修改controller层的代码。\n那应该怎么解耦呢？\n首先不能在EmpController中使用new对象。代码如下： 此时，就存在另一个问题了，不能new，就意味着没有业务层对象（程序运行就报错），怎么办呢？ 我们的解决思路是： 提供一个容器，容器中存储一些对象(例：EmpService对象) controller程序从容器中获取EmpService类型的对象 我们想要实现上述解耦操作，就涉及到Spring中的两个核心概念：\n控制反转： Inversion Of Control，简称IOC。对象的创建控制权由程序自身转移到外部（容器），这种思想称为控制反转。\n对象的创建权由程序员主动创建转移到容器(由容器创建、管理对象)。这个容器称为：IOC容器或Spring容器\n依赖注入： Dependency Injection，简称DI。容器为应用程序提供运行时，所依赖的资源，称之为依赖注入。\n程序运行时需要某个资源，此时容器就为其提供这个资源。\n例：EmpController程序运行时需要EmpService对象，Spring容器就为其提供并注入EmpService对象\nIOC容器中创建、管理的对象，称之为：bean对象\n分层解耦-IOC\u0026amp;DI-入门 IOC\u0026amp;DI入门\n任务：完成Controller层、Service层、Dao层的代码解耦\n思路：\n删除Controller层、Service层中new对象的代码\nService层及Dao层的实现类，交给IOC容器管理\n为Controller及Service注入运行时依赖的对象\nController程序中注入依赖的Service层对象 Service程序中注入依赖的Dao层对象 第1步：删除Controller层、Service层中new对象的代码\n第2步：Service层及Dao层的实现类，交给IOC容器管理\n使用Spring提供的注解：@Component ，就可以实现类交给IOC容器管理 第3步：为Controller及Service注入运行时依赖的对象\n使用Spring提供的注解：@Autowired ，就可以实现程序运行时IOC容器自动注入需要的依赖对象 完整的三层代码：\nController层： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @RestController public class EmpController { @Autowired //运行时,从IOC容器中获取该类型对象,赋值给该变量 private EmpService empService ; @RequestMapping(\u0026#34;/listEmp\u0026#34;) public Result list(){ //1. 调用service, 获取数据 List\u0026lt;Emp\u0026gt; empList = empService.listEmp(); //3. 响应数据 return Result.success(empList); } } Service层： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Component //将当前对象交给IOC容器管理,成为IOC容器的bean public class EmpServiceA implements EmpService { @Autowired //运行时,从IOC容器中获取该类型对象,赋值给该变量 private EmpDao empDao ; @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 调用dao, 获取数据 List\u0026lt;Emp\u0026gt; empList = empDao.listEmp(); //2. 对数据进行转换处理 - gender, job empList.stream().forEach(emp -\u0026gt; { //处理 gender 1: 男, 2: 女 String gender = emp.getGender(); if(\u0026#34;1\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;男\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;女\u0026#34;); } //处理job - 1: 讲师, 2: 班主任 , 3: 就业指导 String job = emp.getJob(); if(\u0026#34;1\u0026#34;.equals(job)){ emp.setJob(\u0026#34;讲师\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(job)){ emp.setJob(\u0026#34;班主任\u0026#34;); }else if(\u0026#34;3\u0026#34;.equals(job)){ emp.setJob(\u0026#34;就业指导\u0026#34;); } }); return empList; } } Dao层： 1 2 3 4 5 6 7 8 9 10 11 @Component //将当前对象交给IOC容器管理,成为IOC容器的bean public class EmpDaoA implements EmpDao { @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 加载并解析emp.xml String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); System.out.println(file); List\u0026lt;Emp\u0026gt; empList = XmlParserUtils.parse(file, Emp.class); return empList; } } 运行测试：\n启动SpringBoot引导类，打开浏览器，输入：http://localhost:8080/emp.html 分层解耦-IOC\u0026amp;DI-IOC详解 bean的声明\n前面我们提到IOC控制反转，就是将对象的控制权交给Spring的IOC容器，由IOC容器创建及管理对象。IOC容器创建的对象称为bean对象。\n在之前的入门案例中，要把某个对象交给IOC容器管理，需要在类上添加一个注解：@Component\n而Spring框架为了更好的标识web应用程序开发当中，bean对象到底归属于哪一层，又提供了@Component的衍生注解：\n@Controller （标注在控制层类上） @Service （标注在业务层类上） @Repository （标注在数据访问层类上） 修改入门案例代码：\n*Controller层：* 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @RestController //@RestController = @Controller + @ResponseBody public class EmpController { @Autowired //运行时,从IOC容器中获取该类型对象,赋值给该变量 private EmpService empService ; @RequestMapping(\u0026#34;/listEmp\u0026#34;) public Result list(){ //1. 调用service, 获取数据 List\u0026lt;Emp\u0026gt; empList = empService.listEmp(); //3. 响应数据 return Result.success(empList); } } Service层： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @Service public class EmpServiceA implements EmpService { @Autowired //运行时,从IOC容器中获取该类型对象,赋值给该变量 private EmpDao empDao ; @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 调用dao, 获取数据 List\u0026lt;Emp\u0026gt; empList = empDao.listEmp(); //2. 对数据进行转换处理 - gender, job empList.stream().forEach(emp -\u0026gt; { //处理 gender 1: 男, 2: 女 String gender = emp.getGender(); if(\u0026#34;1\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;男\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(gender)){ emp.setGender(\u0026#34;女\u0026#34;); } //处理job - 1: 讲师, 2: 班主任 , 3: 就业指导 String job = emp.getJob(); if(\u0026#34;1\u0026#34;.equals(job)){ emp.setJob(\u0026#34;讲师\u0026#34;); }else if(\u0026#34;2\u0026#34;.equals(job)){ emp.setJob(\u0026#34;班主任\u0026#34;); }else if(\u0026#34;3\u0026#34;.equals(job)){ emp.setJob(\u0026#34;就业指导\u0026#34;); } }); return empList; } } Dao层： 1 2 3 4 5 6 7 8 9 10 11 @Repository public class EmpDaoA implements EmpDao { @Override public List\u0026lt;Emp\u0026gt; listEmp() { //1. 加载并解析emp.xml String file = this.getClass().getClassLoader().getResource(\u0026#34;emp.xml\u0026#34;).getFile(); System.out.println(file); List\u0026lt;Emp\u0026gt; empList = XmlParserUtils.parse(file, Emp.class); return empList; } } 要把某个对象交给IOC容器管理，需要在对应的类上加上如下注解之一：\n注解 说明 位置 @Controller @Component的衍生注解 标注在控制器类上 @Service @Component的衍生注解 标注在业务类上 @Repository @Component的衍生注解 标注在数据访问类上（由于与mybatis整合，用的少） @Component 声明bean的基础注解 不属于以上三类时，用此注解 查看源码\n在IOC容器中，每一个Bean都有一个属于自己的名字，可以通过注解的value属性指定bean的名字。如果没有指定，默认为类名首字母小写。\n注意事项:\n声明bean的时候，可以通过value属性指定bean的名字，如果没有指定，默认为类名首字母小写。\n使用以上四个注解都可以声明bean，但是在springboot集成web开发中，声明控制器bean只能用@Controller。\n组件扫描\n问题：使用前面学习的四个注解声明的bean，一定会生效吗？\n答案：不一定。（原因：bean想要生效，还需要被组件扫描）\n下面我们通过修改项目工程的目录结构，来测试bean对象是否生效：\n运行程序后，报错：\n为什么没有找到bean对象呢？\n使用四大注解声明的bean，要想生效，还需要被组件扫描注解@ComponentScan扫描 解决方案：手动添加@ComponentScan注解，指定要扫描的包 （==仅做了解，不推荐==）\n推荐做法（如下图）：\n将我们定义的controller，service，dao这些包呢，都放在引导类所在包com.itheima的子包下，这样我们定义的bean就会被自动的扫描到 分****层解耦-IOC\u0026amp;DI-DI详解 依赖注入，是指IOC容器要为应用程序去提供运行时所依赖的资源，而资源指的就是对象。\n在入门程序案例中，我们使用了@Autowired这个注解，完成了依赖注入的操作，而这个Autowired翻译过来叫：自动装配。\n@Autowired注解，默认是按照类型进行自动装配的（去IOC容器中找某个类型的对象，然后完成注入操作）\n入门程序举例：在EmpController运行的时候，就要到IOC容器当中去查找EmpService这个类型的对象，而我们的IOC容器中刚好有一个EmpService这个类型的对象，所以就找到了这个类型的对象完成注入操作。\n那如果在IOC容器中，存在多个相同类型的bean对象，会出现什么情况呢？\n程序运行会报错\n如何解决上述问题呢？Spring提供了以下几种解决方案：\n@Primary @Qualifier @Resource 使用@Primary注解：当存在多个相同类型的Bean注入时，加上@Primary注解，来确定默认的实现。\n使用@Qualifier注解：指定当前要注入的bean对象。 在@Qualifier的value属性中，指定注入的bean的名称。\n@Qualifier注解不能单独使用，必须配合@Autowired使用 使用@Resource注解：是按照bean的名称进行注入。通过name属性指定要注入的bean的名称。\n面试题 ： @Autowird 与 @Resource的区别\n@Autowired 是spring框架提供的注解，而@Resource是JDK提供的注解\n@Autowired 默认是按照类型注入，而@Resource是按照名称注入\n","date":"2025-04-13T16:01:23+08:00","image":"https://nova-bryan.github.io/p/%E8%AF%B7%E6%B1%82%E5%93%8D%E5%BA%94%E5%88%86%E5%B1%82%E8%A7%A3%E8%80%A6/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/%E8%AF%B7%E6%B1%82%E5%93%8D%E5%BA%94%E5%88%86%E5%B1%82%E8%A7%A3%E8%80%A6/","title":"请求响应,分层解耦"},{"content":"事务回顾 在数据库阶段我们已学习过事务了，我们讲到：\n事务是一组操作的集合，它是一个不可分割的工作单位。事务会把所有的操作作为一个整体，一起向数据库提交或者是撤销操作请求。所以这组操作要么同时成功，要么同时失败。\n怎么样来控制这组操作，让这组操作同时成功或同时失败呢？此时就要涉及到事务的具体操作了。\n事务的操作主要有三步：\n开启事务（一组操作开始前，开启事务）：start transaction / begin ; 提交事务（这组操作全部成功后，提交事务）：commit ; 回滚事务（中间任何一个操作出现异常，回滚事务）：rollback ; Spring事务管理-案例 简单的回顾了事务的概念以及事务的基本操作之后，接下来我们看一个事务管理案例：解散部门 （解散部门就是删除部门）\n需求：当部门解散了不仅需要把部门信息删除了，还需要把该部门下的员工数据也删除了。\n步骤：\n根据ID删除部门数据 根据部门ID删除该部门下的员工 代码实现：\n1.DeptServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; //根据部门id，删除部门信息及部门下的所有员工 @Override public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 2.DeptMapper\n1 2 3 4 5 6 7 8 9 @Mapper public interface DeptMapper { /** * 根据id删除部门信息 * @param id 部门id */ @Delete(\u0026#34;delete from dept where id = #{id}\u0026#34;) void deleteById(Integer id); } 3.EmpMapper\n1 2 3 4 5 6 7 8 @Mapper public interface EmpMapper { //根据部门id删除部门下所有员工 @Delete(\u0026#34;delete from emp where dept_id=#{deptId}\u0026#34;) public int deleteByDeptId(Integer deptId); } 重启SpringBoot服务，使用postman测试部门删除：\n代码正常情况下，dept表和Emp表中的数据已删除\n修改DeptServiceImpl类中代码，添加可能出现异常的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; //根据部门id，删除部门信息及部门下的所有员工 @Override public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int i = 1/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 重启SpringBoot服务，使用postman测试部门删除：\n查看数据库表：\n删除了2号部门 2号部门下的员工数据没有删除 以上程序出现的问题：即使程序运行抛出了异常，部门依然删除了，但是部门下的员工却没有删除，造成了数据的不一致。\n原因分析 原因：\n先执行根据id删除部门的操作，这步执行完毕，数据库表 dept 中的数据就已经删除了。 执行 1/0 操作，抛出异常 抛出异常之前，下面所有的代码都不会执行了，根据部门ID删除该部门下的员工，这个操作也不会执行 。 此时就出现问题了，部门删除了，部门下的员工还在，业务操作前后数据不一致。\n而要想保证操作前后，数据的一致性，就需要让解散部门中涉及到的两个业务操作，要么全部成功，要么全部失败 。 那我们如何，让这两个操作要么全部成功，要么全部失败呢 ？\n那就可以通过事务来实现，因为一个事务中的多个业务操作，要么全部成功，要么全部失败。\n此时，我们就需要在delete删除业务功能中添加事务。\n在方法运行之前，开启事务，如果方法成功执行，就提交事务，如果方法执行的过程当中出现异常了，就回滚事务。\n思考：开发中所有的业务操作，一旦我们要进行控制事务，是不是都是这样的套路？\n答案：是的。\n所以在spring框架当中就已经把事务控制的代码都已经封装好了，并不需要我们手动实现。我们使用了spring框架，我们只需要通过一个简单的注解@Transactional就搞定了。\nTransactionla注解 @Transactional作用：就是在当前这个方法执行开始之前来开启事务，方法执行完毕之后提交事务。如果在这个方法执行的过程当中出现了异常，就会进行事务的回滚操作。\n@Transactional注解：我们一般会在业务层当中来控制事务，因为在业务层当中，一个业务功能可能会包含多个数据访问的操作。在业务层来控制事务，我们就可以将多个数据访问操作控制在一个事务范围内。\n@Transactional注解书写位置：\n方法 当前方法交给spring进行事务管理 类 当前类中所有的方法都交由spring进行事务管理 接口 接口下所有的实现类当中所有的方法都交给spring 进行事务管理 接下来，我们就可以在业务方法delete上加上 @Transactional 来控制事务 。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Override @Transactional //当前方法添加了事务管理 public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int i = 1/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 在业务功能上添加@Transactional注解进行事务管理后，我们重启SpringBoot服务，使用postman测试：\n添加Spring事务管理后，由于服务端程序引发了异常，所以事务进行回滚。\n说明：可以在application.yml配置文件中开启事务管理日志，这样就可以在控制看到和事务相关的日志信息了\n1 2 3 4 #spring事务管理日志 logging: level: org.springframework.jdbc.support.JdbcTransactionManager: debug 事务管理-事务进阶-rollbackFor属性 前面我们通过spring事务管理注解@Transactional已经控制了业务层方法的事务。接下来我们要来详细的介绍一下@Transactional事务管理注解的使用细节。我们这里主要介绍@Transactional注解当中的两个常见的属性：\n异常回滚的属性：rollbackFor 事务传播行为：propagation 我们先来学习下rollbackFor属性。\n我们在之前编写的业务方法上添加了@Transactional注解，来实现事务管理。\n1 2 3 4 5 6 7 8 9 10 11 @Transactional public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int i = 1/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } 以上业务功能delete()方法在运行时，会引发除0的算数运算异常(运行时异常)，出现异常之后，由于我们在方法上加了@Transactional注解进行事务管理，所以发生异常会执行rollback回滚操作，从而保证事务操作前后数据是一致的。\n下面我们在做一个测试，我们修改业务功能代码，在模拟异常的位置上直接抛出Exception异常（编译时异常）\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Transactional public void delete(Integer id) throws Exception { //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 if(true){ throw new Exception(\u0026#34;出现异常了~~~\u0026#34;); } //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } 说明：在service中向上抛出一个Exception编译时异常之后，由于是controller调用service，所以在controller中要有异常处理代码，此时我们选择在controller中继续把异常向上抛。\n1 2 3 4 5 6 7 8 9 @DeleteMapping(\u0026#34;/depts/{id}\u0026#34;) public Result delete(@PathVariable Integer id) throws Exception { //日志记录 log.info(\u0026#34;根据id删除部门\u0026#34;); //调用service层功能 deptService.delete(id); //响应 return Result.success(); } 重新启动服务后测试：\n抛出异常之后事务会不会回滚\n现有表中数据：\n使用postman测试，删除5号部门\n发生了Exception异常，但事务依然提交了\ndept表中数据：\n通过以上测试可以得出一个结论：默认情况下，只有出现RuntimeException(运行时异常)才会回滚事务。\n假如我们想让所有的异常都回滚，需要来配置@Transactional注解当中的rollbackFor属性，通过rollbackFor这个属性可以指定出现何种异常类型回滚事务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Override @Transactional(rollbackFor=Exception.class) public void delete(Integer id){ //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常发生 int num = id/0; //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); } } 接下来我们重新启动服务，测试删除部门的操作：\n控制台日志：执行了删除3号部门的操作， 因为异常又进行了事务回滚\n数据表：3号部门没有删除\n结论：\n在Spring的事务管理中，默认只有运行时异常 RuntimeException才会回滚。 如果还需要回滚指定类型的异常，可以通过rollbackFor属性来指定。 事务管理-事务进阶-propagation属性 介绍 我们接着继续学习@Transactional注解当中的第二个属性propagation，这个属性是用来配置事务的传播行为的。\n什么是事务的传播行为呢？\n就是当一个事务方法被另一个事务方法调用时，这个事务方法应该如何进行事务控制。 例如：两个事务方法，一个A方法，一个B方法。在这两个方法上都添加了@Transactional注解，就代表这两个方法都具有事务，而在A方法当中又去调用了B方法。\n所谓事务的传播行为，指的就是在A方法运行的时候，首先会开启一个事务，在A方法当中又调用了B方法， B方法自身也具有事务，那么B方法在运行的时候，到底是加入到A方法的事务当中来，还是B方法在运行的时候新建一个事务？这个就涉及到了事务的传播行为。\n我们要想控制事务的传播行为，在@Transactional注解的后面指定一个属性propagation，通过 propagation 属性来指定传播行为。接下来我们就来介绍一下常见的事务传播行为。\n属性值 含义 REQUIRED 【默认值】需要事务，有则加入，无则创建新事务 REQUIRES_NEW 需要新事务，无论有无，总是创建新事务 SUPPORTS 支持事务，有则加入，无则在无事务状态中运行 NOT_SUPPORTED 不支持事务，在无事务状态下运行,如果当前存在已有事务,则挂起当前事务 MANDATORY 必须有事务，否则抛异常 NEVER 必须没事务，否则抛异常 对于这些事务传播行为，我们只需要关注以下两个就可以了：\nREQUIRED（默认值） REQUIRES_NEW 案例 **需求：**解散部门时需要记录操作日志\n由于解散部门是一个非常重要而且非常危险的操作，所以在业务当中要求每一次执行解散部门的操作都需要留下痕迹，就是要记录操作日志。而且还要求无论是执行成功了还是执行失败了，都需要留下痕迹。\n步骤：\n执行解散部门的业务：先删除部门，再删除部门下的员工（前面已实现） 记录解散部门的日志，到日志表（未实现） 准备工作：\n创建数据库表 dept_log 日志表： 1 2 3 4 5 create table dept_log( id int auto_increment comment \u0026#39;主键ID\u0026#39; primary key, create_time datetime null comment \u0026#39;操作时间\u0026#39;, description varchar(300) null comment \u0026#39;操作描述\u0026#39; )comment \u0026#39;部门操作日志表\u0026#39;; 2. 引入资料中提供的实体类：DeptLog\n1 2 3 4 5 6 7 8 @Data @NoArgsConstructor @AllArgsConstructor public class DeptLog { private Integer id; private LocalDateTime createTime; private String description; } 3. 引入资料中提供的Mapper接口：DeptLogMapper\n1 2 3 4 5 6 7 @Mapper public interface DeptLogMapper { @Insert(\u0026#34;insert into dept_log(create_time,description) values(#{createTime},#{description})\u0026#34;) void insert(DeptLog log); } 4. 引入资料中提供的业务接口：DeptLogService\n1 2 3 public interface DeptLogService { void insert(DeptLog deptLog); } 5. 引入资料中提供的业务实现类：DeptLogServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 @Service public class DeptLogServiceImpl implements DeptLogService { @Autowired private DeptLogMapper deptLogMapper; @Transactional //事务传播行为：有事务就加入、没有事务就新建事务 @Override public void insert(DeptLog deptLog) { deptLogMapper.insert(deptLog); } } 代码实现:\n业务实现类：DeptServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Slf4j @Service //@Transactional //当前业务实现类中的所有的方法，都添加了spring事务管理机制 public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Autowired private EmpMapper empMapper; @Autowired private DeptLogService deptLogService; //根据部门id，删除部门信息及部门下的所有员工 @Override @Log @Transactional(rollbackFor = Exception.class) public void delete(Integer id) throws Exception { try { //根据部门id删除部门信息 deptMapper.deleteById(id); //模拟：异常 if(true){ throw new Exception(\u0026#34;出现异常了~~~\u0026#34;); } //删除部门下的所有员工信息 empMapper.deleteByDeptId(id); }finally { //不论是否有异常，最终都要执行的代码：记录日志 DeptLog deptLog = new DeptLog(); deptLog.setCreateTime(LocalDateTime.now()); deptLog.setDescription(\u0026#34;执行了解散部门的操作，此时解散的是\u0026#34;+id+\u0026#34;号部门\u0026#34;); //调用其他业务类中的方法 deptLogService.insert(deptLog); } } //省略其他代码... } 测试:\n重新启动SpringBoot服务，测试删除3号部门后会发生什么？\n执行了删除3号部门操作 执行了插入部门日志操作 程序发生Exception异常 执行事务回滚（删除、插入操作因为在一个事务范围内，两个操作都会被回滚） 然后在dept_log表中没有记录日志数据\n原因分析:\n接下来我们就需要来分析一下具体是什么原因导致的日志没有成功的记录。\n在执行delete操作时开启了一个事务 当执行insert操作时，insert设置的事务传播行是默认值REQUIRED，表示有事务就加入，没有则新建事务 此时：delete和insert操作使用了同一个事务，同一个事务中的多个操作，要么同时成功，要么同时失败，所以当异常发生时进行事务回滚，就会回滚delete和insert操作 解决方案：\n在DeptLogServiceImpl类中insert方法上，添加@Transactional(propagation = Propagation.REQUIRES_NEW)\nPropagation.REQUIRES_NEW ：不论是否有事务，都创建新事务 ，运行在一个独立的事务中。\n1 2 3 4 5 6 7 8 9 10 11 12 @Service public class DeptLogServiceImpl implements DeptLogService { @Autowired private DeptLogMapper deptLogMapper; @Transactional(propagation = Propagation.REQUIRES_NEW)//事务传播行为：不论是否有事务，都新建事务 @Override public void insert(DeptLog deptLog) { deptLogMapper.insert(deptLog); } } 重启SpringBoot服务，再次测试删除3号部门：\n那此时，DeptServiceImpl中的delete方法运行时，会开启一个事务。 当调用 deptLogService.insert(deptLog) 时，也会创建一个新的事务，那此时，当insert方法运行完毕之后，事务就已经提交了。 即使外部的事务出现异常，内部已经提交的事务，也不会回滚了，因为是两个独立的事务。\n到此事务传播行为已演示完成，事务的传播行为我们只需要掌握两个：REQUIRED、REQUIRES_NEW。\nREQUIRED ：大部分情况下都是用该传播行为即可。 REQUIRES_NEW ：当我们不希望事务之间相互影响时，可以使用该传播行为。比如：下订单前需要记录日志，不论订单保存成功与否，都需要保证日志记录能够记录成功。 AOP基础-快速入门 AOP概述 什么是AOP？\nAOP英文全称：Aspect Oriented Programming（面向切面编程、面向方面编程），其实说白了，面向切面编程就是面向特定方法编程。 那什么又是面向方法编程呢，为什么又需要面向方法编程呢？来我们举个例子做一个说明：\n比如，我们这里有一个项目，项目中开发了很多的业务功能。\n然而有一些业务功能执行效率比较低，执行耗时较长，我们需要针对于这些业务方法进行优化。 那首先第一步就需要定位出执行耗时比较长的业务方法，再针对于业务方法再来进行优化。\n此时我们就需要统计当前这个项目当中每一个业务方法的执行耗时。那么统计每一个业务方法的执行耗时该怎么实现？\n可能多数人首先想到的就是在每一个业务方法运行之前，记录这个方法运行的开始时间。在这个方法运行完毕之后，再来记录这个方法运行的结束时间。拿结束时间减去开始时间，不就是这个方法的执行耗时吗？\n以上分析的实现方式是可以解决需求问题的。但是对于一个项目来讲，里面会包含很多的业务模块，每个业务模块又包含很多增删改查的方法，如果我们要在每一个模块下的业务方法中，添加记录开始时间、结束时间、计算执行耗时的代码，就会让程序员的工作变得非常繁琐。\n而AOP面向方法编程，就可以做到在不改动这些原始方法的基础上，针对特定的方法进行功能的增强。\nAOP的作用：在程序运行期间在不修改源代码的基础上对已有方法进行增强（无侵入性: 解耦）\n我们要想完成统计各个业务方法执行耗时的需求，我们只需要定义一个模板方法，将记录方法执行耗时这一部分公共的逻辑代码，定义在模板方法当中，在这个方法开始运行之前，来记录这个方法运行的开始时间，在方法结束运行的时候，再来记录方法运行的结束时间，中间就来运行原始的业务方法。\n而中间运行的原始业务方法，可能是其中的一个业务方法，比如：我们只想通过 部门管理的 list 方法的执行耗时，那就只有这一个方法是原始业务方法。 而如果，我们是先想统计所有部门管理的业务方法执行耗时，那此时，所有的部门管理的业务方法都是 原始业务方法。 那面向这样的指定的一个或多个方法进行编程，我们就称之为 面向切面编程。\n那此时，当我们再调用部门管理的 list 业务方法时啊，并不会直接执行 list 方法的逻辑，而是会执行我们所定义的 模板方法 ， 然后再模板方法中：\n记录方法运行开始时间 运行原始的业务方法（那此时原始的业务方法，就是 list 方法） 记录方法运行结束时间，计算方法执行耗时 不论，我们运行的是那个业务方法，最后其实运行的就是我们定义的模板方法，而在模板方法中，就完成了原始方法执行耗时的统计操作 。(那这样呢，我们就通过一个模板方法就完成了指定的一个或多个业务方法执行耗时的统计)\n而大家会发现，这个流程，我们是不是似曾相识啊？\n对了，就是和我们之前所学习的动态代理技术是非常类似的。 我们所说的模板方法，其实就是代理对象中所定义的方法，那代理对象中的方法以及根据对应的业务需要， 完成了对应的业务功能，当运行原始业务方法时，就会运行代理对象中的方法，从而实现统计业务方法执行耗时的操作。\n其实，AOP面向切面编程和OOP面向对象编程一样，它们都仅仅是一种编程思想，而动态代理技术是这种思想最主流的实现方式。而Spring的AOP是Spring框架的高级技术，旨在管理bean对象的过程中底层使用动态代理机制，对特定的方法进行编程(功能增强)。\nAOP的优势：\n减少重复代码 提高开发效率 维护方便 AOP快速入门 在了解了什么是AOP后，我们下面通过一个快速入门程序，体验下AOP的开发，并掌握Spring中AOP的开发步骤。\n**需求：**统计各个业务层方法执行耗时。\n实现步骤：\n导入依赖：在pom.xml中导入AOP的依赖 编写AOP程序：针对于特定方法根据业务需要进行编程 为演示方便，可以自建新项目或导入提供的springboot-aop-quickstart项目工程\npom.xml\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-aop\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; AOP程序：TimeAspect\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Component @Aspect //当前类为切面类 @Slf4j public class TimeAspect { @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public Object recordTime(ProceedingJoinPoint pjp) throws Throwable { //记录方法执行开始时间 long begin = System.currentTimeMillis(); //执行原始方法 Object result = pjp.proceed(); //记录方法执行结束时间 long end = System.currentTimeMillis(); //计算方法执行耗时 log.info(pjp.getSignature()+\u0026#34;执行耗时: {}毫秒\u0026#34;,end-begin); return result; } } 重新启动SpringBoot服务测试程序：\n查询3号部门信息 我们可以再测试下：查询所有部门信息（同样执行AOP程序）\n我们通过AOP入门程序完成了业务方法执行耗时的统计，那其实AOP的功能远不止于此，常见的应用场景如下：\n记录系统的操作日志 权限控制 事务管理：我们前面所讲解的Spring事务管理，底层其实也是通过AOP来实现的，只要添加@Transactional注解之后，AOP程序自动会在原始方法运行前先来开启事务，在原始方法运行完毕之后提交或回滚事务 这些都是AOP应用的典型场景。\n通过入门程序，我们也应该感受到了AOP面向切面编程的一些优势：\n代码无侵入：没有修改原始的业务方法，就已经对原始的业务方法进行了功能的增强或者是功能的改变 减少了重复代码 提高开发效率 维护方便 AOP基础-核心概念 通过SpringAOP的快速入门，感受了一下AOP面向切面编程的开发方式。下面我们再来学习AOP当中涉及到的一些核心概念。\n1. 连接点：JoinPoint，可以被AOP控制的方法（暗含方法执行时的相关信息）\n连接点指的是可以被aop控制的方法。例如：入门程序当中所有的业务方法都是可以被aop控制的方法\n在SpringAOP提供的JoinPoint当中，封装了连接点方法在执行时的相关信息。（后面会有具体的讲解）\n2. 通知：Advice，指哪些重复的逻辑，也就是共性功能（最终体现为一个方法）\n在入门程序中是需要统计各个业务方法的执行耗时的，此时我们就需要在这些业务方法运行开始之前，先记录这个方法运行的开始时间，在每一个业务方法运行结束的时候，再来记录这个方法运行的结束时间。\n但是在AOP面向切面编程当中，我们只需要将这部分重复的代码逻辑抽取出来单独定义。抽取出来的这一部分重复的逻辑，也就是共性的功能。\n3. 切入点：PointCut，匹配连接点的条件，通知仅会在切入点方法执行时被应用\n在通知当中，我们所定义的共性功能到底要应用在哪些方法上？此时就涉及到了切入点pointcut概念。切入点指的是匹配连接点的条件。通知仅会在切入点方法运行时才会被应用。\n在aop的开发当中，我们通常会通过一个切入点表达式来描述切入点(后面会有详解)。\n假如：切入点表达式改为DeptServiceImpl.list()，此时就代表仅仅只有list这一个方法是切入点。只有list()方法在运行的时候才会应用通知。\n4. 切面：Aspect，描述通知与切入点的对应关系（通知+切入点）\n当通知和切入点结合在一起，就形成了一个切面。通过切面就能够描述当前aop程序需要针对于哪个原始方法，在什么时候执行什么样的操作。\n切面所在的类，我们一般称为切面类（被@Aspect注解标识的类）\n5. 目标对象：Target，通知所应用的对象\n目标对象指的就是通知所应用的对象，我们就称之为目标对象。\nAOP的核心概念我们介绍完毕之后，接下来我们再来分析一下我们所定义的通知是如何与目标对象结合在一起，对目标对象当中的方法进行功能增强的。\nSpring的AOP底层是基于动态代理技术来实现的，也就是说在程序运行的时候，会自动的基于动态代理技术为目标对象生成一个对应的代理对象。在代理对象当中就会对目标对象当中的原始方法进行功能的增强。\nAOP进阶-通知类型 在入门程序当中，我们已经使用了一种功能最为强大的通知类型：Around环绕通知。\n1 2 3 4 5 6 7 8 9 10 11 12 @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public Object recordTime(ProceedingJoinPoint pjp) throws Throwable { //记录方法执行开始时间 long begin = System.currentTimeMillis(); //执行原始方法 Object result = pjp.proceed(); //记录方法执行结束时间 long end = System.currentTimeMillis(); //计算方法执行耗时 log.info(pjp.getSignature()+\u0026#34;执行耗时: {}毫秒\u0026#34;,end-begin); return result; } 只要我们在通知方法上加上了@Around注解，就代表当前通知是一个环绕通知。\nSpring中AOP的通知类型：\n@Around：环绕通知，此注解标注的通知方法在目标方法前、后都被执行 @Before：前置通知，此注解标注的通知方法在目标方法前被执行 @After ：后置通知，此注解标注的通知方法在目标方法后被执行，无论是否有异常都会执行 @AfterReturning ： 返回后通知，此注解标注的通知方法在目标方法后被执行，有异常不会执行 @AfterThrowing ： 异常后通知，此注解标注的通知方法发生异常后执行 下面我们通过代码演示，来加深对于不同通知类型的理解：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @Slf4j @Component @Aspect public class MyAspect1 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(JoinPoint joinPoint){ log.info(\u0026#34;before ...\u0026#34;); } //环绕通知 @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { log.info(\u0026#34;around before ...\u0026#34;); //调用目标对象的原始方法执行 Object result = proceedingJoinPoint.proceed(); //原始方法如果执行时有异常，环绕通知中的后置代码不会在执行了 log.info(\u0026#34;around after ...\u0026#34;); return result; } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(JoinPoint joinPoint){ log.info(\u0026#34;after ...\u0026#34;); } //返回后通知（程序在正常执行的情况下，会执行的后置通知） @AfterReturning(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void afterReturning(JoinPoint joinPoint){ log.info(\u0026#34;afterReturning ...\u0026#34;); } //异常通知（程序在出现异常的情况下，执行的后置通知） @AfterThrowing(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void afterThrowing(JoinPoint joinPoint){ log.info(\u0026#34;afterThrowing ...\u0026#34;); } } 重新启动SpringBoot服务，进行测试：\n1. 没有异常情况下：\n使用postman测试查询所有部门数据 查看idea中控制台日志输出\n程序没有发生异常的情况下，@AfterThrowing标识的通知方法不会执行。\n2. 出现异常情况下：\n修改DeptServiceImpl业务实现类中的代码： 添加异常\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Override public List\u0026lt;Dept\u0026gt; list() { List\u0026lt;Dept\u0026gt; deptList = deptMapper.list(); //模拟异常 int num = 10/0; return deptList; } //省略其他代码... } 重新启动SpringBoot服务，测试发生异常情况下通知的执行：\n查看idea中控制台日志输出 程序发生异常的情况下：\n@AfterReturning标识的通知方法不会执行，@AfterThrowing标识的通知方法执行了 @Around环绕通知中原始方法调用时有异常，通知中的环绕后的代码逻辑也不会在执行了 （因为原始方法调用已经出异常了） 在使用通知时的注意事项：\n@Around环绕通知需要自己调用 ProceedingJoinPoint.proceed() 来让原始方法执行，其他通知不需要考虑目标方法执行 @Around环绕通知方法的返回值，必须指定为Object，来接收原始方法的返回值，否则原始方法执行完毕，是获取不到返回值的。 五种常见的通知类型，我们已经测试完毕了，此时我们再来看一下刚才所编写的代码，有什么问题吗？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) //环绕通知 @Around(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) //返回后通知（程序在正常执行的情况下，会执行的后置通知） @AfterReturning(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) //异常通知（程序在出现异常的情况下，执行的后置通知） @AfterThrowing(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) 我们发现啊，每一个注解里面都指定了切入点表达式，而且这些切入点表达式都一模一样。此时我们的代码当中就存在了大量的重复性的切入点表达式，假如此时切入点表达式需要变动，就需要将所有的切入点表达式一个一个的来改动，就变得非常繁琐了。\n怎么来解决这个切入点表达式重复的问题？ 答案就是：抽取\nSpring提供了@PointCut注解，该注解的作用是将公共的切入点表达式抽取出来，需要用到时引用该切入点表达式即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 @Slf4j @Component @Aspect public class MyAspect1 { //切入点方法（公共的切入点表达式） @Pointcut(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) private void pt(){ } //前置通知（引用切入点） @Before(\u0026#34;pt()\u0026#34;) public void before(JoinPoint joinPoint){ log.info(\u0026#34;before ...\u0026#34;); } //环绕通知 @Around(\u0026#34;pt()\u0026#34;) public Object around(ProceedingJoinPoint proceedingJoinPoint) throws Throwable { log.info(\u0026#34;around before ...\u0026#34;); //调用目标对象的原始方法执行 Object result = proceedingJoinPoint.proceed(); //原始方法在执行时：发生异常 //后续代码不在执行 log.info(\u0026#34;around after ...\u0026#34;); return result; } //后置通知 @After(\u0026#34;pt()\u0026#34;) public void after(JoinPoint joinPoint){ log.info(\u0026#34;after ...\u0026#34;); } //返回后通知（程序在正常执行的情况下，会执行的后置通知） @AfterReturning(\u0026#34;pt()\u0026#34;) public void afterReturning(JoinPoint joinPoint){ log.info(\u0026#34;afterReturning ...\u0026#34;); } //异常通知（程序在出现异常的情况下，执行的后置通知） @AfterThrowing(\u0026#34;pt()\u0026#34;) public void afterThrowing(JoinPoint joinPoint){ log.info(\u0026#34;afterThrowing ...\u0026#34;); } } 需要注意的是：当切入点方法使用private修饰时，仅能在当前切面类中引用该表达式， 当外部其他切面类中也要引用当前类中的切入点表达式，就需要把private改为public，而在引用的时候，具体的语法为：\n全类名.方法名()，具体形式如下：\n1 2 3 4 5 6 7 8 9 10 @Slf4j @Component @Aspect public class MyAspect2 { //引用MyAspect1切面类中的切入点表达式 @Before(\u0026#34;com.itheima.aspect.MyAspect1.pt()\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect2 -\u0026gt; before ...\u0026#34;); } } AOP进阶-通知顺序 讲解完了Spring中AOP所支持的5种通知类型之后，接下来我们再来研究通知的执行顺序。\n当在项目开发当中，我们定义了多个切面类，而多个切面类中多个切入点都匹配到了同一个目标方法。此时当目标方法在运行的时候，这多个切面类当中的这些通知方法都会运行。\n此时我们就有一个疑问，这多个通知方法到底哪个先运行，哪个后运行？ 下面我们通过程序来验证（这里呢，我们就定义两种类型的通知进行测试，一种是前置通知@Before，一种是后置通知@After）\n定义多个切面类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Slf4j @Component @Aspect public class MyAspect2 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect2 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect2 -\u0026gt; after ...\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Slf4j @Component @Aspect public class MyAspect3 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect3 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect3 -\u0026gt; after ...\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 @Slf4j @Component @Aspect public class MyAspect4 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect4 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect4 -\u0026gt; after ...\u0026#34;); } } 重新启动SpringBoot服务，测试通知的执行顺序：\n备注：\n把DeptServiceImpl实现类中模拟异常的代码删除或注释掉。 注释掉其他切面类(把@Aspect注释即可)，仅保留MyAspect2、MyAspect3、MyAspect4 ，这样就可以清晰看到执行的结果，而不被其他切面类干扰。 使用postman测试查询所有部门数据 查看idea中控制台日志输出\n通过以上程序运行可以看出在不同切面类中，默认按照切面类的类名字母排序：\n目标方法前的通知方法：字母排名靠前的先执行 目标方法后的通知方法：字母排名靠前的后执行 如果我们想控制通知的执行顺序有两种方式：\n修改切面类的类名（这种方式非常繁琐、而且不便管理） 使用Spring提供的@Order注解 使用@Order注解，控制通知的执行顺序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Slf4j @Component @Aspect @Order(2) //切面类的执行顺序（前置通知：数字越小先执行; 后置通知：数字越小越后执行） public class MyAspect2 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect2 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect2 -\u0026gt; after ...\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Slf4j @Component @Aspect @Order(3) //切面类的执行顺序（前置通知：数字越小先执行; 后置通知：数字越小越后执行） public class MyAspect3 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect3 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect3 -\u0026gt; after ...\u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Slf4j @Component @Aspect @Order(1) //切面类的执行顺序（前置通知：数字越小先执行; 后置通知：数字越小越后执行） public class MyAspect4 { //前置通知 @Before(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect4 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;execution(* com.itheima.service.*.*(..))\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect4 -\u0026gt; after ...\u0026#34;); } } 重新启动SpringBoot服务，测试通知执行顺序：\n通知的执行顺序大家主要知道两点即可：\n不同的切面类当中，默认情况下通知的执行顺序是与切面类的类名字母排序是有关系的 可以在切面类上面加上@Order注解，来控制不同的切面类通知的执行顺序 AOP进阶-切入点表达式-execution 从AOP的入门程序到现在，我们一直都在使用切入点表达式来描述切入点。下面我们就来详细的介绍一下切入点表达式的具体写法。\n切入点表达式：\n描述切入点方法的一种表达式 作用：主要用来决定项目中的哪些方法需要加入通知 常见形式： execution(……)：根据方法的签名来匹配 2. @annotation(……) ：根据注解匹配\n首先我们先学习第一种最为常见的execution切入点表达式。\nexecution主要根据方法的返回值、包名、类名、方法名、方法参数等信息来匹配，语法为：\n1 execution(访问修饰符? 返回值 包名.类名.?方法名(方法参数) throws 异常?) 其中带?的表示可以省略的部分\n访问修饰符：可省略（比如: public、protected） 包名.类名： 可省略 throws 异常：可省略（注意是方法上声明抛出的异常，不是实际抛出的异常） 示例：\n1 @Before(\u0026#34;execution(void com.itheima.service.impl.DeptServiceImpl.delete(java.lang.Integer))\u0026#34;) 可以使用通配符描述切入点\n* ：单个独立的任意符号，可以通配任意返回值、包名、类名、方法名、任意类型的一个参数，也可以通配包、类、方法名的一部分 .. ：多个连续的任意符号，可以通配任意层级的包，或任意类型、任意个数的参数 切入点表达式的语法规则：\n方法的访问修饰符可以省略 返回值可以使用*号代替（任意返回值类型） 包名可以使用*号代替，代表任意包（一层包使用一个*） 使用..配置包名，标识此包以及此包下的所有子包 类名可以使用*号代替，标识任意类 方法名可以使用*号代替，表示任意方法 可以使用 * 配置参数，一个任意类型的参数 可以使用.. 配置参数，任意个任意类型的参数 切入点表达式示例\n省略方法的修饰符号 1 execution(void com.itheima.service.impl.DeptServiceImpl.delete(java.lang.Integer)) 使用*代替返回值类型 1 execution(* com.itheima.service.impl.DeptServiceImpl.delete(java.lang.Integer)) 使用*代替包名（一层包使用一个*） 1 execution(* com.itheima.*.*.DeptServiceImpl.delete(java.lang.Integer)) 使用..省略包名 1 execution(* com..DeptServiceImpl.delete(java.lang.Integer)) 使用*代替类名 1 execution(* com..*.delete(java.lang.Integer)) 使用*代替方法名 1 execution(* com..*.*(java.lang.Integer)) 使用 * 代替参数 1 execution(* com.itheima.service.impl.DeptServiceImpl.delete(*)) 使用..省略参数 1 execution(* com..*.*(..)) 注意事项：\n根据业务需要，可以使用 且（\u0026amp;\u0026amp;）、或（||）、非（!） 来组合比较复杂的切入点表达式。 1 execution(* com.itheima.service.DeptService.list(..)) || execution(* com.itheima.service.DeptService.delete(..)) 切入点表达式的书写建议：\n所有业务方法名在命名时尽量规范，方便切入点表达式快速匹配。如：查询类方法都是 find 开头，更新类方法都是update开头 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 //业务类 @Service public class DeptServiceImpl implements DeptService { public List\u0026lt;Dept\u0026gt; findAllDept() { //省略代码... } public Dept findDeptById(Integer id) { //省略代码... } public void updateDeptById(Integer id) { //省略代码... } public void updateDeptByMoreCondition(Dept dept) { //省略代码... } //其他代码... } 1 2 //匹配DeptServiceImpl类中以find开头的方法 execution(* com.itheima.service.impl.DeptServiceImpl.find*(..)) 描述切入点方法通常基于接口描述，而不是直接描述实现类，增强拓展性 1 execution(* com.itheima.service.DeptService.*(..)) 在满足业务需要的前提下，尽量缩小切入点的匹配范围。如：包名匹配尽量不使用 ..，使用 * 匹配单个包 1 execution(* com.itheima.*.*.DeptServiceImpl.find*(..)) AOP进阶-切入点表达式-@annotation 已经学习了execution切入点表达式的语法。那么如果我们要匹配多个无规则的方法，比如：list()和 delete()这两个方法。这个时候我们基于execution这种切入点表达式来描述就不是很方便了。而在之前我们是将两个切入点表达式组合在了一起完成的需求，这个是比较繁琐的。\n我们可以借助于另一种切入点表达式annotation来描述这一类的切入点，从而来简化切入点表达式的书写。\n实现步骤：\n编写自定义注解 在业务类要做为连接点的方法上添加自定义注解 自定义注解：MyLog\n1 2 3 4 @Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface MyLog { } 业务类：DeptServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Override @MyLog //自定义注解（表示：当前方法属于目标方法） public List\u0026lt;Dept\u0026gt; list() { List\u0026lt;Dept\u0026gt; deptList = deptMapper.list(); //模拟异常 //int num = 10/0; return deptList; } @Override @MyLog //自定义注解（表示：当前方法属于目标方法） public void delete(Integer id) { //1. 删除部门 deptMapper.delete(id); } @Override public void save(Dept dept) { dept.setCreateTime(LocalDateTime.now()); dept.setUpdateTime(LocalDateTime.now()); deptMapper.save(dept); } @Override public Dept getById(Integer id) { return deptMapper.getById(id); } @Override public void update(Dept dept) { dept.setUpdateTime(LocalDateTime.now()); deptMapper.update(dept); } } 切面类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 @Slf4j @Component @Aspect public class MyAspect6 { //针对list方法、delete方法进行前置通知和后置通知 //前置通知 @Before(\u0026#34;@annotation(com.itheima.anno.MyLog)\u0026#34;) public void before(){ log.info(\u0026#34;MyAspect6 -\u0026gt; before ...\u0026#34;); } //后置通知 @After(\u0026#34;@annotation(com.itheima.anno.MyLog)\u0026#34;) public void after(){ log.info(\u0026#34;MyAspect6 -\u0026gt; after ...\u0026#34;); } } 重启SpringBoot服务，测试查询所有部门数据，查看控制台日志：\n到此我们两种常见的切入点表达式我已经介绍完了。\nexecution切入点表达式 根据我们所指定的方法的描述信息来匹配切入点方法，这种方式也是最为常用的一种方式 如果我们要匹配的切入点方法的方法名不规则，或者有一些比较特殊的需求，通过execution切入点表达式描述比较繁琐 annotation 切入点表达式 基于注解的方式来匹配切入点方法。这种方式虽然多一步操作，我们需要自定义一个注解，但是相对来比较灵活。我们需要匹配哪个方法，就在方法上加上对应的注解就可以了 AOP进阶-连接点 讲解完了切入点表达式之后，接下来我们再来讲解最后一个部分连接点。我们前面在讲解AOP核心概念的时候，我们提到过什么是连接点，连接点可以简单理解为可以被AOP控制的方法。\n我们目标对象当中所有的方法是不是都是可以被AOP控制的方法。而在SpringAOP当中，连接点又特指方法的执行。\n在Spring中用JoinPoint抽象了连接点，用它可以获得方法执行时的相关信息，如目标类名、方法名、方法参数等。\n对于@Around通知，获取连接点信息只能使用ProceedingJoinPoint类型 对于其他四种通知，获取连接点信息只能使用JoinPoint，它是ProceedingJoinPoint的父类型 示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @Slf4j @Component @Aspect public class MyAspect7 { @Pointcut(\u0026#34;@annotation(com.itheima.anno.MyLog)\u0026#34;) private void pt(){} //前置通知 @Before(\u0026#34;pt()\u0026#34;) public void before(JoinPoint joinPoint){ log.info(joinPoint.getSignature().getName() + \u0026#34; MyAspect7 -\u0026gt; before ...\u0026#34;); } //后置通知 @Before(\u0026#34;pt()\u0026#34;) public void after(JoinPoint joinPoint){ log.info(joinPoint.getSignature().getName() + \u0026#34; MyAspect7 -\u0026gt; after ...\u0026#34;); } //环绕通知 @Around(\u0026#34;pt()\u0026#34;) public Object around(ProceedingJoinPoint pjp) throws Throwable { //获取目标类名 String name = pjp.getTarget().getClass().getName(); log.info(\u0026#34;目标类名：{}\u0026#34;,name); //目标方法名 String methodName = pjp.getSignature().getName(); log.info(\u0026#34;目标方法名：{}\u0026#34;,methodName); //获取方法执行时需要的参数 Object[] args = pjp.getArgs(); log.info(\u0026#34;目标方法参数：{}\u0026#34;, Arrays.toString(args)); //执行原始方法 Object returnValue = pjp.proceed(); return returnValue; } } 重新启动SpringBoot服务，执行查询部门数据的功能：\nAOP案例-记录操作日志 需求 需求：将案例中增、删、改相关接口的操作日志记录到数据库表中\n就是当访问部门管理和员工管理当中的增、删、改相关功能接口时，需要详细的操作日志，并保存在数据表中，便于后期数据追踪。 操作日志信息包含：\n操作人、操作时间、执行方法的全类名、执行方法名、方法运行时参数、返回值、方法执行时长 所记录的日志信息包括当前接口的操作人是谁操作的，什么时间点操作的，以及访问的是哪个类当中的哪个方法，在访问这个方法的时候传入进来的参数是什么，访问这个方法最终拿到的返回值是什么，以及整个接口方法的运行时长是多长时间。\n分析 问题1：项目当中增删改相关的方法是不是有很多？\n很多 问题2：我们需要针对每一个功能接口方法进行修改，在每一个功能接口当中都来记录这些操作日志吗？\n这种做法比较繁琐 以上两个问题的解决方案：可以使用AOP解决(每一个增删改功能接口中要实现的记录操作日志的逻辑代码是相同)。\n可以把这部分记录操作日志的通用的、重复性的逻辑代码抽取出来定义在一个通知方法当中，我们通过AOP面向切面编程的方式，在不改动原始功能的基础上来对原始的功能进行增强。目前我们所增强的功能就是来记录操作日志，所以也可以使用AOP的技术来实现。使用AOP的技术来实现也是最为简单，最为方便的。\n问题3：既然要基于AOP面向切面编程的方式来完成的功能，那么我们要使用 AOP五种通知类型当中的哪种通知类型？\n答案：环绕通知 所记录的操作日志当中包括：操作人、操作时间，访问的是哪个类、哪个方法、方法运行时参数、方法的返回值、方法的运行时长。\n方法返回值，是在原始方法执行后才能获取到的。\n方法的运行时长，需要原始方法运行之前记录开始时间，原始方法运行之后记录结束时间。通过计算获得方法的执行耗时。\n基于以上的分析我们确定要使用Around环绕通知。\n问题4：最后一个问题，切入点表达式我们该怎么写？\n答案：使用annotation来描述表达式 要匹配业务接口当中所有的增删改的方法，而增删改方法在命名上没有共同的前缀或后缀。此时如果使用execution切入点表达式也可以，但是会比较繁琐。 当遇到增删改的方法名没有规律时，就可以使用 annotation切入点表达式\n步骤 简单分析了一下大概的实现思路后，接下来我们就要来完成案例了。案例的实现步骤其实就两步：\n准备工作 引入AOP的起步依赖 导入资料中准备好的数据库表结构，并引入对应的实体类 编码实现 自定义注解@Log 定义切面类，完成记录操作日志的逻辑 实现 准备工作\n1.AOP起步依赖\n1 2 3 4 5 \u0026lt;!--AOP起步依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-aop\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \\2. 导入资料中准备好的数据库表结构，并引入对应的实体类\n数据表\n1 2 3 4 5 6 7 8 9 10 11 -- 操作日志表 create table operate_log( id int unsigned primary key auto_increment comment \u0026#39;ID\u0026#39;, operate_user int unsigned comment \u0026#39;操作人\u0026#39;, operate_time datetime comment \u0026#39;操作时间\u0026#39;, class_name varchar(100) comment \u0026#39;操作的类名\u0026#39;, method_name varchar(100) comment \u0026#39;操作的方法名\u0026#39;, method_params varchar(1000) comment \u0026#39;方法参数\u0026#39;, return_value varchar(2000) comment \u0026#39;返回值\u0026#39;, cost_time bigint comment \u0026#39;方法执行耗时, 单位:ms\u0026#39; ) comment \u0026#39;操作日志表\u0026#39;; 实体类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //操作日志实体类 @Data @NoArgsConstructor @AllArgsConstructor public class OperateLog { private Integer id; //主键ID private Integer operateUser; //操作人ID private LocalDateTime operateTime; //操作时间 private String className; //操作类名 private String methodName; //操作方法名 private String methodParams; //操作方法参数 private String returnValue; //操作方法返回值 private Long costTime; //操作耗时 } Mapper接口\n1 2 3 4 5 6 7 8 9 @Mapper public interface OperateLogMapper { //插入日志数据 @Insert(\u0026#34;insert into operate_log (operate_user, operate_time, class_name, method_name, method_params, return_value, cost_time) \u0026#34; + \u0026#34;values (#{operateUser}, #{operateTime}, #{className}, #{methodName}, #{methodParams}, #{returnValue}, #{costTime});\u0026#34;) public void insert(OperateLog log); } 编码实现\n自定义注解@Log 1 2 3 4 5 6 7 8 /** * 自定义Log注解 */ @Target({ElementType.METHOD}) @Documented @Retention(RetentionPolicy.RUNTIME) public @interface Log { } 修改业务实现类，在增删改业务方法上添加@Log注解 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @Slf4j @Service public class EmpServiceImpl implements EmpService { @Autowired private EmpMapper empMapper; @Override @Log public void update(Emp emp) { emp.setUpdateTime(LocalDateTime.now()); //更新修改时间为当前时间 empMapper.update(emp); } @Override @Log public void save(Emp emp) { //补全数据 emp.setCreateTime(LocalDateTime.now()); emp.setUpdateTime(LocalDateTime.now()); //调用添加方法 empMapper.insert(emp); } @Override @Log public void delete(List\u0026lt;Integer\u0026gt; ids) { empMapper.delete(ids); } //省略其他代码... } 以同样的方式，修改EmpServiceImpl业务类\n定义切面类，完成记录操作日志的逻辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @Slf4j @Component @Aspect //切面类 public class LogAspect { @Autowired private HttpServletRequest request; @Autowired private OperateLogMapper operateLogMapper; @Around(\u0026#34;@annotation(com.itheima.anno.Log)\u0026#34;) public Object recordLog(ProceedingJoinPoint joinPoint) throws Throwable { //操作人ID - 当前登录员工ID //获取请求头中的jwt令牌, 解析令牌 String jwt = request.getHeader(\u0026#34;token\u0026#34;); Claims claims = JwtUtils.parseJWT(jwt); Integer operateUser = (Integer) claims.get(\u0026#34;id\u0026#34;); //操作时间 LocalDateTime operateTime = LocalDateTime.now(); //操作类名 String className = joinPoint.getTarget().getClass().getName(); //操作方法名 String methodName = joinPoint.getSignature().getName(); //操作方法参数 Object[] args = joinPoint.getArgs(); String methodParams = Arrays.toString(args); long begin = System.currentTimeMillis(); //调用原始目标方法运行 Object result = joinPoint.proceed(); long end = System.currentTimeMillis(); //方法返回值 String returnValue = JSONObject.toJSONString(result); //操作耗时 Long costTime = end - begin; //记录操作日志 OperateLog operateLog = new OperateLog(null,operateUser,operateTime,className,methodName,methodParams,returnValue,costTime); operateLogMapper.insert(operateLog); log.info(\u0026#34;AOP记录操作日志: {}\u0026#34; , operateLog); return result; } } 代码实现细节： 获取request对象，从请求头中获取到jwt令牌，解析令牌获取出当前用户的id。\n重启SpringBoot服务，测试操作日志记录功能：\n添加一个新的部门 数据表 ","date":"2025-04-13T16:01:23+08:00","image":"https://nova-bryan.github.io/p/%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E5%92%8Caop/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%E5%92%8Caop/","title":"事务管理和AOP"},{"content":"题目 答案： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 import java.util.*; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); // 读取输入 int n = sc.nextInt(); // 矿洞数 int m = sc.nextInt(); // 最大步数 int[] L = new int[2000005]; // 记录左侧矿洞 int[] R = new int[2000005]; // 记录右侧矿洞 int[] Ls = new int[2000005]; // 左侧前缀和 int[] Rs = new int[2000005]; // 右侧前缀和 int zero = 0; // 记录起点 0 处的矿洞数量 // 读取矿洞信息 for (int i = 0; i \u0026lt; n; i++) { int t = sc.nextInt(); if (t \u0026lt; 0) { L[-t]++; // 左侧 } else if (t \u0026gt; 0) { R[t]++; // 右侧 } else { zero++; // 记录起点0 } } // 计算前缀和 for (int i = 1; i \u0026lt;= 2000000; i++) { Ls[i] = L[i] + Ls[i - 1]; Rs[i] = R[i] + Rs[i - 1]; } int ans = 0; // 记录最大可收集矿石数 // 枚举向左走的步数 for (int i = 0; i \u0026lt;= m; i++) { int j = (m - i) / 2; // 剩余步数一半用于回头 if (i \u0026lt; j) j = m - 2 * i; // 计算当前走法的矿石数 int sums = Rs[i] + Ls[Math.max(0, j)]; ans = Math.max(ans, sums); } ans += zero; // 加上起点0的矿石数量 // 输出答案 System.out.println(ans); } } ","date":"2025-04-10T00:00:00Z","image":"https://nova-bryan.github.io/p/acwing-5995.%E6%8C%96%E7%9F%BF/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/acwing-5995.%E6%8C%96%E7%9F%BF/","title":"AcWing 5995.挖矿"},{"content":"题目： 答案： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { static List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; ans; public List\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; pathSum(TreeNode root, int targetSum) { if (root == null) return ans; ans=new ArrayList\u0026lt;\u0026gt;(); dfs(new ArrayList\u0026lt;\u0026gt;(),root,0,targetSum); return ans; } public static void dfs(List\u0026lt;Integer\u0026gt; list,TreeNode node,int sum,int targetSum){ if (node==null){ return; } list.add(node.val); sum += node.val; if (node.left == null \u0026amp;\u0026amp; node.right == null \u0026amp;\u0026amp; sum == targetSum) { ans.add(new ArrayList\u0026lt;\u0026gt;(list)); // 这里一定要 new 一个副本 } dfs(list,node.left,sum,targetSum); dfs(list,node.right,sum,targetSum); list.remove(list.size()-1); } } ","date":"2025-04-09T00:00:00Z","image":"https://nova-bryan.github.io/p/leetcode113.%E8%B7%AF%E5%BE%84%E6%80%BB%E5%90%88ii/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/leetcode113.%E8%B7%AF%E5%BE%84%E6%80%BB%E5%90%88ii/","title":"LeetCode113.路径总合II"},{"content":"题目： 答案： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import java.util.ArrayList; import java.util.List; class Solution { static boolean flag = false; static boolean[][] isFlag; public boolean exist(char[][] board, String word) { flag = false; // 每次调用要重置 isFlag = new boolean[board.length][board[0].length]; List\u0026lt;Character\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; word.length(); i++) { list.add(word.charAt(i)); } // 从每一个点开始尝试匹配 for (int i = 0; i \u0026lt; board.length; i++) { for (int j = 0; j \u0026lt; board[0].length; j++) { dfs(i, j, board, list, 0); // 第0个字符开始匹配 if (flag) return true; } } return false; } public static void dfs(int i, int j, char[][] board, List\u0026lt;Character\u0026gt; list, int index) { // 如果已经全部匹配完了 if (index == list.size()) { flag = true; return; } // 越界 + 已访问 + 当前字符不匹配 if (i \u0026lt; 0 || i \u0026gt;= board.length || j \u0026lt; 0 || j \u0026gt;= board[0].length || isFlag[i][j] || board[i][j] != list.get(index)) { return; } isFlag[i][j] = true; // 递归向四个方向搜索 dfs(i + 1, j, board, list, index + 1); dfs(i - 1, j, board, list, index + 1); dfs(i, j + 1, board, list, index + 1); dfs(i, j - 1, board, list, index + 1); isFlag[i][j] = false; // 回溯 } } ","date":"2025-04-09T00:00:00Z","image":"https://nova-bryan.github.io/p/leetcode79.%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/leetcode79.%E5%8D%95%E8%AF%8D%E6%90%9C%E7%B4%A2/","title":"LeetCode79.单词搜索"},{"content":"题目 答案 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { TreeNode prev = null; public boolean isValidBST(TreeNode root) { return inorder(root); } public boolean inorder(TreeNode node) { if (node == null) return true; // 左子树 if (!inorder(node.left)) return false; // 当前节点必须大于中序遍历的前一个节点 if (prev != null \u0026amp;\u0026amp; node.val \u0026lt;= prev.val) return false; prev = node; // 更新前一个节点 // 右子树 return inorder(node.right); } } ","date":"2025-04-09T00:00:00Z","image":"https://nova-bryan.github.io/p/leetcode98.%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/leetcode98.%E9%AA%8C%E8%AF%81%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/","title":"LeetCode98.验证二叉搜索树"},{"content":"题目: 博主解题思路： 整体思路：\n本题是一个关于两头奶牛分蛋糕的博弈问题，关键在于分析在双方都采取最优策略的情况下，如何计算出贝茜和埃尔茜最终吃到的蛋糕量。由于贝茜先行动（堆叠相邻蛋糕），埃尔茜后行动（选择最左或最右蛋糕藏起来），且最终贝茜吃剩下的一个蛋糕，埃尔茜吃藏起来的蛋糕，所以核心是找出埃尔茜能藏起来的最大蛋糕量，再用蛋糕总量减去埃尔茜的量，即可得到贝茜的量。\n具体步骤\n处理多组测试用例 首先读取测试用例的数量 T，然后对每个测试用例进行处理。对于每个测试用例，先读取蛋糕的数量 N。\n计算前缀和 使用前缀和数组 s 来方便计算区间内蛋糕的总量。前缀和数组 s[i] 表示前 i 个蛋糕的大小之和。通过遍历输入的蛋糕大小数组，计算出前缀和数组： 1 2 3 4 long[] s = new long[n + 1]; String[] split = br.readLine().split(\u0026#34; \u0026#34;); for (int i = 1; i \u0026lt;= n; i++) s[i] = Long.parseLong(split[i - 1]) + s[i - 1]; 确定埃尔茜藏蛋糕的次数 每一轮埃尔茜藏一个蛋糕，最终只剩下一个蛋糕给贝茜吃，所以埃尔茜藏蛋糕的次数为 (n - 1) / 2，记为 eTotal。\n找出埃尔茜能藏起来的最大蛋糕量 埃尔茜藏蛋糕的过程可以看作是从蛋糕序列的左右两端选取一些连续的蛋糕。我们通过枚举不同的组合，找出能使埃尔茜藏起来的蛋糕总量最大的情况。具体做法是，枚举从左边选取 i 个蛋糕，从右边选取 eTotal - i 个蛋糕的所有可能组合，计算这些组合的蛋糕总量，并取最大值作为埃尔茜能藏起来的最大蛋糕量 e：\n1 2 3 4 int eTotal = n - 1 \u0026gt;\u0026gt; 1; long e = 0; for (int i = 0; i \u0026lt;= eTotal; i++) e = Math.max(e, s[i] + (s[n] - s[n - eTotal + i])); 计算贝茜吃到的蛋糕量 用所有蛋糕的总量 s[n] 减去埃尔茜藏起来的最大蛋糕量 e，即可得到贝茜吃到的蛋糕量 b：\n1 long b = s[n] - e; 输出结果 对于每个测试用例，输出贝茜和埃尔茜分别吃到的蛋糕量 b 和 e。\n总结\n通过上述步骤，我们利用前缀和数组来高效计算区间内蛋糕的总量，通过枚举不同的左右两端蛋糕组合，找出埃尔茜能藏起来的最大蛋糕量，进而计算出贝茜吃到的蛋糕量，从而解决了该博弈问题。\n答案： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import java.io.*; public class Main { public static void main(String[] args) throws IOException { BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); int t = Integer.parseInt(br.readLine()); while (t-- != 0) { int n = Integer.parseInt(br.readLine()); long[] s = new long[n + 1]; String[] split = br.readLine().split(\u0026#34; \u0026#34;); for (int i = 1; i \u0026lt;= n; i++) s[i] = Long.parseLong(split[i - 1]) + s[i - 1]; int eTotal = n - 1 \u0026gt;\u0026gt; 1; long e = 0; for (int i = 0; i \u0026lt;= eTotal; i++) e = Math.max(e, s[i] + (s[n] - s[n - eTotal + i])); long b = s[n] - e; bw.write(b + \u0026#34; \u0026#34; + e); bw.newLine(); } br.close(); bw.close(); } }\t","date":"2025-03-26T00:00:00Z","image":"https://nova-bryan.github.io/p/acwing-6118.-%E8%9B%8B%E7%B3%95%E6%B8%B8%E6%88%8F/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/acwing-6118.-%E8%9B%8B%E7%B3%95%E6%B8%B8%E6%88%8F/","title":"AcWing 6118. 蛋糕游戏"},{"content":"题目 博主解题思路： ​\t首先熟读题目后，发现要求的是，一段定长字符串中的abb形式的定长字串，如果定长字串的数量大于等于F的话，那么就可以列为一组答案，值得注意的是，字符串中可能存在至多一个字符与原始字符串不用，也就是说，可以在字串中更改一个字符，如果字符更改后的关联的定长字串的数量能大于等于F，那么也可以列入答案当中，那么思路就很明显了，首先，我们定义一个cnt数组，用来统计原字符串中的定长字串数量，再依次改变原字符串中的每一个字符，共有25种更改情况，每次改变字符，都会改变三个定长字串，如：\n值得注意的是，每次我们更改字符之前，我们将原来的关联的三个字串数量-1，这样子我们修改之后，得到新字串，加入到字串数量中，才能有效的判断是否出现了有效的答案字串，不然就会多加1，在改变字符之后，对应的三个新字串数量加一，判断是否有答案字串，如果有答案字串，那么就标记一下（开辟一个新的数组来记录答案字串），更新完之后，再减去字串数量-1，最后恢复现场，将原字符放回原位，并更新原字串数量，这样子就能达到至多只有一个字符改变的效果，最后统计答案字串数量，并输出答案子串\n说实话这个题目难度对目前的我来说难度还是比较大，但是搞懂本题之后，对我的思维拓展还是比较大，模拟题目或者需要重复执行类似操作的题目，可以开辟一个更新函数来重复操作，直观并且更为简洁\n答案： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 import java.util.Scanner; public class Main{ private static final int N = 20010; // N 设为静态常量 private static final int M = 26; private static Integer n; private static Integer m; private static char[] s; private static int[][] cnt=new int[M][M]; private static boolean[][] ans=new boolean[M][M]; public static void main(String agrs[]){ Scanner sc=new Scanner(System.in); n =sc.nextInt(); m=sc.nextInt(); sc.nextLine(); String S=sc.nextLine(); s=S.toCharArray(); //先将字符转换为数字 for (int i = 0; i \u0026lt; s.length; i++) s[i]-=\u0026#39;a\u0026#39;; //先统计原字符串中的哞叫可能 update(0,n-1,1); for (int i = 0; i \u0026lt; n; i++) { char t=s[i]; update(i-2,i+2,-1); for (int j = 0; j \u0026lt; 26; j++) { if (j!=t){ s[i]= (char) j; update(i-2,i+2,1); update(i-2,i+2,-1); } } s[i]=t; update(i-2,i+2,1); } int res=0; for (int i = 0; i \u0026lt; 26; i++) { for (int j = 0; j \u0026lt; 26 ; j++) { if (ans[i][j]) res++; } } System.out.println(res); for (int i = 0; i \u0026lt; 26; i++) { for (int j = 0; j \u0026lt; 26 ; j++) { if (ans[i][j]) { System.out.println(String.format(\u0026#34;%c%c%c\u0026#34;, (char) (i + \u0026#39;a\u0026#39;), (char) (j + \u0026#39;a\u0026#39;), (char) (j + \u0026#39;a\u0026#39;))); } } } } public static void update(int l,int r,int v){ l=Math.max(l,0); r=Math.min(r,n-1); for (int i = l; i+2 \u0026lt;= r ; i++) { char a=s[i],b=s[i+1],c=s[i+2]; if(a!=b\u0026amp;\u0026amp;b==c){ cnt[a][b]+=v; if (cnt[a][b]\u0026gt;=m) ans[a][b]=true; } } } } ","date":"2025-03-24T00:00:00Z","image":"https://nova-bryan.github.io/p/acwing-6123.-%E5%93%9E%E5%8F%AB%E6%97%B6%E9%97%B4/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/acwing-6123.-%E5%93%9E%E5%8F%AB%E6%97%B6%E9%97%B4/","title":"AcWing 6123. 哞叫时间"},{"content":"Markdown 基本语法 Markdown是一种轻量级标记语言，排版语法简洁，让人们更多地关注内容本身而非排版。它使用易读易写的纯文本格式编写文档，可与HTML混编，可导出 HTML、PDF 以及本身的 .md 格式的文件。因简洁、高效、易读、易写，Markdown被大量使用，如Github、Wikipedia、简书等。\n在线体验一下 Markdown在线编辑器 (opens new window)。\n千万不要被「标记」、「语言」吓到，Markdown的语法十分简单，常用的标记符号不超过十个，用于日常写作记录绰绰有余，不到半小时就能完全掌握。\n就是这十个不到的标记符号，却能让人优雅地沉浸式记录，专注内容而不是纠结排版，达到「心中无尘，码字入神」的境界。\nMarkdown 标题语法 要创建标题，请在单词或短语前面添加井号 (#) 。# 的数量代表了标题的级别。例如，添加三个 # 表示创建一个三级标题 (\u0026lt;h3\u0026gt;) (例如：### My Header)。\nMarkdown语法 HTML 预览效果 # Heading level 1 \u0026lt;h1\u0026gt;Heading level 1\u0026lt;/h1\u0026gt; Heading level 1 ## Heading level 2 \u0026lt;h2\u0026gt;Heading level 2\u0026lt;/h2\u0026gt; Heading level 2 ### Heading level 3 \u0026lt;h3\u0026gt;Heading level 3\u0026lt;/h3\u0026gt; Heading level 3 #### Heading level 4 \u0026lt;h4\u0026gt;Heading level 4\u0026lt;/h4\u0026gt; Heading level 4 ##### Heading level 5 \u0026lt;h5\u0026gt;Heading level 5\u0026lt;/h5\u0026gt; Heading level 5 ###### Heading level 6 \u0026lt;h6\u0026gt;Heading level 6\u0026lt;/h6\u0026gt; Heading level 6 可选语法 还可以在文本下方添加任意数量的 == 号来标识一级标题，或者 \u0026ndash; 号来标识二级标题。\nMarkdown语法 HTML 预览效果 Heading level 1=============== \u0026lt;h1\u0026gt;Heading level 1\u0026lt;/h1\u0026gt; Heading level 1 Heading level 2--------------- \u0026lt;h2\u0026gt;Heading level 2\u0026lt;/h2\u0026gt; Heading level 2 最佳实践 不同的 Markdown 应用程序处理 # 和标题之间的空格方式并不一致。为了兼容考虑，请用一个空格在 # 和标题之间进行分隔。\n✅ Do this ❌ Don\u0026rsquo;t do this # Here's a Heading #Here's a Heading Markdown 段落 要创建段落，请使用空白行将一行或多行文本进行分隔。\nMarkdown语法 HTML 预览效果 I really like using Markdown.I think I'll use it to format all of my documents from now on. \u0026lt;p\u0026gt;I really like using Markdown.\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;I think I'll use it to format all of my documents from now on.\u0026lt;/p\u0026gt; I really like using Markdown.I think I\u0026rsquo;ll use it to format all of my documents from now on. 段落（Paragraph）用法的最佳实 不要用空格（spaces）或制表符（ tabs）缩进段落。\n✅ Do this ❌ Don\u0026rsquo;t do this Don't put tabs or spaces in front of your paragraphs.Keep lines left-aligned like this. This can result in unexpected formatting problems. Don't add tabs or spaces in front of paragraphs. Markdown 换行语法 在一行的末尾添加两个或多个空格，然后按回车键,即可创建一个换行(\u0026lt;br\u0026gt;)。\nMarkdown语法 HTML 预览效果 This is the first line. And this is the second line. \u0026lt;p\u0026gt;This is the first line.\u0026lt;br\u0026gt;And this is the second line.\u0026lt;/p\u0026gt; This is the first line. And this is the second line. 换行（Line Break）用法的最佳实践 几乎每个 Markdown 应用程序都支持两个或多个空格进行换行，称为 结尾空格（trailing whitespace) 的方式，但这是有争议的，因为很难在编辑器中直接看到空格，并且很多人在每个句子后面都会有意或无意地添加两个空格。由于这个原因，你可能要使用除结尾空格以外的其它方式来换行。幸运的是，几乎每个 Markdown 应用程序都支持另一种换行方式：HTML 的 \u0026lt;br\u0026gt; 标签。\n为了兼容性，请在行尾添加“结尾空格”或 HTML 的 \u0026lt;br\u0026gt; 标签来实现换行。\n还有两种其他方式我并不推荐使用。CommonMark 和其它几种轻量级标记语言支持在行尾添加反斜杠 (\\) 的方式实现换行，但是并非所有 Markdown 应用程序都支持此种方式，因此从兼容性的角度来看，不推荐使用。并且至少有两种轻量级标记语言支持无须在行尾添加任何内容，只须键入回车键（return）即可实现换行。\n✅ Do this ❌ Don\u0026rsquo;t do this First line with two spaces after. And the next line.First line with the HTML tag after.\u0026lt;br\u0026gt;And the next line. First line with a backslash after.\\And the next line.First line with nothing after.And the next line. Markdown 强调语法 通过将文本设置为粗体或斜体来强调其重要性。\n粗体（Bold） 要加粗文本，请在单词或短语的前后各添加两个星号（asterisks）或下划线（underscores）。如需加粗一个单词或短语的中间部分用以表示强调的话，请在要加粗部分的两侧各添加两个星号（asterisks）。\nMarkdown语法 HTML 预览效果 I just love **bold text**. I just love \u0026lt;strong\u0026gt;bold text\u0026lt;/strong\u0026gt;. I just love bold text. I just love __bold text__. I just love \u0026lt;strong\u0026gt;bold text\u0026lt;/strong\u0026gt;. I just love bold text. Love**is**bold Love\u0026lt;strong\u0026gt;is\u0026lt;/strong\u0026gt;bold Loveisbold 粗体（Bold）用法最佳实践 Markdown 应用程序在如何处理单词或短语中间的下划线上并不一致。为兼容考虑，在单词或短语中间部分加粗的话，请使用星号（asterisks）。\n✅ Do this ❌ Don\u0026rsquo;t do this Love**is**bold Love__is__bold 斜体（Italic） 要用斜体显示文本，请在单词或短语前后添加一个星号（asterisk）或下划线（underscore）。要斜体突出单词的中间部分，请在字母前后各添加一个星号，中间不要带空格。\nMarkdown语法 HTML 预览效果 Italicized text is the *cat's meow*. Italicized text is the \u0026lt;em\u0026gt;cat's meow\u0026lt;/em\u0026gt;. Italicized text is the cat’s meow. Italicized text is the _cat's meow_. Italicized text is the \u0026lt;em\u0026gt;cat's meow\u0026lt;/em\u0026gt;. Italicized text is the cat’s meow. A*cat*meow A\u0026lt;em\u0026gt;cat\u0026lt;/em\u0026gt;meow Acatmeow 斜体（Italic）用法的最佳实践 要同时用粗体和斜体突出显示文本，请在单词或短语的前后各添加三个星号或下划线。要加粗并用斜体显示单词或短语的中间部分，请在要突出显示的部分前后各添加三个星号，中间不要带空格。\n✅ Do this ❌ Don\u0026rsquo;t do this A*cat*meow A_cat_meow 粗体（Bold）和斜体（Italic） 要同时用粗体和斜体突出显示文本，请在单词或短语的前后各添加三个星号或下划线。要加粗并用斜体显示单词或短语的中间部分，请在要突出显示的部分前后各添加三个星号，中间不要带空格。\nMarkdown语法 HTML 预览效果 This text is ***really important***. This text is \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;really important\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;. This text is *really important*. This text is ___really important___. This text is \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;really important\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;. This text is *really important*. This text is __*really important*__. This text is \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;really important\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;. This text is *really important*. This text is **_really important_**. This text is \u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;really important\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;. This text is *really important*. This is really***very***important text. This is really\u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;very\u0026lt;/em\u0026gt;\u0026lt;/strong\u0026gt;important text. This is really***very***important text. 粗体（Bold）和斜体（Italic）用法的最佳实践 Markdown 应用程序在处理单词或短语中间添加的下划线上并不一致。为了实现兼容性，请使用星号将单词或短语的中间部分加粗并以斜体显示，以示重要。\n✅ Do this ❌ Don\u0026rsquo;t do this This is really***very***important text. This is really___very___important text. Markdown 引用语法 要创建块引用，请在段落前添加一个 \u0026gt; 符号。\n1 \u0026gt; Dorothy followed her through many of the beautiful rooms in her castle. 渲染效果如下所示：\nDorothy followed her through many of the beautiful rooms in her castle.\n多个段落的块引用\n块引用可以包含多个段落。为段落之间的空白行添加一个 \u0026gt; 符号。\n1 2 3 \u0026gt; Dorothy followed her through many of the beautiful rooms in her castle. \u0026gt; \u0026gt; The Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood. 渲染效果如下：\nDorothy followed her through many of the beautiful rooms in her castle.\nThe Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood.\n嵌套块引用\n块引用可以嵌套。在要嵌套的段落前添加一个 \u0026gt;\u0026gt; 符号。\n1 2 3 \u0026gt; Dorothy followed her through many of the beautiful rooms in her castle. \u0026gt; \u0026gt;\u0026gt; The Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood. 渲染效果如下：\nDorothy followed her through many of the beautiful rooms in her castle.\nThe Witch bade her clean the pots and kettles and sweep the floor and keep the fire fed with wood.\n带有其它元素的块引用\n块引用可以包含其他 Markdown 格式的元素。并非所有元素都可以使用，你需要进行实验以查看哪些元素有效。\n1 2 3 4 5 6 \u0026gt; #### The quarterly results look great! \u0026gt; \u0026gt; - Revenue was off the chart. \u0026gt; - Profits were higher than ever. \u0026gt; \u0026gt; *Everything* is going according to **plan**. 渲染效果如下：\nThe quarterly results look great! Revenue was off the chart. Profits were higher than ever. Everything is going according to plan.\nMarkdown 列表语法 可以将多个条目组织成有序或无序列表。\n有序列表 要创建有序列表，请在每个列表项前添加数字并紧跟一个英文句点。数字不必按数学顺序排列，但是列表应当以数字 1 起始。\nMarkdown语法 HTML 预览效果 1. First item2. Second item3. Third item4. Fourth item \u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt; First itemSecond itemThird itemFourth item 1. First item1. Second item1. Third item1. Fourth item \u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt; First itemSecond itemThird itemFourth item 1. First item8. Second item3. Third item5. Fourth item \u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt; First itemSecond itemThird itemFourth item 1. First item2. Second item3. Third item 1. Indented item 2. Indented item4. Fourth item \u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;Indented item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Indented item\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ol\u0026gt; First itemSecond itemThird itemIndented itemIndented itemFourth item 有序列表最佳实践 CommonMark and a few other lightweight markup languages let you use a parenthesis ()) as a delimiter (e.g., 1) First item), but not all Markdown applications support this, so it isn’t a great option from a compatibility perspective. For compatibility, use periods only.\n✅ Do this ❌ Don\u0026rsquo;t do this 1. First item2. Second item 1) First item2) Second item 无序列表 要创建无序列表，请在每个列表项前面添加破折号 (-)、星号 (*) 或加号 (+) 。缩进一个或多个列表项可创建嵌套列表。\nMarkdown语法 HTML 预览效果 - First item- Second item- Third item- Fourth item \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; First itemSecond itemThird itemFourth item * First item* Second item* Third item* Fourth item \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; First itemSecond itemThird itemFourth item + First item+ Second item+ Third item+ Fourth item \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; First itemSecond itemThird itemFourth item - First item- Second item- Third item - Indented item - Indented item- Fourth item \u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;First item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Second item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Third item\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt;Indented item\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Indented item\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt;Fourth item\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt; First itemSecond itemThird itemIndented itemIndented itemFourth item 无序列表最佳实践 Markdown applications don’t agree on how to handle different delimiters in the same list. For compatibility, don\u0026rsquo;t mix and match delimiters in the same list — pick one and stick with it.\n✅ Do this ❌ Don\u0026rsquo;t do this - First item- Second item- Third item- Fourth item + First item* Second item- Third item+ Fourth item 在列表中嵌套其他元素 要在保留列表连续性的同时在列表中添加另一种元素，请将该元素缩进四个空格或一个制表符，如下例所示：\n段落 1 2 3 4 5 6 * This is the first list item. * Here\u0026#39;s the second list item. I need to add another paragraph below the second list item. * And here\u0026#39;s the third list item. 渲染效果如下：\nThis is the first list item.\nHere\u0026rsquo;s the second list item.\nI need to add another paragraph below the second list item.\nAnd here\u0026rsquo;s the third list item.\n引用块 1 2 3 4 5 6 * This is the first list item. * Here\u0026#39;s the second list item. \u0026gt; A blockquote would look great below the second list item. * And here\u0026#39;s the third list item. 渲染效果如下：\nThis is the first list item.\nHere\u0026rsquo;s the second list item.\nA blockquote would look great below the second list item.\nAnd here\u0026rsquo;s the third list item.\n代码块 代码块通常采用四个空格或一个制表符缩进。当它们被放在列表中时，请将它们缩进八个空格或两个制表符。\n1 2 3 4 5 6 7 8 9 1. Open the file. 2. Find the following code block on line 21: \u0026amp;lt;html\u0026gt; \u0026amp;lt;head\u0026gt; \u0026amp;lt;title\u0026gt;Test\u0026amp;lt;/title\u0026gt; \u0026amp;lt;/head\u0026gt; 3. Update the title to match the name of your website. 渲染效果如下：\nOpen the file.\nFind the following code block on line 21:\n1 2 3 4 \u0026amp;lt;html\u0026gt; \u0026amp;lt;head\u0026gt; \u0026amp;lt;title\u0026gt;Test\u0026amp;lt;/title\u0026gt; \u0026amp;lt;/head\u0026gt; Update the title to match the name of your website.\n图片 1 2 3 4 5 6 1. Open the file containing the Linux mascot. 2. Marvel at its beauty. ![Tux, the Linux mascot](/assets/images/tux.png) 3. Close the file. 渲染效果如下：\nOpen the file containing the Linux mascot.\nMarvel at its beauty.\nClose the file.\n列表 You can nest an unordered list in an ordered list, or vice versa.\n1 2 3 4 5 6 1. First item 2. Second item 3. Third item - Indented item - Indented item 4. Fourth item 渲染效果如下：\nFirst item Second item Third item Indented item Indented item Fourth item Markdown 代码语法 要将单词或短语表示为代码，请将其包裹在反引号 (```) 中。\nMarkdown语法 HTML 预览效果 At the command prompt, type nano. At the command prompt, type \u0026lt;code\u0026gt;nano\u0026lt;/code\u0026gt;. At the command prompt, type nano. 转义反引号 如果你要表示为代码的单词或短语中包含一个或多个反引号，则可以通过将单词或短语包裹在双反引号(````)中。\nMarkdown语法 HTML 预览效果 Use `code` in your Markdown file. \u0026lt;code\u0026gt;Use code in your Markdown file.\u0026lt;/code\u0026gt; Use code in your Markdown file. 代码块 要创建代码块，请将代码块的每一行缩进至少四个空格或一个制表符。\n1 2 3 4 \u0026amp;lt;html\u0026gt; \u0026amp;lt;head\u0026gt; \u0026amp;lt;/head\u0026gt; \u0026amp;lt;/html\u0026gt; 渲染效果如下：\n1 2 3 4 \u0026amp;lt;html\u0026gt; \u0026amp;lt;head\u0026gt; \u0026amp;lt;/head\u0026gt; \u0026amp;lt;/html\u0026gt; Note: 要创建不用缩进的代码块，请使用 围栏式代码块（fenced code blocks）.\nMarkdown 分隔线语法 要创建分隔线，请在单独一行上使用三个或多个星号 (***)、破折号 (---) 或下划线 (___) ，并且不能包含其他内容。\n1 2 3 4 5 *** --- _________________ 以上三个分隔线的渲染效果看起来都一样：\n分隔线（Horizontal Rule）用法最佳实践 为了兼容性，请在分隔线的前后均添加空白行。\n✅ Do this ❌ Don\u0026rsquo;t do this Try to put a blank line before...---...and after a horizontal rule. Without blank lines, this would be a heading.---Don't do this! Markdown 链接语法 链接文本放在中括号内，链接地址放在后面的括号中，链接title可选。\n超链接Markdown语法代码：[超链接显示名](超链接地址 \u0026quot;超链接title\u0026quot;)\n对应的HTML代码：\u0026lt;a href=\u0026quot;超链接地址\u0026quot; title=\u0026quot;超链接title\u0026quot;\u0026gt;超链接显示名\u0026lt;/a\u0026gt;\n1 这是一个链接 [Markdown语法](https://markdown.com.cn)。 渲染效果如下：\n这是一个链接 Markdown语法 (opens new window)。\n给链接增加 Title 链接title是当鼠标悬停在链接上时会出现的文字，这个title是可选的，它放在圆括号中链接地址后面，跟链接地址之间以空格分隔。\n1 这是一个链接 [Markdown语法](https://markdown.com.cn \u0026#34;最好的markdown教程\u0026#34;)。 渲染效果如下：\n这是一个链接 Markdown语法 (opens new window)。\n网址和Email地址 使用尖括号可以很方便地把URL或者email地址变成可点击的链接。\n1 2 \u0026lt;https://markdown.com.cn\u0026gt; \u0026lt;fake@example.com\u0026gt; 渲染效果如下：\nhttps://markdown.com.cn(opens new window) fake@example.com\n带格式化的链接 强调 链接, 在链接语法前后增加星号。 要将链接表示为代码，请在方括号中添加反引号。\n1 2 3 I love supporting the **[EFF](https://eff.org)**. This is the *[Markdown Guide](https://www.markdownguide.org)*. See the section on [`code`](#code). 渲染效果如下：\nI love supporting the EFF (opens new window). This is the Markdown Guide (opens new window). See the section on code.\n引用类型链接 引用样式链接是一种特殊的链接，它使URL在Markdown中更易于显示和阅读。参考样式链接分为两部分：与文本保持内联的部分以及存储在文件中其他位置的部分，以使文本易于阅读。\n链接的第一部分格式 引用类型的链接的第一部分使用两组括号进行格式设置。第一组方括号包围应显示为链接的文本。第二组括号显示了一个标签，该标签用于指向您存储在文档其他位置的链接。\n尽管不是必需的，可以在第一组和第二组括号之间包含一个空格。第二组括号中的标签不区分大小写，可以包含字母，数字，空格或标点符号。\n以下示例格式对于链接的第一部分效果相同：\n[hobbit-hole][1] [hobbit-hole] [1] 链接的第二部分格式 引用类型链接的第二部分使用以下属性设置格式：\n放在括号中的标签，其后紧跟一个冒号和至少一个空格（例如[label]:）。 链接的URL，可以选择将其括在尖括号中。 链接的可选标题，可以将其括在双引号，单引号或括号中。 以下示例格式对于链接的第二部分效果相同：\n[1]: https://en.wikipedia.org/wiki/Hobbit#Lifestyle [1]: https://en.wikipedia.org/wiki/Hobbit#Lifestyle \u0026quot;Hobbit lifestyles\u0026quot; [1]: https://en.wikipedia.org/wiki/Hobbit#Lifestyle 'Hobbit lifestyles' [1]: https://en.wikipedia.org/wiki/Hobbit#Lifestyle (Hobbit lifestyles) [1]: \u0026lt;https://en.wikipedia.org/wiki/Hobbit#Lifestyle\u0026gt; \u0026quot;Hobbit lifestyles\u0026quot; [1]: \u0026lt;https://en.wikipedia.org/wiki/Hobbit#Lifestyle\u0026gt; 'Hobbit lifestyles' [1]: \u0026lt;https://en.wikipedia.org/wiki/Hobbit#Lifestyle\u0026gt; (Hobbit lifestyles) 可以将链接的第二部分放在Markdown文档中的任何位置。有些人将它们放在出现的段落之后，有些人则将它们放在文档的末尾（例如尾注或脚注）。\n链接最佳实践 不同的 Markdown 应用程序处理URL中间的空格方式不一样。为了兼容性，请尽量使用%20代替空格。\n✅ Do this ❌ Don\u0026rsquo;t do this [link](https://www.example.com/my%20great%20page) [link](https://www.example.com/my great page) Markdown 图片语法 要添加图像，请使用感叹号 (!), 然后在方括号增加替代文本，图片链接放在圆括号里，括号里的链接后可以增加一个可选的图片标题文本。\n插入图片Markdown语法代码：![图片alt](图片链接 \u0026quot;图片title\u0026quot;)。\n对应的HTML代码：\u0026lt;img src=\u0026quot;图片链接\u0026quot; alt=\u0026quot;图片alt\u0026quot; title=\u0026quot;图片title\u0026quot;\u0026gt;\n1 ![这是图片](/assets/img/philly-magic-garden.jpg \u0026#34;Magic Gardens\u0026#34;) 渲染效果如下：\n链接图片 给图片增加链接，请将图像的Markdown 括在方括号中，然后将链接添加在圆括号中。\n1 [![沙漠中的岩石图片](/assets/img/shiprock.jpg \u0026#34;Shiprock\u0026#34;)](https://markdown.com.cn) 渲染效果如下：\nMarkdown 转义字符语法 要显示原本用于格式化 Markdown 文档的字符，请在字符前面添加反斜杠字符 \\ 。\n1 \\* Without the backslash, this would be a bullet in an unordered list. 渲染效果如下：\n* Without the backslash, this would be a bullet in an unordered list.\n可做转义的字符 以下列出的字符都可以通过使用反斜杠字符从而达到转义目的。\nCharacter Name \\ backslash ` backtick (see also escaping backticks in code) * asterisk _ underscore { } curly braces [ ] brackets ( ) parentheses # pound sign + plus sign - minus sign (hyphen) . dot ! exclamation mark | pipe (see also escaping pipe in tables) 特殊字符自动转义 在 HTML 文件中，有两个字符需要特殊处理： \u0026lt; 和 \u0026amp; 。 \u0026lt; 符号用于起始标签，\u0026amp; 符号则用于标记 HTML 实体，如果你只是想要使用这些符号，你必须要使用实体的形式，像是 \u0026lt; 和 \u0026amp;。\n\u0026amp; 符号其实很容易让写作网页文件的人感到困扰，如果你要打「AT\u0026amp;T」 ，你必须要写成「AT\u0026amp;T」 ，还得转换网址内的 \u0026amp; 符号，如果你要链接到：\n1 http://images.google.com/images?num=30\u0026amp;q=larry+bird 你必须要把网址转成：\n1 http://images.google.com/images?num=30\u0026amp;amp;q=larry+bird 才能放到链接标签的 href 属性里。不用说也知道这很容易忘记，这也可能是 HTML 标准检查所检查到的错误中，数量最多的。\nMarkdown 允许你直接使用这些符号，它帮你自动转义字符。如果你使用 \u0026amp; 符号的作为 HTML 实体的一部分，那么它不会被转换，而在其它情况下，它则会被转换成 \u0026amp;。所以你如果要在文件中插入一个著作权的符号，你可以这样写：\n1 \u0026amp;copy; Markdown 将不会对这段文字做修改，但是如果你这样写：\n1 AT\u0026amp;T Markdown 就会将它转为：\n1 AT\u0026amp;amp;T 类似的状况也会发生在 \u0026lt; 符号上，因为 Markdown 支持 行内 HTML ，如果你使用 \u0026lt; 符号作为 HTML 标签的分隔符，那 Markdown 也不会对它做任何转换，但是如果你是写：\n1 4 \u0026lt; 5 Markdown 将会把它转换为：\n1 4 \u0026amp;lt; 5 需要特别注意的是，在 Markdown 的块级元素和内联元素中， \u0026lt; 和 \u0026amp; 两个符号都会被自动转换成 HTML 实体，这项特性让你可以很容易地用 Markdown 写 HTML。（在 HTML 语法中，你要手动把所有的 \u0026lt; 和 \u0026amp; 都转换为 HTML 实体。）\nMarkdown 内嵌 HTML 标签 对于 Markdown 涵盖范围之外的标签，都可以直接在文件里面用 HTML 本身。如需使用 HTML，不需要额外标注这是 HTML 或是 Markdown，只需 HTML 标签添加到 Markdown 文本中即可。\n行级內联标签 HTML 的行级內联标签如 \u0026lt;span\u0026gt;、\u0026lt;cite\u0026gt;、\u0026lt;del\u0026gt; 不受限制，可以在 Markdown 的段落、列表或是标题里任意使用。依照个人习惯，甚至可以不用 Markdown 格式，而采用 HTML 标签来格式化。例如：如果比较喜欢 HTML 的 \u0026lt;a\u0026gt; 或 \u0026lt;img\u0026gt; 标签，可以直接使用这些标签，而不用 Markdown 提供的链接或是图片语法。当你需要更改元素的属性时（例如为文本指定颜色或更改图像的宽度），使用 HTML 标签更方便些。\nHTML 行级內联标签和区块标签不同，在內联标签的范围内， Markdown 的语法是可以解析的。\n1 This **word** is bold. This \u0026lt;em\u0026gt;word\u0026lt;/em\u0026gt; is italic. 渲染效果如下:\nThis word is bold. This word is italic.\n区块标签 区块元素──比如 \u0026lt;div\u0026gt;、\u0026lt;table\u0026gt;、\u0026lt;pre\u0026gt;、\u0026lt;p\u0026gt; 等标签，必须在前后加上空行，以便于内容区分。而且这些元素的开始与结尾标签，不可以用 tab 或是空白来缩进。Markdown 会自动识别这区块元素，避免在区块标签前后加上没有必要的 \u0026lt;p\u0026gt; 标签。\n例如，在 Markdown 文件里加上一段 HTML 表格：\n1 2 3 4 5 6 7 8 9 This is a regular paragraph. \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;Foo\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; This is another regular paragraph. 请注意，Markdown 语法在 HTML 区块标签中将不会被进行处理。例如，你无法在 HTML 区块内使用 Markdown 形式的*强调*。\nHTML 用法最佳实践 出于安全原因，并非所有 Markdown 应用程序都支持在 Markdown 文档中添加 HTML。如有疑问，请查看相应 Markdown 应用程序的手册。某些应用程序只支持 HTML 标签的子集。\n对于 HTML 的块级元素 \u0026lt;div\u0026gt;、\u0026lt;table\u0026gt;、\u0026lt;pre\u0026gt; 和 \u0026lt;p\u0026gt;，请在其前后使用空行（blank lines）与其它内容进行分隔。尽量不要使用制表符（tabs）或空格（spaces）对 HTML 标签做缩进，否则将影响格式。\n在 HTML 块级标签内不能使用 Markdown 语法。例如 \u0026lt;p\u0026gt;italic and **bold**\u0026lt;/p\u0026gt; 将不起作用。\n","date":"2024-05-11T00:00:00Z","image":"https://nova-bryan.github.io/p/markdown_grammar/mark_hu8927113685150964021.png","permalink":"https://nova-bryan.github.io/p/markdown_grammar/","title":"Markdown_Grammar"},{"content":"求整数的因子（也称为 约数）可以使用 从 1 到 √N 迭代的方法，这样可以大幅减少计算次数，从 O(N) 优化到 O(√N)，大幅提升速度。\n1. O(√N) 高效因子枚举 原理：\n如果 x 是 N 的因子，则 N/x 也是 N 的因子，只需要遍历 1 到 √N 即可找到所有因子。 代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import java.util.*; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); int n = sc.nextInt(); // 读取整数 N sc.close(); List\u0026lt;Integer\u0026gt; factors = new ArrayList\u0026lt;\u0026gt;(); for (int i = 1; i * i \u0026lt;= n; i++) { if (n % i == 0) { factors.add(i); // i 是因子 if (i != n / i) { factors.add(n / i); // N / i 也是因子 } } } // 因子排序（如果需要从小到大排列） Collections.sort(factors); System.out.println(factors); } } 示例：\n1 2 输入: 36 输出: [1, 2, 3, 4, 6, 9, 12, 18, 36] 时间复杂度：O(√N)，相比 O(N) 的暴力方法，速度快很多！\n2. 只判断因子个数（不存储） 如果你只想知道 因子个数，可以用计数器：\n1 2 3 4 5 6 7 8 int count = 0; for (int i = 1; i * i \u0026lt;= n; i++) { if (n % i == 0) { count++; if (i != n / i) count++; // 计数因子 } } System.out.println(count); 示例：\n1 2 输入: 36 输出: 9 // 因子个数 3. 特殊优化：只找素因子 如果你只想找 素数因子（即 N 被哪些素数整除），可以用试除法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public static List\u0026lt;Integer\u0026gt; primeFactors(int n) { List\u0026lt;Integer\u0026gt; primes = new ArrayList\u0026lt;\u0026gt;(); // 先除去 2 while (n % 2 == 0) { primes.add(2); n /= 2; } // 再检查奇数 for (int i = 3; i * i \u0026lt;= n; i += 2) { while (n % i == 0) { primes.add(i); n /= i; } } // 如果剩余的是素数 if (n \u0026gt; 1) primes.add(n); return primes; } 示例：\n1 2 输入: 36 输出: [2, 2, 3, 3] // 36 = 2² × 3² 时间复杂度：O(√N)，比 O(N) 方法快很多。\n总结 方法 适用场景 复杂度 备注 O(√N) 遍历因子 找出所有因子 O(√N) 推荐 O(√N) 计数因子 只求因子个数 O(√N) 无需存储 试除法找素因子 仅找素因子 O(√N) 用于分解质因数 如果只是找因子，推荐 O(√N) 方法，速度快，代码简洁！ 🚀\n例题： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 import java.util.*; public class Main { public static void main(String[] args) { Scanner sc = new Scanner(System.in); long n = sc.nextLong(); // 读取 N，注意这里用 long，防止溢出 sc.close(); int maxLen = 0; // 记录最长长度 int start = 0; // 记录最长序列的起始因子 // 遍历起始因子 i，从 2 开始（1 不算） for (int i = 2; i * i \u0026lt;= n; i++) { long product = 1; int j = i; // 计算连续乘积，直到超出 N while (product * j \u0026lt;= n) { product *= j; // 如果这个连续乘积刚好整除 N if (n % product == 0) { int length = j - i + 1; // 计算当前序列的长度 if (length \u0026gt; maxLen) { maxLen = length; start = i; } } j++; // 继续尝试下一个连续因子 } } // 如果没有找到合适的连续因子，最长序列就是 N 本身 if (maxLen == 0) { System.out.println(1); System.out.println(n); } else { System.out.println(maxLen); for (int i = 0; i \u0026lt; maxLen; i++) { if (i \u0026gt; 0) System.out.print(\u0026#34;*\u0026#34;); System.out.print(start + i); } } } } ","date":"2024-05-11T00:00:00Z","image":"https://nova-bryan.github.io/p/%E6%95%B4%E6%95%B0%E5%9B%A0%E5%AD%90%E7%9A%84%E5%BF%AB%E9%80%9F%E6%B1%82%E6%B3%95/image_hu7343995253115549451.png","permalink":"https://nova-bryan.github.io/p/%E6%95%B4%E6%95%B0%E5%9B%A0%E5%AD%90%E7%9A%84%E5%BF%AB%E9%80%9F%E6%B1%82%E6%B3%95/","title":"整数因子的快速求法"},{"content":"MySQL-DQL-基本查询 介绍\nDQL英文全称是Data Query Language(数据查询语言)，用来查询数据库表中的记录。\n查询关键字：SELECT\n查询操作是所有SQL语句当中最为常见，也是最为重要的操作。在一个正常的业务系统中，查询操作的使用频次是要远高于增删改操作的。当我们打开某个网站或APP所看到的展示信息，都是通过从数据库中查询得到的，而在这个查询过程中，还会涉及到条件、排序、分页等操作。\n语法\nDQL查询语句，语法结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT 字段列表 FROM 表名列表 WHERE 条件列表 GROUP BY 分组字段列表 HAVING 分组后条件列表 ORDER BY 排序字段列表 LIMIT 分页参数 我们今天会将上面的完整语法拆分为以下几个部分学习：\n基本查询（不带任何条件） 条件查询（where） 分组查询（group by） 排序查询（order by） 分页查询（limit） 准备一些测试数据用于查询操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 create database db02; -- 创建数据库 use db02; -- 切换数据库 -- 员工管理(带约束) create table tb_emp ( id int unsigned primary key auto_increment comment \u0026#39;ID\u0026#39;, username varchar(20) not null unique comment \u0026#39;用户名\u0026#39;, password varchar(32) default \u0026#39;123456\u0026#39; comment \u0026#39;密码\u0026#39;, name varchar(10) not null comment \u0026#39;姓名\u0026#39;, gender tinyint unsigned not null comment \u0026#39;性别, 说明: 1 男, 2 女\u0026#39;, image varchar(300) comment \u0026#39;图像\u0026#39;, job tinyint unsigned comment \u0026#39;职位, 说明: 1 班主任,2 讲师, 3 学工主管, 4 教研主管\u0026#39;, entrydate date comment \u0026#39;入职时间\u0026#39;, create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;修改时间\u0026#39; ) comment \u0026#39;员工表\u0026#39;; -- 准备测试数据 INSERT INTO tb_emp (id, username, password, name, gender, image, job, entrydate, create_time, update_time) VALUES (1, \u0026#39;jinyong\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;金庸\u0026#39;, 1, \u0026#39;1.jpg\u0026#39;, 4, \u0026#39;2000-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:35\u0026#39;), (2, \u0026#39;zhangwuji\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张无忌\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2015-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:37\u0026#39;), (3, \u0026#39;yangxiao\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;杨逍\u0026#39;, 1, \u0026#39;3.jpg\u0026#39;, 2, \u0026#39;2008-05-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:39\u0026#39;), (4, \u0026#39;weiyixiao\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;韦一笑\u0026#39;, 1, \u0026#39;4.jpg\u0026#39;, 2, \u0026#39;2007-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:41\u0026#39;), (5, \u0026#39;changyuchun\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;常遇春\u0026#39;, 1, \u0026#39;5.jpg\u0026#39;, 2, \u0026#39;2012-12-05\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:43\u0026#39;), (6, \u0026#39;xiaozhao\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;小昭\u0026#39;, 2, \u0026#39;6.jpg\u0026#39;, 3, \u0026#39;2013-09-05\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:45\u0026#39;), (7, \u0026#39;jixiaofu\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;纪晓芙\u0026#39;, 2, \u0026#39;7.jpg\u0026#39;, 1, \u0026#39;2005-08-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:47\u0026#39;), (8, \u0026#39;zhouzhiruo\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;周芷若\u0026#39;, 2, \u0026#39;8.jpg\u0026#39;, 1, \u0026#39;2014-11-09\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:49\u0026#39;), (9, \u0026#39;dingminjun\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;丁敏君\u0026#39;, 2, \u0026#39;9.jpg\u0026#39;, 1, \u0026#39;2011-03-11\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:51\u0026#39;), (10, \u0026#39;zhaomin\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;赵敏\u0026#39;, 2, \u0026#39;10.jpg\u0026#39;, 1, \u0026#39;2013-09-05\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:53\u0026#39;), (11, \u0026#39;luzhangke\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;鹿杖客\u0026#39;, 1, \u0026#39;11.jpg\u0026#39;, 2, \u0026#39;2007-02-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:55\u0026#39;), (12, \u0026#39;hebiweng\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;鹤笔翁\u0026#39;, 1, \u0026#39;12.jpg\u0026#39;, 2, \u0026#39;2008-08-18\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:57\u0026#39;), (13, \u0026#39;fangdongbai\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;方东白\u0026#39;, 1, \u0026#39;13.jpg\u0026#39;, 1, \u0026#39;2012-11-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:35:59\u0026#39;), (14, \u0026#39;zhangsanfeng\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张三丰\u0026#39;, 1, \u0026#39;14.jpg\u0026#39;, 2, \u0026#39;2002-08-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:01\u0026#39;), (15, \u0026#39;yulianzhou\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;俞莲舟\u0026#39;, 1, \u0026#39;15.jpg\u0026#39;, 2, \u0026#39;2011-05-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:03\u0026#39;), (16, \u0026#39;songyuanqiao\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;宋远桥\u0026#39;, 1, \u0026#39;16.jpg\u0026#39;, 2, \u0026#39;2010-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:05\u0026#39;), (17, \u0026#39;chenyouliang\u0026#39;, \u0026#39;12345678\u0026#39;, \u0026#39;陈友谅\u0026#39;, 1, \u0026#39;17.jpg\u0026#39;, null, \u0026#39;2015-03-21\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:07\u0026#39;), (18, \u0026#39;zhang1\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张一\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2015-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:09\u0026#39;), (19, \u0026#39;zhang2\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张二\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2012-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:11\u0026#39;), (20, \u0026#39;zhang3\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张三\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2018-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:13\u0026#39;), (21, \u0026#39;zhang4\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张四\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2015-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:15\u0026#39;), (22, \u0026#39;zhang5\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张五\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2016-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:17\u0026#39;), (23, \u0026#39;zhang6\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张六\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2012-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:19\u0026#39;), (24, \u0026#39;zhang7\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张七\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2006-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:21\u0026#39;), (25, \u0026#39;zhang8\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张八\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2002-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:23\u0026#39;), (26, \u0026#39;zhang9\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张九\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2011-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:25\u0026#39;), (27, \u0026#39;zhang10\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张十\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2004-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:27\u0026#39;), (28, \u0026#39;zhang11\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张十一\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2007-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:29\u0026#39;), (29, \u0026#39;zhang12\u0026#39;, \u0026#39;123456\u0026#39;, \u0026#39;张十二\u0026#39;, 1, \u0026#39;2.jpg\u0026#39;, 2, \u0026#39;2020-01-01\u0026#39;, \u0026#39;2022-10-27 16:35:33\u0026#39;, \u0026#39;2022-10-27 16:36:31\u0026#39;); 基本查询\n在基本查询的DQL语句中，不带任何的查询条件，语法如下：\n查询多个字段 1 select 字段1, 字段2, 字段3 from 表名; 查询所有字段（通配符） 1 select * from 表名; 设置别名 1 select 字段1 [ as 别名1 ] , 字段2 [ as 别名2 ] from 表名; 去除重复记录 1 select distinct 字段列表 from 表名; 案例1：查询指定字段 name，entrydate并返回\n1 select name,entrydate from tb_emp; 案例2：查询返回所有字段\n1 select * from tb_emp; *号代表查询所有字段，在实际开发中尽量少用（不直观、影响效率）\n案例3：查询所有员工的 name,entrydate，并起别名(姓名、入职日期)\n1 2 3 4 5 6 -- 方式1： select name AS 姓名, entrydate AS 入职日期 from tb_emp; -- 方式2： 别名中有特殊字符时，使用\u0026#39;\u0026#39;或\u0026#34;\u0026#34;包含 select name AS \u0026#39;姓 名\u0026#39;, entrydate AS \u0026#39;入职日期\u0026#39; from tb_emp; -- 方式3： select name AS \u0026#34;姓名\u0026#34;, entrydate AS \u0026#34;入职日期\u0026#34; from tb_emp; 案例4：查询已有的员工关联了哪几种职位(不要重复)\n1 select distinct job from tb_emp; MySQL-DQL-条件查询 条件查询\n语法：\n1 select 字段列表 from 表名 where 条件列表 ; -- 条件列表：意味着可以有多个条件 学习条件查询就是学习条件的构建方式，而在SQL语句当中构造条件的运算符分为两类：\n比较运算符 逻辑运算符 常用的比较运算符如下:\n比较运算符 功能 \u0026gt; 大于 \u0026gt;= 大于等于 \u0026lt; 小于 \u0026lt;= 小于等于 = 等于 \u0026lt;\u0026gt; 或 != 不等于 between \u0026hellip; and \u0026hellip; 在某个范围之内(含最小、最大值) in(\u0026hellip;) 在in之后的列表中的值，多选一 like 占位符 模糊匹配(_匹配单个字符, %匹配任意个字符) is null 是null 常用的逻辑运算符如下:\n逻辑运算符 功能 and 或 \u0026amp;\u0026amp; 并且 (多个条件同时成立) or 或 || 或者 (多个条件任意一个成立) not 或 ! 非 , 不是 案例1：查询 姓名 为 杨逍 的员工\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where name = \u0026#39;杨逍\u0026#39;; -- 字符串使用\u0026#39;\u0026#39;或\u0026#34;\u0026#34;包含 案例2：查询 id小于等于5 的员工信息\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where id \u0026lt;=5; 案例3：查询 没有分配职位 的员工信息\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where job is null ; 注意：查询为NULL的数据时，不能使用 = null\n案例4：查询 有职位 的员工信息\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where job is not null ; 案例5：查询 密码不等于 \u0026lsquo;123456\u0026rsquo; 的员工信息\n1 2 3 4 5 6 7 8 -- 方式1： select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where password \u0026lt;\u0026gt; \u0026#39;123456\u0026#39;; -- 方式2： select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where password != \u0026#39;123456\u0026#39;; 案例6：查询 入职日期 在 \u0026lsquo;2000-01-01\u0026rsquo; (包含) 到 \u0026lsquo;2010-01-01\u0026rsquo;(包含) 之间的员工信息\n1 2 3 4 5 6 7 8 -- 方式1： select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where entrydate\u0026gt;=\u0026#39;2000-01-01\u0026#39; and entrydate\u0026lt;=\u0026#39;2010-01-01\u0026#39;; -- 方式2： between...and select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where entrydate between \u0026#39;2000-01-01\u0026#39; and \u0026#39;2010-01-01\u0026#39;; 案例7：查询 入职时间 在 \u0026lsquo;2000-01-01\u0026rsquo; (包含) 到 \u0026lsquo;2010-01-01\u0026rsquo;(包含) 之间 且 性别为女 的员工信息\n1 2 3 4 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where entrydate between \u0026#39;2000-01-01\u0026#39; and \u0026#39;2010-01-01\u0026#39; and gender = 2; 案例8：查询 职位是 2 (讲师), 3 (学工主管), 4 (教研主管) 的员工信息\n1 2 3 4 5 6 7 8 -- 方式1：使用or连接多个条件 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where job=2 or job=3 or job=4; -- 方式2：in关键字 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where job in (2,3,4); 案例9：查询 姓名 为两个字的员工信息\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where name like \u0026#39;__\u0026#39;; # 通配符 \u0026#34;_\u0026#34; 代表任意1个字符 案例10：查询 姓 \u0026lsquo;张\u0026rsquo; 的员工信息\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where name like \u0026#39;张%\u0026#39;; # 通配符 \u0026#34;%\u0026#34; 代表任意个字符（0个 ~ 多个） MySQL-DQL-聚合函数 之前我们做的查询都是横向查询，就是根据条件一行一行的进行判断，而使用聚合函数查询就是纵向查询，它是对一列的值进行计算，然后返回一个结果值。（将一列数据作为一个整体，进行纵向计算）\n语法：\n1 select 聚合函数(字段列表) from 表名 ; 注意 : 聚合函数会忽略空值，对NULL值不作为统计。\n常用聚合函数：\n函数 功能 count 统计数量 max 最大值 min 最小值 avg 平均值 sum 求和 count ：按照列去统计有多少行数据。\n在根据指定的列统计的时候，如果这一列中有null的行，该行不会被统计在其中。 sum ：计算指定列的数值和，如果不是数值类型，那么计算结果为0\nmax ：计算指定列的最大值\nmin ：计算指定列的最小值\navg ：计算指定列的平均值\n案例1：统计该企业员工数量\n1 2 3 4 5 6 7 8 9 10 # count(字段) select count(id) from tb_emp;-- 结果：29 select count(job) from tb_emp;-- 结果：28 （聚合函数对NULL值不做计算） # count(常量) select count(0) from tb_emp; select count(\u0026#39;A\u0026#39;) from tb_emp; # count(*) 推荐此写法（MySQL底层进行了优化） select count(*) from tb_emp; 案例2：统计该企业最早入职的员工\n1 select min(entrydate) from tb_emp; 案例3：统计该企业最迟入职的员工\n1 select max(entrydate) from tb_emp; 案例4：统计该企业员工 ID 的平均值\n1 select avg(id) from tb_emp; 案例5：统计该企业员工的 ID 之和\n1 select sum(id) from tb_emp; MySQL-DQL-分组查询 分组： 按照某一列或者某几列，把相同的数据进行合并输出。\n分组其实就是按列进行分类(指定列下相同的数据归为一类)，然后可以对分类完的数据进行合并计算。\n分组查询通常会使用聚合函数进行计算。\n语法：\n1 select 字段列表 from 表名 [where 条件] group by 分组字段名 [having 分组后过滤条件]; 案例1：根据性别分组 , 统计男性和女性员工的数量\n1 2 3 select gender, count(*) from tb_emp group by gender; -- 按照gender字段进行分组（gender字段下相同的数据归为一组） 案例2：查询入职时间在 \u0026lsquo;2015-01-01\u0026rsquo; (包含) 以前的员工 , 并对结果根据职位分组 , 获取员工数量大于等于2的职位\n1 2 3 4 5 select job, count(*) from tb_emp where entrydate \u0026lt;= \u0026#39;2015-01-01\u0026#39; -- 分组前条件 group by job -- 按照job字段分组 having count(*) \u0026gt;= 2; -- 分组后条件 注意事项:\n• 分组之后，查询的字段一般为聚合函数和分组字段，查询其他字段无任何意义\n• 执行顺序：where \u0026gt; 聚合函数 \u0026gt; having\nwhere与having区别（面试题）\n执行时机不同：where是分组之前进行过滤，不满足where条件，不参与分组；而having是分组之后对结果进行过滤。 判断条件不同：where不能对聚合函数进行判断，而having可以。 MySQL-DQL-排序查询 排序在日常开发中是非常常见的一个操作，有升序排序，也有降序排序。\n语法：\n1 2 3 4 5 select 字段列表 from 表名 [where 条件列表] [group by 分组字段 ] order by 字段1 排序方式1 , 字段2 排序方式2 … ; 排序方式：\nASC ：升序（默认值） DESC：降序 案例1：根据入职时间, 对员工进行升序排序\n1 2 3 4 5 6 7 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp order by entrydate ASC; -- 按照entrydate字段下的数据进行升序排序 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp order by entrydate; -- 默认就是ASC（升序） 注意事项：如果是升序, 可以不指定排序方式ASC\n案例2：根据入职时间，对员工进行降序排序\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp order by entrydate DESC; -- 按照entrydate字段下的数据进行降序排序 案例3：根据入职时间对公司的员工进行升序排序，入职时间相同，再按照更新时间进行降序排序\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp order by entrydate ASC , update_time DESC; 注意事项：如果是多字段排序，当第一个字段值相同时，才会根据第二个字段进行排序\nMySQL-DQL-分页查询 分页操作在业务系统开发时，也是非常常见的一个功能，日常我们在网站中看到的各种各样的分页条，后台也都需要借助于数据库的分页操作。\n分页查询语法：\n1 select 字段列表 from 表名 limit 起始索引, 查询记录数 ; 案例1：从起始索引0开始查询员工数据, 每页展示5条记录\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp limit 0 , 5; -- 从索引0开始，向后取5条记录 案例2：查询 第1页 员工数据, 每页展示5条记录\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp limit 5; -- 如果查询的是第1页数据，起始索引可以省略，直接简写为：limit 条数 案例3：查询 第2页 员工数据, 每页展示5条记录\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp limit 5 , 5; -- 从索引5开始，向后取5条记录 案例4：查询 第3页 员工数据, 每页展示5条记录\n1 2 3 select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp limit 10 , 5; -- 从索引10开始，向后取5条记录 注意事项:\n起始索引从0开始。 计算公式 ： 起始索引 = （查询页码 - 1）* 每页显示记录数 分页查询是数据库的方言，不同的数据库有不同的实现，MySQL中是LIMIT 如果查询的是第一页数据，起始索引可以省略，直接简写为 limit 条数 MySQL-DQL-案例 案例一\n案例：根据需求完成员工管理的条件分页查询\n分析：根据输入的条件，查询第1页数据\n在员工管理的列表上方有一些查询条件：员工姓名、员工性别，员工入职时间(开始时间~结束时间) 姓名：张 性别：男 入职时间：2000-01-01 ~ 2015-12-31 除了查询条件外，在列表的下面还有一个分页条，这就涉及到了分页查询 查询第1页数据（每页显示10条数据） 基于查询的结果，按照修改时间进行降序排序 结论：条件查询 + 分页查询 + 排序查询\n1 2 3 4 5 6 7 8 9 10 11 12 -- 根据输入条件查询第1页数据（每页展示10条记录） -- 输入条件： -- 姓名：张 （模糊查询） -- 性别：男 -- 入职时间：2000-01-01 ~ 2015-12-31 -- 分页： 0 , 10 -- 排序： 修改时间 DESC select id, username, password, name, gender, image, job, entrydate, create_time, update_time from tb_emp where name like \u0026#39;张%\u0026#39; and gender = 1 and entrydate between \u0026#39;2000-01-01\u0026#39; and \u0026#39;2015-12-31\u0026#39; order by update_time desc limit 0 , 10; 案例二\n案例：根据需求完成员工信息的统计\n分析：以上信息统计在开发中也叫图形报表(将统计好的数据以可视化的形式展示出来)\n员工性别统计：以饼状图的形式展示出企业男性员人数和女性员工人数 只要查询出男性员工和女性员工各自有多少人就可以了 员工职位统计：以柱状图的形式展示各职位的在岗人数 只要查询出各个职位有多少人就可以了 员工性别统计：\n1 2 3 4 -- if(条件表达式, true取值 , false取值) select if(gender=1,\u0026#39;男性员工\u0026#39;,\u0026#39;女性员工\u0026#39;) AS 性别, count(*) AS 人数 from tb_emp group by gender; if(表达式, tvalue, fvalue) ：当表达式为true时，取值tvalue；当表达式为false时，取值fvalue\n员工职位统计：\n1 2 3 4 5 6 7 8 9 10 11 -- case 表达式 when 值1 then 结果1 when 值2 then 结果2 ... else result end select (case job when 1 then \u0026#39;班主任\u0026#39; when 2 then \u0026#39;讲师\u0026#39; when 3 then \u0026#39;学工主管\u0026#39; when 4 then \u0026#39;教研主管\u0026#39; else \u0026#39;未分配职位\u0026#39; end) AS 职位 , count(*) AS 人数 from tb_emp group by job; case 表达式 when 值1 then 结果1 [when 值2 then 结果2 \u0026hellip;] [else result] end\nMySQL-多表设计-一对多 项目开发中，在进行数据库表结构设计时，会根据业务需求及业务模块之间的关系，分析并设计表结构，由于业务之间相互关联，所以各个表结构之间也存在着各种联系，基本上分为三种：\n一对多(多对一) 多对多 一对一 一对多\n表设计\n需求：根据页面原型及需求文档 ，完成部门及员工的表结构设计\n员工管理页面原型：（前面已完成tb_emp表结构设计） 部门管理页面原型： 经过上述分析，现已明确的部门表结构：\n业务字段 ： 部门名称 基础字段 ： id(主键)、创建时间、修改时间 部门表 - SQL语句：\n1 2 3 4 5 6 7 8 9 10 11 12 # 建议：创建新的数据库（多表设计存放在新数据库下） create database db03; use db03; -- 部门表 create table tb_dept ( id int unsigned primary key auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(10) not null unique comment \u0026#39;部门名称\u0026#39;, create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;修改时间\u0026#39; ) comment \u0026#39;部门表\u0026#39;; 部门表创建好之后，我们还需要再修改下员工表。为什么要修改员工表呢？是因为我们之前设计员工表(单表)的时候，并没有考虑员工的归属部门。\n员工表：添加归属部门字段\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -- 员工表 create table tb_emp ( id int unsigned primary key auto_increment comment \u0026#39;ID\u0026#39;, username varchar(20) not null unique comment \u0026#39;用户名\u0026#39;, password varchar(32) default \u0026#39;123456\u0026#39; comment \u0026#39;密码\u0026#39;, name varchar(10) not null comment \u0026#39;姓名\u0026#39;, gender tinyint unsigned not null comment \u0026#39;性别, 说明: 1 男, 2 女\u0026#39;, image varchar(300) comment \u0026#39;图像\u0026#39;, job tinyint unsigned comment \u0026#39;职位, 说明: 1 班主任,2 讲师, 3 学工主管, 4 教研主管\u0026#39;, entrydate date comment \u0026#39;入职时间\u0026#39;, dept_id int unsigned comment \u0026#39;部门ID\u0026#39;, -- 员工的归属部门 create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;修改时间\u0026#39; ) comment \u0026#39;员工表\u0026#39;; 测试数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- 部门表测试数据 insert into tb_dept (id, name, create_time, update_time) values (1,\u0026#39;学工部\u0026#39;,now(),now()), (2,\u0026#39;教研部\u0026#39;,now(),now()), (3,\u0026#39;咨询部\u0026#39;,now(),now()), (4,\u0026#39;就业部\u0026#39;,now(),now()), (5,\u0026#39;人事部\u0026#39;,now(),now()); -- 员工表测试数据 INSERT INTO tb_emp (id, username, password, name, gender, image, job, entrydate,dept_id, create_time, update_time) VALUES (1,\u0026#39;jinyong\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;金庸\u0026#39;,1,\u0026#39;1.jpg\u0026#39;,4,\u0026#39;2000-01-01\u0026#39;,2,now(),now()), (2,\u0026#39;zhangwuji\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;张无忌\u0026#39;,1,\u0026#39;2.jpg\u0026#39;,2,\u0026#39;2015-01-01\u0026#39;,2,now(),now()), (3,\u0026#39;yangxiao\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;杨逍\u0026#39;,1,\u0026#39;3.jpg\u0026#39;,2,\u0026#39;2008-05-01\u0026#39;,2,now(),now()), (4,\u0026#39;weiyixiao\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;韦一笑\u0026#39;,1,\u0026#39;4.jpg\u0026#39;,2,\u0026#39;2007-01-01\u0026#39;,2,now(),now()), (5,\u0026#39;changyuchun\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;常遇春\u0026#39;,1,\u0026#39;5.jpg\u0026#39;,2,\u0026#39;2012-12-05\u0026#39;,2,now(),now()), (6,\u0026#39;xiaozhao\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;小昭\u0026#39;,2,\u0026#39;6.jpg\u0026#39;,3,\u0026#39;2013-09-05\u0026#39;,1,now(),now()), (7,\u0026#39;jixiaofu\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;纪晓芙\u0026#39;,2,\u0026#39;7.jpg\u0026#39;,1,\u0026#39;2005-08-01\u0026#39;,1,now(),now()), (8,\u0026#39;zhouzhiruo\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;周芷若\u0026#39;,2,\u0026#39;8.jpg\u0026#39;,1,\u0026#39;2014-11-09\u0026#39;,1,now(),now()), (9,\u0026#39;dingminjun\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;丁敏君\u0026#39;,2,\u0026#39;9.jpg\u0026#39;,1,\u0026#39;2011-03-11\u0026#39;,1,now(),now()), (10,\u0026#39;zhaomin\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;赵敏\u0026#39;,2,\u0026#39;10.jpg\u0026#39;,1,\u0026#39;2013-09-05\u0026#39;,1,now(),now()), (11,\u0026#39;luzhangke\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;鹿杖客\u0026#39;,1,\u0026#39;11.jpg\u0026#39;,1,\u0026#39;2007-02-01\u0026#39;,1,now(),now()), (12,\u0026#39;hebiweng\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;鹤笔翁\u0026#39;,1,\u0026#39;12.jpg\u0026#39;,1,\u0026#39;2008-08-18\u0026#39;,1,now(),now()), (13,\u0026#39;fangdongbai\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;方东白\u0026#39;,1,\u0026#39;13.jpg\u0026#39;,2,\u0026#39;2012-11-01\u0026#39;,2,now(),now()), (14,\u0026#39;zhangsanfeng\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;张三丰\u0026#39;,1,\u0026#39;14.jpg\u0026#39;,2,\u0026#39;2002-08-01\u0026#39;,2,now(),now()), (15,\u0026#39;yulianzhou\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;俞莲舟\u0026#39;,1,\u0026#39;15.jpg\u0026#39;,2,\u0026#39;2011-05-01\u0026#39;,2,now(),now()), (16,\u0026#39;songyuanqiao\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;宋远桥\u0026#39;,1,\u0026#39;16.jpg\u0026#39;,2,\u0026#39;2010-01-01\u0026#39;,2,now(),now()), (17,\u0026#39;chenyouliang\u0026#39;,\u0026#39;123456\u0026#39;,\u0026#39;陈友谅\u0026#39;,1,\u0026#39;17.jpg\u0026#39;,NULL,\u0026#39;2015-03-21\u0026#39;,NULL,now(),now()); 员工表 - 部门表之间的关系：\n一对多关系实现：在数据库表中多的一方，添加字段，来关联属于一这方的主键。\nMySQL-多表设计-一对多-外键 外键约束\n问题\n表结构创建完毕后，我们看到两张表的数据分别为： 现在员工表中有五个员工都归属于1号部门(学工部)，当删除了1号部门后，数据变为：\n1号部门被删除了，但是依然还有5个员工是属于1号部门的。 此时：就出现数据的不完整、不一致了。\n问题分析\n目前上述的两张表(员工表、部门表)，在数据库层面，并未建立关联，所以是无法保证数据的一致性和完整性的\n问题解决\n想解决上述的问题呢，我们就可以通过数据库中的 外键约束 来解决。\n外键约束：让两张表的数据建立连接，保证数据的一致性和完整性。\n对应的关键字：foreign key\n外键约束的语法：\n1 2 3 4 5 6 7 8 9 10 -- 创建表时指定 create table 表名( 字段名 数据类型, ... [constraint] [外键名称] foreign key (外键字段名) references 主表 (主表列名) ); -- 建完表后，添加外键 alter table 表名 add constraint 外键名称 foreign key(外键字段名) references 主表(主表列名); 那接下来，我们就为员工表的dept_id 建立外键约束，来关联部门表的主键。\n方式1：通过SQL语句操作\n1 2 3 -- 修改表： 添加外键约束 alter table tb_emp add constraint fk_dept_id foreign key (dept_id) references tb_dept(id); 方式2：图形化界面操作\n当我们添加外键约束时，我们得保证当前数据库表中的数据是完整的。 所以，我们需要将之前删除掉的数据再添加回来。\n当我们添加了外键之后，再删除ID为1的部门，就会发现，此时数据库报错了，不允许删除。\n外键约束（foreign key）：保证了数据的完整性和一致性。\n物理外键和逻辑外键\n物理外键 概念：使用foreign key定义外键关联另外一张表。 缺点： 影响增、删、改的效率（需要检查外键关系）。 仅用于单节点数据库，不适用与分布式、集群场景。 容易引发数据库的死锁问题，消耗性能。 逻辑外键 概念：在业务层逻辑中，解决外键关联。 通过逻辑外键，就可以很方便的解决上述问题。 在现在的企业开发中，很少会使用物理外键，都是使用逻辑外键。 甚至在一些数据库开发规范中，会明确指出禁止使用物理外键 foreign key\nMySQL-多表设计-一对一\u0026amp;多对多 一对一\n一对一关系表在实际开发中应用起来比较简单，通常是用来做单表的拆分，也就是将一张大表拆分成两张小表，将大表中的一些基础字段放在一张表当中，将其他的字段放在另外一张表当中，以此来提高数据的操作效率。\n一对一的应用场景： 用户表(基本信息+身份信息)\n基本信息：用户的ID、姓名、性别、手机号、学历 身份信息：民族、生日、身份证号、身份证签发机关，身份证的有效期(开始时间、结束时间) 如果在业务系统当中，对用户的基本信息查询频率特别的高，但是对于用户的身份信息查询频率很低，此时出于提高查询效率的考虑，我就可以将这张大表拆分成两张小表，第一张表存放的是用户的基本信息，而第二张表存放的就是用户的身份信息。他们两者之间一对一的关系，一个用户只能对应一个身份证，而一个身份证也只能关联一个用户。\n那么在数据库层面怎么去体现上述两者之间是一对一的关系呢？\n其实一对一我们可以看成一种特殊的一对多。一对多我们是怎么设计表关系的？是不是在多的一方添加外键。同样我们也可以通过外键来体现一对一之间的关系，我们只需要在任意一方来添加一个外键就可以了。\n一对一 ：在任意一方加入外键，关联另外一方的主键，并且设置外键为唯一的(UNIQUE)\nSQL脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 -- 用户基本信息表 create table tb_user( id int unsigned primary key auto_increment comment \u0026#39;ID\u0026#39;, name varchar(10) not null comment \u0026#39;姓名\u0026#39;, gender tinyint unsigned not null comment \u0026#39;性别, 1 男 2 女\u0026#39;, phone char(11) comment \u0026#39;手机号\u0026#39;, degree varchar(10) comment \u0026#39;学历\u0026#39; ) comment \u0026#39;用户基本信息表\u0026#39;; -- 测试数据 insert into tb_user values (1,\u0026#39;白眉鹰王\u0026#39;,1,\u0026#39;18812340001\u0026#39;,\u0026#39;初中\u0026#39;), (2,\u0026#39;青翼蝠王\u0026#39;,1,\u0026#39;18812340002\u0026#39;,\u0026#39;大专\u0026#39;), (3,\u0026#39;金毛狮王\u0026#39;,1,\u0026#39;18812340003\u0026#39;,\u0026#39;初中\u0026#39;), (4,\u0026#39;紫衫龙王\u0026#39;,2,\u0026#39;18812340004\u0026#39;,\u0026#39;硕士\u0026#39;); -- 用户身份信息表 create table tb_user_card( id int unsigned primary key auto_increment comment \u0026#39;ID\u0026#39;, nationality varchar(10) not null comment \u0026#39;民族\u0026#39;, birthday date not null comment \u0026#39;生日\u0026#39;, idcard char(18) not null comment \u0026#39;身份证号\u0026#39;, issued varchar(20) not null comment \u0026#39;签发机关\u0026#39;, expire_begin date not null comment \u0026#39;有效期限-开始\u0026#39;, expire_end date comment \u0026#39;有效期限-结束\u0026#39;, user_id int unsigned not null unique comment \u0026#39;用户ID\u0026#39;, constraint fk_user_id foreign key (user_id) references tb_user(id) ) comment \u0026#39;用户身份信息表\u0026#39;; -- 测试数据 insert into tb_user_card values (1,\u0026#39;汉\u0026#39;,\u0026#39;1960-11-06\u0026#39;,\u0026#39;100000100000100001\u0026#39;,\u0026#39;朝阳区公安局\u0026#39;,\u0026#39;2000-06-10\u0026#39;,null,1), (2,\u0026#39;汉\u0026#39;,\u0026#39;1971-11-06\u0026#39;,\u0026#39;100000100000100002\u0026#39;,\u0026#39;静安区公安局\u0026#39;,\u0026#39;2005-06-10\u0026#39;,\u0026#39;2025-06-10\u0026#39;,2), (3,\u0026#39;汉\u0026#39;,\u0026#39;1963-11-06\u0026#39;,\u0026#39;100000100000100003\u0026#39;,\u0026#39;昌平区公安局\u0026#39;,\u0026#39;2006-06-10\u0026#39;,null,3), (4,\u0026#39;回\u0026#39;,\u0026#39;1980-11-06\u0026#39;,\u0026#39;100000100000100004\u0026#39;,\u0026#39;海淀区公安局\u0026#39;,\u0026#39;2008-06-10\u0026#39;,\u0026#39;2028-06-10\u0026#39;,4); 多对多\n多对多的关系在开发中属于也比较常见的。比如：学生和老师的关系，一个学生可以有多个授课老师，一个授课老师也可以有多个学生。在比如：学生和课程的关系，一个学生可以选修多门课程，一个课程也可以供多个学生选修。\n案例：学生与课程的关系\n关系：一个学生可以选修多门课程，一门课程也可以供多个学生选择 实现关系：建立第三张中间表，中间表至少包含两个外键，分别关联两方主键 SQL脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 -- 学生表 create table tb_student( id int auto_increment primary key comment \u0026#39;主键ID\u0026#39;, name varchar(10) comment \u0026#39;姓名\u0026#39;, no varchar(10) comment \u0026#39;学号\u0026#39; ) comment \u0026#39;学生表\u0026#39;; -- 学生表测试数据 insert into tb_student(name, no) values (\u0026#39;黛绮丝\u0026#39;, \u0026#39;2000100101\u0026#39;),(\u0026#39;谢逊\u0026#39;, \u0026#39;2000100102\u0026#39;),(\u0026#39;殷天正\u0026#39;, \u0026#39;2000100103\u0026#39;),(\u0026#39;韦一笑\u0026#39;, \u0026#39;2000100104\u0026#39;); -- 课程表 create table tb_course( id int auto_increment primary key comment \u0026#39;主键ID\u0026#39;, name varchar(10) comment \u0026#39;课程名称\u0026#39; ) comment \u0026#39;课程表\u0026#39;; -- 课程表测试数据 insert into tb_course (name) values (\u0026#39;Java\u0026#39;), (\u0026#39;PHP\u0026#39;), (\u0026#39;MySQL\u0026#39;) , (\u0026#39;Hadoop\u0026#39;); -- 学生课程表（中间表） create table tb_student_course( id int auto_increment comment \u0026#39;主键\u0026#39; primary key, student_id int not null comment \u0026#39;学生ID\u0026#39;, course_id int not null comment \u0026#39;课程ID\u0026#39;, constraint fk_courseid foreign key (course_id) references tb_course (id), constraint fk_studentid foreign key (student_id) references tb_student (id) )comment \u0026#39;学生课程中间表\u0026#39;; -- 学生课程表测试数据 insert into tb_student_course(student_id, course_id) values (1,1),(1,2),(1,3),(2,2),(2,3),(3,4); MySQL-多表设计-案例-关系分析 案例： 参考页面原型及需求，设计合理的表结构\n步骤\n阅读页面原型及需求文档，分析各个模块涉及到的表结构，及表结构之间的关系。 根据页面原型及需求文档，分析各个表结构中具体的字段及约束。 分析\n页面原型-分类管理 分类的信息：分类名称、分类类型[菜品/套餐]、分类排序、分类状态[禁用/启用]、分类的操作时间(修改时间)。\n页面原型-菜品管理 菜品的信息：菜品名称、菜品图片、菜品分类、菜品售价、菜品售卖状态、菜品的操作时间(修改时间)。\n思考：分类与菜品之间是什么关系？\n思考逻辑：一个分类下可以有多个菜品吗？反过来再想一想，一个菜品会对应多个分类吗？ 答案：一对多关系。一个分类下会有多个菜品，而一个菜品只能归属一个分类。\n设计表原则：在多的一方，添加字段，关联属于一这方的主键。\n页面原型-套餐管理 套餐的信息：套餐名称、套餐图片、套餐分类、套餐价格、套餐售卖状态、套餐的操作时间。\n思考：套餐与菜品之间是什么关系？\n思考逻辑：一个套餐下可以有多个菜品吗？反过来再想一想，一个菜品可以出现在多个套餐中吗？ 答案：多对多关系。一个套餐下会有多个菜品，而一个菜品也可以出现在多个套餐中。\n设计表原则：创建第三张中间表，建立两个字段分别关联菜品表的主键和套餐表的主键。\n分析页面原型及需求文档后，我们获得：\n分类表 业务字段：分类名称、分类类型、分类排序、分类状态 基础字段：id(主键)、分类的创建时间、分类的修改时间 菜品表 业务字段：菜品名称、菜品图片、菜品分类、菜品售价、菜品售卖状态 基础字段：id(主键)、分类的创建时间、分类的修改时间 套餐表 业务字段：套餐名称、套餐图片、套餐分类、套餐价格、套餐售卖状态 基础字段：id(主键)、分类的创建时间、分类的修改时间 表结构之间的关系：\n分类表 - 菜品表 ： 一对多 在菜品表中添加字段(菜品分类)，关联分类表 菜品表 - 套餐表 ： 多对多 创建第三张中间表(套餐菜品关联表)，在中间表上添加两个字段(菜品id、套餐id)，分别关联菜品表和分类表 MySQL-多表设计-案例-表结构 表结构\n分类表：category\n业务字段：分类名称、分类类型、分类排序、分类状态 基础字段：id(主键)、创建时间、修改时间 1 2 3 4 5 6 7 8 9 10 11 -- 分类表 create table category ( id int unsigned primary key auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(20) not null unique comment \u0026#39;分类名称\u0026#39;, type tinyint unsigned not null comment \u0026#39;类型 1 菜品分类 2 套餐分类\u0026#39;, sort tinyint unsigned not null comment \u0026#39;顺序\u0026#39;, status tinyint unsigned not null default 0 comment \u0026#39;状态 0 禁用，1 启用\u0026#39;, create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;更新时间\u0026#39; ) comment \u0026#39;菜品及套餐分类\u0026#39;; 菜品表：dish\n业务字段：菜品名称、菜品图片、菜品分类、菜品售价、菜品售卖状态 基础字段：id(主键)、分类的创建时间、分类的修改时间 1 2 3 4 5 6 7 8 9 10 11 12 13 -- 菜品表 create table dish ( id int unsigned primary key auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(20) not null unique comment \u0026#39;菜品名称\u0026#39;, category_id int unsigned not null comment \u0026#39;菜品分类ID\u0026#39;, -- 逻辑外键 price decimal(8, 2) not null comment \u0026#39;菜品价格\u0026#39;, image varchar(300) not null comment \u0026#39;菜品图片\u0026#39;, description varchar(200) comment \u0026#39;描述信息\u0026#39;, status tinyint unsigned not null default 0 comment \u0026#39;状态, 0 停售 1 起售\u0026#39;, create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;更新时间\u0026#39; ) comment \u0026#39;菜品\u0026#39;; 套餐表：setmeal\n业务字段：套餐名称、套餐图片、套餐分类、套餐价格、套餐售卖状态 基础字段：id(主键)、分类的创建时间、分类的修改时间 1 2 3 4 5 6 7 8 9 10 11 12 13 -- 套餐表 create table setmeal ( id int unsigned primary key auto_increment comment \u0026#39;主键ID\u0026#39;, name varchar(20) not null unique comment \u0026#39;套餐名称\u0026#39;, category_id int unsigned not null comment \u0026#39;分类id\u0026#39;, -- 逻辑外键 price decimal(8, 2) not null comment \u0026#39;套餐价格\u0026#39;, image varchar(300) not null comment \u0026#39;图片\u0026#39;, description varchar(200) comment \u0026#39;描述信息\u0026#39;, status tinyint unsigned not null default 0 comment \u0026#39;状态 0:停用 1:启用\u0026#39;, create_time datetime not null comment \u0026#39;创建时间\u0026#39;, update_time datetime not null comment \u0026#39;更新时间\u0026#39; ) comment \u0026#39;套餐\u0026#39;; 套餐菜品关联表：setmeal_dish\n1 2 3 4 5 6 7 8 -- 套餐菜品关联表 create table setmeal_dish ( id int unsigned primary key auto_increment comment \u0026#39;主键ID\u0026#39;, setmeal_id int unsigned not null comment \u0026#39;套餐id \u0026#39;, -- 逻辑外键 dish_id int unsigned not null comment \u0026#39;菜品id\u0026#39;, -- 逻辑外键 copies tinyint unsigned not null comment \u0026#39;份数\u0026#39; ) comment \u0026#39;套餐菜品关联表\u0026#39;; 小结\n","date":"2024-04-16T16:01:23+08:00","image":"https://nova-bryan.github.io/p/mysql-dql/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/mysql-dql/","title":"Mysql DQL"},{"content":"苍穹外卖学习文档 软件开发整体介绍 软件开发流程 需求分析 需求规格说明书、产品原型\n设计 UI设计、数据库设计、接口设计\n编码 项目代码、单元测试\n测试 测试用例、测试报告\n上线运维 软件环境安装、配置\n角色分工 项目经理\n对整体项目负责，任务分配、把控进度\n产品经理\n进行需求调研。输出需求调研文档、产品原型等\nUI设计师\n根据产品模型输出界面效果图\n架构师\n项目整体架构设计、技术选型等\n开发工程师\n代码实现\n测试工程师\n编写测试用例，输出测试报告\n运维工程师\n软件环境搭建、项目上线\n软件环境 开发环境 开发人员在开发阶段使用的环境，一般外部用户无法访问\n测试环境 专门给测试人员使用的环境，用于测试项目，一般外部用户无法访问\n生产环境 即线上环境，正式提供对外服务的环境\n苍穹外卖项目介绍 项目介绍 定位：专门为餐饮企业定制的一款软件产品\n功能架构：\n产品原型 用于展示项目的业务功能\n技术选型 展示项目中使用到的技术框架和中间件等\n开发环境搭建 前端环境搭建 整体结构 通过Nginx代理\n后端环境搭建 熟悉项目结构 sky-common子模块 constant：常量类 context：项目上下文相关 enumeration：枚举类 exception：自定义异常类 json：处理json转换 properties：springboot配置属性类，把配置文件中的配置项封装成对象 result：后端返回的结果 utils：工具类 sky-pojo子模块 sky-server子模块 存放的是 配置文件、配置类、拦截器、controller、service、mapper、启动类等\n使用Git进行版本控制 创建Git本地仓库 创建Git远程仓库 将本地文件推送到Git远程仓库 数据库环境搭建 前后端联调 Nginx🆕 反向代理，就是让前端发送的动态请求由Nginx转发到后端服务器\nNginx反向代理的好处\n提高访问速度\n进行负载均衡\n就是把大量的请求按照我们指定的方式均衡的分配给集群中的每台服务器\n保证后端服务安全\nNginx反向代理的配置方式 Nginx负载均衡的配置方式 Nginx负载均衡策略 轮询：平均接收到请求\n完善登录功能 修改数据库中的明文密码，改为MD5加密后的密文 修改Java代码，前端提交的密码进行MD5加密后再跟数据库中密码比对 导入接口文档 前后端分离开发流程 操作步骤 这里YApi可换成ApiPost，导入数据选择YApi即可\nSwagger 介绍 使用方式 导入knife4j的maven坐标\n在配置类中加入knife4j相关配置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /** * 通过knife4j生成接口文档 * @return */ @Bean public Docket docket() { ApiInfo apiInfo = new ApiInfoBuilder() .title(\u0026#34;苍穹外卖项目接口文档\u0026#34;) .version(\u0026#34;2.0\u0026#34;) .description(\u0026#34;苍穹外卖项目接口文档\u0026#34;) .build(); Docket docket = new Docket(DocumentationType.SWAGGER_2) .apiInfo(apiInfo) .select() //指定生成接口需要扫描的包 .apis(RequestHandlerSelectors.basePackage(\u0026#34;com.sky.controller\u0026#34;)) .paths(PathSelectors.any()) .build(); return docket; } 设置静态资源映射，否则接口文档页面无法访问\n1 2 3 4 5 6 7 8 9 /** * 设置静态资源映射 * @param registry */ protected void addResourceHandlers(ResourceHandlerRegistry registry) { log.info(\u0026#34;开始设置静态资源映射...\u0026#34;); registry.addResourceHandler(\u0026#34;/doc.html\u0026#34;).addResourceLocations(\u0026#34;classpath:/META-INF/resources/\u0026#34;); registry.addResourceHandler(\u0026#34;/webjars/**\u0026#34;).addResourceLocations(\u0026#34;classpath:/META-INF/resources/webjars/\u0026#34;); } 常用注解 通过注解可以控制生成的接口文档，使接口文档拥有更好的可读性，常用注解如下：\n员工管理、分类管理 员工管理界面\n分类管理界面\n新增员工 需求分析和设计 产品原型 接口设计 本项目约定：\n管理端发出的请求，统一使用/admin作为前缀 用户端发出的请求，统一使用/user作为前缀 数据库设计 employee表为员工表，用于存储商家内部的员工信息。具体表结构如下：\n字段名 数据类型 说明 备注 id bigint 主键 自增⭐ name varchar(32) 姓名 username varchar(32) 用户名 唯一⭐ password varchar(64) 密码 phone varchar(11) 手机号 sex varchar(2) 性别 id_number varchar(18) 身份证号 status int 账号状态 1正常 0锁定⭐ create_time datetime 创建时间 update_time datetime 最后修改时间 create_user bigint 创建人id update_user bigint 最后修改人id 代码开发 根据新增员工接口设计对应的DTO\n注意：当前端提交的数据和实体类中对应的属性差别比较大时，建议使用DTO来封装数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /** * 新增员工 * @param employeeDTO */ public void save(EmployeeDTO employeeDTO) { Employee employee = new Employee(); //对象属性拷贝，属性名必须一致 BeanUtils.copyProperties(employeeDTO, employee); //设置账号的状态，默认正常状态，1表示正常，0表示锁定 employee.setStatus(StatusConstant.ENABLE); //设置密码，默认密码为123456 employee.setPassword(DigestUtils.md5DigestAsHex(PasswordConstant.DEFAULT_PASSWORD.getBytes())); //设置当前记录的创建时间和修改时间 employee.setCreateTime(LocalDateTime.now()); employee.setUpdateTime(LocalDateTime.now()); //设置当前记录的创建人id和修改人id // TODO 后期需要改为当前登录的用户id employee.setCreateUser(10L); employee.setUpdateUser(10L); employeeMapper.insert(employee); } 功能测试 通过Swagger接口文档测试 通过前后端联调测试 注意：由于开发阶段前端和后端是并行开发的，后端完成某个功能后，此时前端对应的功能可能还没有开发完成，导致无法进行前后端联调测试。所以在开发阶段，后端测试主要以接口文档测试为主。\n代码完善 程序存在的问题：\n录入的用户名已存在，抛出异常后没有处理 新增员工时，创建人id和修改人id设置为了固定值 解析出登录员工id后，如何传递给Service的save方法？\n员工分页查询 需求分析和设计 产品原型\n业务规则：\n根据每页展示员工信息 每页展示10条数据 分页查询时可以根据需要，输入员工姓名进行查询 接口设计\n代码开发 员工信息分页查询后端返回的对象类型为：Result\u0026lt;PageResult\u0026gt;\nmybatis提供的分页查询框架pagehelper\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.pagehelper\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pagehelper-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${pagehelper}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 功能测试 可以通过接口文档进行测试，也可以进行前后端联调测试。\n代码完善 最后操作时间需要修改成年月日\n解决方式：\n方式一：在属性上加入注解，对日期进行格式化\n1 2 @JsonFormat(pattern = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;) private LocalDateTime updateTime 方式二：在Webconfiguration中扩展SpringMvc的消息转换器，统一对日期类型进行格式化处理\n启用禁用员工账号 需求分析和设计 产品原型\n业务规则：\n可以对状态为“启用”的员工账号进行“禁用”操作 可以对状态为“禁用”的员工账号进行“启用”操作 状态为“禁用”的员工账号不能登录系统 接口设计\n代码开发 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 /** * 启用/禁用员工账号 * @param status * @param id * @return */ @PostMapping(\u0026#34;status/{status}\u0026#34;) @ApiOperation(\u0026#34;启用/禁用员工账号\u0026#34;) public Result startOrStop(@PathVariable(\u0026#34;status\u0026#34;) Integer status, Long id) { log.info(\u0026#34;启用/禁用员工账号：{}, {}\u0026#34;, status, id); employeeService.startOrStop(status, id); return Result.success(); } /** * 启用/禁用员工账号 * @param status * @param id */ public void startOrStop(Integer status, Long id) { // update employee set status = ? where id = ? /*Employee employee = new Employee(); employee.setStatus(status); employee.setId(id);*/ Employee employee = Employee.builder() .status(status) .id(id) .build(); employeeMapper.update(employee); } \u0026lt;update id=\u0026#34;update\u0026#34; parameterType=\u0026#34;Employee\u0026#34;\u0026gt; update employee \u0026lt;set\u0026gt; \u0026lt;if test=\u0026#34;name != null\u0026#34;\u0026gt;name = #{name},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;username != null\u0026#34;\u0026gt;username = #{username},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;password != null\u0026#34;\u0026gt;password = #{password},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;phone != null\u0026#34;\u0026gt;phone = #{phone},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;sex != null\u0026#34;\u0026gt;sex = #{sex},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;idNumber != null\u0026#34;\u0026gt;id_Number = #{idNumber},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;updateTime != null\u0026#34;\u0026gt;update_Time = #{updateTime},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;updateUser != null\u0026#34;\u0026gt;update_User = #{updateUser},\u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;status != null\u0026#34;\u0026gt;status = #{status},\u0026lt;/if\u0026gt; \u0026lt;/set\u0026gt; where id = #{id} \u0026lt;/update\u0026gt; 功能测试 编辑员工 需求分析和设计 产品原型\n编辑员工功能涉及到两个接口：\n根据id查询员工信息 编辑员工信息 代码开发 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /** * 编辑员工信息 * @param employeeDTO * @return */ @PutMapping @ApiOperation(\u0026#34;编辑员工信息\u0026#34;) public Result update(@RequestBody EmployeeDTO employeeDTO) { log.info(\u0026#34;编辑员工信息: {}\u0026#34;, employeeDTO); employeeService.update(employeeDTO); return Result.success(); } /** * 编辑员工信息 * @param employeeDTO */ public void update(EmployeeDTO employeeDTO) { Employee employee = new Employee(); BeanUtils.copyProperties(employeeDTO, employee); employee.setUpdateTime(LocalDateTime.now()); employee.setUpdateUser(BaseContext.getCurrentId()); employeeMapper.update(employee); } 功能测试 导入分类模块功能代码 需求分析和设计 产品原型\n业务规则：\n分类名称必须是唯一的 分类按照类型可以分为菜品分类和套餐分类 新添加的分类状态默认为禁用 接口设计：\n新增分类 分类分页查询 根据id删除分类 修改分类 启用禁用分类 根据类型查询分类 数据库设计（category表）：\ncategory表为分类表，用于存储商品的分类信息。具体表结构如下\n字段名 数据类型 说明 备注 id bigint 主键 自增 name varchar(32) 分类名称 唯一 type int 分类类型 1菜品分类 2套餐分类 sort int 排序字段 用于分类数据的排序 status int 状态 1启用 0禁用 create_time datetime 创建时间 update_time datetime 最后修改时间 create_user bigint 创建人id update_user bigint 最后修改人id 代码导入 功能测试 菜品管理 公共字段自动填充🌟 问题分析 业务表中的公共字段：\n问题：代码冗余、不便于后期维护\n实现思路 自定义注解AutoFill，用于标识需要进行公共字段自动填充的方法 自定义切面类AutoFillAspect，统一拦截加入了AutoFill注解的方法，通过反射为公共字段赋值 在Mapper的方法上加入AutoFill注解 代码开发 功能测试 新增菜品 需求分析和设计 产品原型：\n业务规则：\n菜品名称必须是唯一的 菜品必须属于某个分类下，不能单独存在 新增菜品时可以根据情况选择菜品的口味 每个菜品必须对应一张图片 接口设计：\n根据类型查询分类（已完成）\n文件上传\n新增菜品\n数据库设计（dish菜品表和dish_flavor口味表）：\ndish表为菜品表，用于存储菜品的信息。具体表结构如下\n字段名 数据类型 说明 备注 id bigint 主键 自增 name varchar(32) 菜品名称 唯一 category_id bigint 分类id 逻辑外键 price decimal(10,2) 菜品价格 image varchar(255) 图片路径 description varchar(255) 菜品描述 status int 售卖状态 1起售 0停售 create_time datetime 创建时间 update_time datetime 最后修改时间 create_user bigint 创建人id update_user bigint 最后修改人id dish_flavor表为菜品口味表，用于存储菜品的口味信息。具体表结构如下\n字段名 数据类型 说明 备注 id bigint 主键 自增 dish_id bigint 菜品id 逻辑外键 name varchar(32) 口味名称 value varchar(255) 口味值 代码开发 开发文件上传接口：\n功能测试 菜品分页查询 需求分析和设计 产品原型\n业务规则\n根据页码展示菜品信息 每页展示10条数据 分页查询时可以根据需要输入菜品名称、菜品分类、菜品状态进行查询 接口设计\n代码开发 根据菜品分页查询接口定义设计对应的DTO：\n根据菜品分页查询接口定义设计对应的VO：\n功能测试 删除菜品 需求分析和设计 产品原型\n业务规则：\n可以一次删除一个菜品，也可以批量删除菜品 起售中的菜品不能删除 被套餐关联的菜品不能删除 删除菜品后，关联的口味数据也需要删除掉 接口设计：\n数据库设计：\n代码开发 功能测试 修改菜品 需求分析和设计 产品原型\n接口设计：\n根据id查询商品\n根据类型查询分类（已实现）\n文件上传（已实现）\n修改商品\n代码开发 功能测试 店铺营业状态设置 Redis入门 Redis简介 Redis是一个基于内存的key-value结构数据库。\n基于内存存储，读写性能高 适合存储热点数据（热点商品、资讯、新闻） 企业应用广泛 官网：https://redis.io/\n中文网：https://www.redis.net.cn/\nRedis下载与安装 Redis服务启动与停止 Redis数据类型 5种常用数据类型介绍 Redis存储的是key-value结构的数据，其中key是字符串类型，value有5种常用的数据类型：\n字符串 string\n普通字符串，Redis中最简单的数据类型\n哈希 hash\n也叫散列，类似于Java中的HashMap结构\n列表 list\n按照插入顺序排序，可以有重复元素，类似于Java中的LinkedList\n集合 set\n无序集合，没有重复元素，类似于Java中的HashSet\n有序集合 sorted set/zset\n集合中每个元素关联一个分数（score），Redis根据分数升序排序，没有重复元素\n各种数据类型的特点 Redis常用命令 字符串操作命令 SET key value 设置指定key的值 GET key 获取指定key的值 SETEX key seconds value 设置指定key的值，并将key的过期时间设为seconds秒 \u0026mdash;\u0026gt; 短信验证码 SETNX key value 只有在key不存在时设置key的值 哈希操作命令 Redis hash 是一个String类型的 field 和 value 的映射表，hash特别适合用于存储对象，常用命令：\nHSET key field value 将哈希表 key 中的字段 field 的值为 value HGET key field 获取存储在哈希表中指定字段的值 HDEL key field 删除存储在哈希表中的指定字段 HKEYS key 获取哈希表中所有字段 HVALS key 获取哈希表中所有值 列表操作命令 Redis列表是简单的字符串列表，按照插入顺序排序，常用命令：\nLPUSH key value1 [value2] 将一个或多个值插入到列表头部 LRANGE key start stop 获取列表指定范围内的元素 RPOP key 移除并获取列表最后一个元素 LLEN key 获取列表长度 集合操作命令 Redis set 是 String 类型的无序集合。集合成员是唯一的，集合中不能出现重复的数据，常用命令：\nSADD key member1 [member2] 向集合添加一个或多个成员 SMEMBERS key 返回集合中所有的成员 SCARD key 获取集合的成员数 SINTER key1 [key2] 返回给定的所有集合的交集 SUNION key1 [key2] 返回所有给定集合的并集 SREM key member1 [member2] 删除集合中一个或多个成员 有序集合操作命令 Redis的有序集合是String类型元素的集合，且不允许有重复成员。每个元素都会关联一个double类型的分数。常用命令：\nZADD key score1 member1 [score2 member2] 向有序集合添加一个或多个成员 ZRANGE key start stop [WITHSCORES] 通过索引区间返回有序集合中指定区间内的成员 ZINCRBY key increment member 有序集合中对指定成员的分数加上增量increment ZREM key member [member\u0026hellip; ] 移除有序集合中的一个或多个成员 通用命令 Redis的通用命令是不分数据类型的，都可以使用的命令：\nKEYS pattern 查找所有符合给定模式（pattern）的key EXISTS key 检查给定key是否存在 TYPE key 返回key所存储的值的类型 DEL key 该命令用于在key存在时删除key 在Java中操作Redis Redis的Java客户端 Redis的Java客户端很多，常用的几种：\nJedis\nLettuce\nSpring Data Redis\n是Spring的一部分，对Redis底层开发包进行了高度封装。在Spring项目中，可以使用Spring Data Redis来简化操作。\nSpring Data Redis使用方式 操作步骤：\n导入Spring Data Redis 的Maven坐标\n配置Redis数据源\n编写配置类，创建RedisTemplate对象\n通过RedisTemplate对象操作Redis\n店铺营业状态 需求分析和设计 产品原型\n接口设计：\n设置营业状态 管理端查询营业状态 用户端查询营业状态 本项目约定：\n管理端发出的请求，统一使用/admin作为前缀 用户端发出的请求，统一使用/user作为前缀 营业状态数据存储方式：基于Redis的字符串来进行存储\n代码开发 功能测试 微信登录、商品浏览 HttpClient🆕 介绍 HttpClient 是Apache Jakarta Common下的子项目，可以用来提供高效的、最新的、功能丰富的支持 HTTP 协议的客户端编程工具包，并且它支持 HTTP 协议最新的版本和建议。\n核心API：\nHttpClient HttpClients CloseableHttpClient HttpGet HttpPost 发送请求步骤：\n创建HttpClient对象 创建Http请求对象 \u0026mdash;\u0026gt; HttpGet/HttpPost 调用HttpClient的execute方法发送请求 入门案例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 /** * 测试通过HttpClient发送GET方式请求 */ @Test public void testGET() throws Exception{ //创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建请求对象 HttpGet httpGet = new HttpGet(\u0026#34;http://localhost:8080/user/shop/status\u0026#34;); //发送请求，并接受响应结果 CloseableHttpResponse response = httpClient.execute(httpGet); //获取服务端返回的状态码 int statusCode = response.getStatusLine().getStatusCode(); System.out.println(\u0026#34;服务端返回的状态码为：\u0026#34; + statusCode); HttpEntity entity = response.getEntity(); String body = EntityUtils.toString(entity); System.out.println(\u0026#34;服务端返回的数据为：\u0026#34; + body); //关闭资源 response.close(); httpClient.close(); } /** * 测试通过HttpClient发送POST请求 */ @Test public void testPost() throws Exception { //创建Httpclient对象 CloseableHttpClient httpClient = HttpClients.createDefault(); //创建请求对象 HttpPost httpPost = new HttpPost(\u0026#34;http://localhost:8080/admin/employee/login\u0026#34;); JSONObject jsonObject = new JSONObject(); jsonObject.put(\u0026#34;username\u0026#34;, \u0026#34;admin\u0026#34;); jsonObject.put(\u0026#34;password\u0026#34;, \u0026#34;123456\u0026#34;); StringEntity entity = new StringEntity(jsonObject.toString()); //指定请求的编码方式 entity.setContentEncoding(\u0026#34;utf-8\u0026#34;); //数据格式 entity.setContentType(\u0026#34;application/json\u0026#34;); httpPost.setEntity(entity); //发送请求 CloseableHttpResponse response = httpClient.execute(httpPost); //解析返回结果 int statusCode = response.getStatusLine().getStatusCode(); System.out.println(\u0026#34;响应码为：\u0026#34; + statusCode); HttpEntity entity1 = response.getEntity(); String body = EntityUtils.toString(entity1); System.out.println(\u0026#34;响应数据为：\u0026#34; + body); //关闭资源 response.close(); httpClient.close(); } 微信小程序开发 介绍 准备工作 入门案例 操作步骤：\n了解微信小程序目录结构\n小程序包含一个描述整体程序的 app 和多个描述各自页面的 page。一个小程序主体部分由三个文件组成，必须放在项目的根目录，如下：\n一个小程序页面由四个文件组成：\n编写小程序代码\n编译小程序\n微信登录 导入小程序代码 微信登录流程 官网：https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/login.html\n需求分析和设计 产品原型：\n业务规则：\n基于微信登录实现小程序的登录功能 如果是新用户需要自动完成注册 接口设计：\n数据库设计（user表）：\n代码开发 配置微信登录所需配置项：\n配置为微信用户生成jwt令牌时使用的配置项：\n功能测试 导入商品浏览功能代码 需求分析和设计 产品原型：\n接口设计：\n查询分类\n根据分类id查询菜品\n根据分类id查询套餐\n根据套餐id查询包含的菜品\n代码导入 功能测试 缓存商品、购物车 缓存菜品 问题说明 用户端小程序展示的菜品数据都是通过查询数据库获得，如果用户端访问量比较大，数据库访问压力随之增大。\n实现思路 通过Redis来缓存菜品数据，减少数据库查询操作。\n缓存逻辑分析：\n每个分类下的菜品保存一份缓存数据\n数据库中菜品数据有变更时清理缓存数据\n代码开发 修改管理端接口 DishController 的相关方法，加入清理缓存的逻辑，需要改造的方法:\n新增菜品 修改菜品 批量删除菜品 起售、停售菜品 功能测试 缓存套餐 Spring Cache⭐ Spring Cache 是一个框架，实现了基于注解的缓存功能，只需要简单地加一个注解，就能实现缓存功能。\nSpring Cache 提供了一层抽象，底层可以切换不同的缓存实现，例如:\nEHCache Caffeine Redis 常用注解：\n在启动类上添加@EnableCaching注解\n1 2 3 4 5 6 7 8 9 @Slf4j @SpringBootApplication @EnableCaching //开启缓存注解功能 public class CacheDemoApplication { public static void main(String[] args) { SpringApplication.run(CacheDemoApplication.class,args); log.info(\u0026#34;项目启动成功...\u0026#34;); } } 在controller上使用@Cacheable、@Cacheput、@CacheEvict注解\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 @RestController @RequestMapping(\u0026#34;/user\u0026#34;) @Slf4j public class UserController { @Autowired private UserMapper userMapper; @PostMapping @CachePut(cacheNames = \u0026#34;userCache\u0026#34;, key = \u0026#34;#user.id\u0026#34;) //如果使用SpringCache缓存数据，key的生成：userCache::2 public User save(@RequestBody User user){ userMapper.insert(user); return user; } @DeleteMapping @CacheEvict(cacheNames = \u0026#34;userCache\u0026#34;, key = \u0026#34;#id\u0026#34;) public void deleteById(Long id){ userMapper.deleteById(id); } @DeleteMapping(\u0026#34;/delAll\u0026#34;) @CacheEvict(cacheNames = \u0026#34;userCache\u0026#34;, allEntries = true) //删除userCache下的所有缓存 public void deleteAll(){ userMapper.deleteAll(); } @GetMapping @Cacheable(cacheNames = \u0026#34;userCache\u0026#34;, key = \u0026#34;#id\u0026#34;) //key的生成：userCache::10 public User getById(Long id){ User user = userMapper.getById(id); return user; } } 实现思路 具体的实现思路如下：\n导入SpringCache和Redis相关Maven坐标 在启动类上加入@EnableCache注解，开启缓存注解功能 在用户端接口SetmealController的list方法上加入@Cacheable注解 在管理端接口SetmealController的save、delete、update、startOrStop等方法上加入@CacheEvict注解 代码开发 功能测试 添加购物车 需求分析和设计 产品原型：\n接口设计：\n请求方式：POST 请求路径：/user/shoppingCart/add 请求参数：套餐id、菜品id、口味 返回结果：code、data、msg 数据库设计：\n作用：暂时存放所选商品的地方 选的什么商品 每个商品买了几个 不同用户的购物车需要区分开 代码开发 功能测试 查看购物车 需求分析和设计 产品原型：\n接口设计：\n代码开发 功能测试 清空购物车 需求分析和设计 产品原型：\n接口设计：\n功能测试 代码开发 用户下单、订单支付 导入地址簿功能代码 需求分析和设计 产品原型：\n业务功能：\n查询地址列表 新增地址 修改地址 删除地址 设置默认地址 查询默认地址 接口设计：\n新增地址 查询当前登录用户的所有地址信息 查询默认地址 根据id删除地址 根据id修改地址 根据id查询地址 设置默认地址 数据库设计（address_book表）：\naddress_book表为地址表，用于存储C端用户的收货地址信息。具体表结构如下：\n字段名 数据类型 说明 备注 id bigint 主键 自增 user_id bigint 用户id 逻辑外键 consignee varchar(50) 收货人 sex varchar(2) 性别 phone varchar(11) 手机号 province_code varchar(12) 省份编码 province_name varchar(32) 省份名称 city_code varchar(12) 城市编码 city_name varchar(32) 城市名称 district_code varchar(12) 区县编码 district_name varchar(32) 区县名称 detail varchar(200) 详细地址信息 具体到门牌号 label varchar(100) 标签 公司、家、学校 is_default tinyint(1) 是否默认地址 1是 0否 代码导入 功能测试 用户下单 需求分析和设计 用户下单业务说明：\n在电商系统中，用户是通过下单的方式通知商家，用户已经购买了商品，需要商家进行备货和发货。\n用户下单后会产生订单相关数据，订单数据需要能够体现如下信息：\n用户点餐业务流程：\n接口设计（分析）：\n接口设计：\n数据库（orders表、order_deatail表）设计：\norders表为订单表，用于存储C端用户的订单数据。具体表结构如下：\n字段名 数据类型 说明 备注 id bigint 主键 自增 number varchar(50) 订单号 status int 订单状态 1待付款 2待接单 3已接单 4派送中 5已完成 6已取消 user_id bigint 用户id 逻辑外键 address_book_id bigint 地址id 逻辑外键 order_time datetime 下单时间 checkout_time datetime 付款时间 pay_method int 支付方式 1微信支付 2支付宝支付 pay_status tinyint 支付状态 0未支付 1已支付 2退款 amount decimal(10,2) 订单金额 remark varchar(100) 备注信息 phone varchar(11) 手机号 address varchar(255) 详细地址信息 user_name varchar(32) 用户姓名 consignee varchar(32) 收货人 cancel_reason varchar(255) 订单取消原因 rejection_reason varchar(255) 拒单原因 cancel_time datetime 订单取消时间 estimated_delivery_time datetime 预计送达时间 delivery_status tinyint 配送状态 1立即送出 0选择具体时间 delivery_time datetime 送达时间 pack_amount int 打包费 tableware_number int 餐具数量 tableware_status tinyint 餐具数量状态 1按餐量提供 0选择具体数量 order_detail表为订单明细表，用于存储C端用户的订单明细数据。具体表结构如下：\n字段名 数据类型 说明 备注 id bigint 主键 自增 name varchar(32) 商品名称 image varchar(255) 商品图片路径 order_id bigint 订单id 逻辑外键 dish_id bigint 菜品id 逻辑外键 setmeal_id bigint 套餐id 逻辑外键 dish_flavor varchar(50) 菜品口味 number int 商品数量 amount decimal(10,2) 商品单价 代码开发 根据用户下单接口的参数设计DTO：\n根据用户下单接口的返回结果设计VO：\n功能测试 订单支付 微信支付介绍 微信支付产品：\n参考：https://pay.weixin.qq.com/static/product/product_index.shtm\n微信支付接入流程：\n微信小程序支付时序图：\nJSAPI下单：商户系统调用该接口在微信支付服务后台生成预支付交易单\n微信小程序调起支付：通过JSAPI下单接口获取到发起支付的必要参数prepay_id，然后使用微信支付提供的小程序方法调起小程序支付\n微信支付准备工作 微信小程序支付时序图：\n获取微信支付平台证书、商户私钥文件\n获取临时域名：支付成功后微信服务通过该域名回调我们的程序\n代码导入 微信支付相关配置：\n功能测试 用户端历史订单模块 1. 查询历史订单 1.1 需求分析和设计 产品原型：\n业务规则\n分页查询历史订单 可以根据订单状态查询 展示订单数据时，需要展示的数据包括：下单时间、订单状态、订单金额、订单明细（商品名称、图片） 接口设计：参见接口文档\n1.2 代码实现 1.2.1 user/OrderController 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /** * 历史订单查询 * * @param page * @param pageSize * @param status 订单状态 1待付款 2待接单 3已接单 4派送中 5已完成 6已取消 * @return */ @GetMapping(\u0026#34;/historyOrders\u0026#34;) @ApiOperation(\u0026#34;历史订单查询\u0026#34;) public Result\u0026lt;PageResult\u0026gt; page(int page, int pageSize, Integer status) { PageResult pageResult = orderService.pageQuery4User(page, pageSize, status); return Result.success(pageResult); } 1.2.2 OrderService 1 2 3 4 5 6 7 8 /** * 用户端订单分页查询 * @param page * @param pageSize * @param status * @return */ PageResult pageQuery4User(int page, int pageSize, Integer status); 1.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 /** * 用户端订单分页查询 * * @param pageNum * @param pageSize * @param status * @return */ public PageResult pageQuery4User(int pageNum, int pageSize, Integer status) { // 设置分页 PageHelper.startPage(pageNum, pageSize); OrdersPageQueryDTO ordersPageQueryDTO = new OrdersPageQueryDTO(); ordersPageQueryDTO.setUserId(BaseContext.getCurrentId()); ordersPageQueryDTO.setStatus(status); // 分页条件查询 Page\u0026lt;Orders\u0026gt; page = orderMapper.pageQuery(ordersPageQueryDTO); List\u0026lt;OrderVO\u0026gt; list = new ArrayList(); // 查询出订单明细，并封装入OrderVO进行响应 if (page != null \u0026amp;\u0026amp; page.getTotal() \u0026gt; 0) { for (Orders orders : page) { Long orderId = orders.getId();// 订单id // 查询订单明细 List\u0026lt;OrderDetail\u0026gt; orderDetails = orderDetailMapper.getByOrderId(orderId); OrderVO orderVO = new OrderVO(); BeanUtils.copyProperties(orders, orderVO); orderVO.setOrderDetailList(orderDetails); list.add(orderVO); } } return new PageResult(page.getTotal(), list); } 1.2.4 OrderMapper 1 2 3 4 5 /** * 分页条件查询并按下单时间排序 * @param ordersPageQueryDTO */ Page\u0026lt;Orders\u0026gt; pageQuery(OrdersPageQueryDTO ordersPageQueryDTO); 1.2.5 OrderMapper.xml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 \u0026lt;select id=\u0026#34;pageQuery\u0026#34; resultType=\u0026#34;Orders\u0026#34;\u0026gt; select * from orders \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;number != null and number!=\u0026#39;\u0026#39;\u0026#34;\u0026gt; and number like concat(\u0026#39;%\u0026#39;,#{number},\u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;phone != null and phone!=\u0026#39;\u0026#39;\u0026#34;\u0026gt; and phone like concat(\u0026#39;%\u0026#39;,#{phone},\u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;userId != null\u0026#34;\u0026gt; and user_id = #{userId} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;status != null\u0026#34;\u0026gt; and status = #{status} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;beginTime != null\u0026#34;\u0026gt; and order_time \u0026amp;gt;= #{beginTime} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;endTime != null\u0026#34;\u0026gt; and order_time \u0026amp;lt;= #{endTime} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; order by order_time desc \u0026lt;/select\u0026gt; 1.2.6 OrderDetailMapper 1 2 3 4 5 6 7 /** * 根据订单id查询订单明细 * @param orderId * @return */ @Select(\u0026#34;select * from order_detail where order_id = #{orderId}\u0026#34;) List\u0026lt;OrderDetail\u0026gt; getByOrderId(Long orderId); 1.3 功能测试 略\n2. 查询订单详情 2.1 需求分析和设计 产品原型：\n接口设计：参见接口文档\n2.2 代码实现 2.2.1 user/OrderController 1 2 3 4 5 6 7 8 9 10 11 12 /** * 查询订单详情 * * @param id * @return */ @GetMapping(\u0026#34;/orderDetail/{id}\u0026#34;) @ApiOperation(\u0026#34;查询订单详情\u0026#34;) public Result\u0026lt;OrderVO\u0026gt; details(@PathVariable(\u0026#34;id\u0026#34;) Long id) { OrderVO orderVO = orderService.details(id); return Result.success(orderVO); } 2.2.2 OrderService 1 2 3 4 5 6 /** * 查询订单详情 * @param id * @return */ OrderVO details(Long id); 2.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /** * 查询订单详情 * * @param id * @return */ public OrderVO details(Long id) { // 根据id查询订单 Orders orders = orderMapper.getById(id); // 查询该订单对应的菜品/套餐明细 List\u0026lt;OrderDetail\u0026gt; orderDetailList = orderDetailMapper.getByOrderId(orders.getId()); // 将该订单及其详情封装到OrderVO并返回 OrderVO orderVO = new OrderVO(); BeanUtils.copyProperties(orders, orderVO); orderVO.setOrderDetailList(orderDetailList); return orderVO; } 2.2.4 OrderMapper 1 2 3 4 5 6 /** * 根据id查询订单 * @param id */ @Select(\u0026#34;select * from orders where id=#{id}\u0026#34;) Orders getById(Long id); 2.3 功能测试 略\n3. 取消订单 3.1 需求分析和设计 产品原型：\n业务规则：\n待支付和待接单状态下，用户可直接取消订单 商家已接单状态下，用户取消订单需电话沟通商家 派送中状态下，用户取消订单需电话沟通商家 如果在待接单状态下取消订单，需要给用户退款 取消订单后需要将订单状态修改为“已取消” 接口设计：参见接口文档\n3.2 代码实现 3.2.1 user/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 用户取消订单 * * @return */ @PutMapping(\u0026#34;/cancel/{id}\u0026#34;) @ApiOperation(\u0026#34;取消订单\u0026#34;) public Result cancel(@PathVariable(\u0026#34;id\u0026#34;) Long id) throws Exception { orderService.userCancelById(id); return Result.success(); } 3.2.2 OrderService 1 2 3 4 5 /** * 用户取消订单 * @param id */ void userCancelById(Long id) throws Exception; 3.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 /** * 用户取消订单 * * @param id */ public void userCancelById(Long id) throws Exception { // 根据id查询订单 Orders ordersDB = orderMapper.getById(id); // 校验订单是否存在 if (ordersDB == null) { throw new OrderBusinessException(MessageConstant.ORDER_NOT_FOUND); } //订单状态 1待付款 2待接单 3已接单 4派送中 5已完成 6已取消 if (ordersDB.getStatus() \u0026gt; 2) { throw new OrderBusinessException(MessageConstant.ORDER_STATUS_ERROR); } Orders orders = new Orders(); orders.setId(ordersDB.getId()); // 订单处于待接单状态下取消，需要进行退款 if (ordersDB.getStatus().equals(Orders.TO_BE_CONFIRMED)) { //调用微信支付退款接口 weChatPayUtil.refund( ordersDB.getNumber(), //商户订单号 ordersDB.getNumber(), //商户退款单号 new BigDecimal(0.01),//退款金额，单位 元 new BigDecimal(0.01));//原订单金额 //支付状态修改为 退款 orders.setPayStatus(Orders.REFUND); } // 更新订单状态、取消原因、取消时间 orders.setStatus(Orders.CANCELLED); orders.setCancelReason(\u0026#34;用户取消\u0026#34;); orders.setCancelTime(LocalDateTime.now()); orderMapper.update(orders); } 3.3 功能测试 略\n4. 再来一单 4.1 需求分析和设计 产品原型：\n接口设计：参见接口文档\n业务规则：\n再来一单就是将原订单中的商品重新加入到购物车中 4.2 代码实现 4.2.1 user/OrderController 1 2 3 4 5 6 7 8 9 10 11 12 /** * 再来一单 * * @param id * @return */ @PostMapping(\u0026#34;/repetition/{id}\u0026#34;) @ApiOperation(\u0026#34;再来一单\u0026#34;) public Result repetition(@PathVariable Long id) { orderService.repetition(id); return Result.success(); } 4.2.2 OrderService 1 2 3 4 5 6 /** * 再来一单 * * @param id */ void repetition(Long id); 4.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /** * 再来一单 * * @param id */ public void repetition(Long id) { // 查询当前用户id Long userId = BaseContext.getCurrentId(); // 根据订单id查询当前订单详情 List\u0026lt;OrderDetail\u0026gt; orderDetailList = orderDetailMapper.getByOrderId(id); // 将订单详情对象转换为购物车对象 List\u0026lt;ShoppingCart\u0026gt; shoppingCartList = orderDetailList.stream().map(x -\u0026gt; { ShoppingCart shoppingCart = new ShoppingCart(); // 将原订单详情里面的菜品信息重新复制到购物车对象中 BeanUtils.copyProperties(x, shoppingCart, \u0026#34;id\u0026#34;); shoppingCart.setUserId(userId); shoppingCart.setCreateTime(LocalDateTime.now()); return shoppingCart; }).collect(Collectors.toList()); // 将购物车对象批量添加到数据库 shoppingCartMapper.insertBatch(shoppingCartList); } 4.2.4 ShoppingCartMapper 1 2 3 4 5 6 /** * 批量插入购物车数据 * * @param shoppingCartList */ void insertBatch(List\u0026lt;ShoppingCart\u0026gt; shoppingCartList); 4.2.5 ShoppingCartMapper.xml 1 2 3 4 5 6 7 8 \u0026lt;insert id=\u0026#34;insertBatch\u0026#34; parameterType=\u0026#34;list\u0026#34;\u0026gt; insert into shopping_cart (name, image, user_id, dish_id, setmeal_id, dish_flavor, number, amount, create_time) values \u0026lt;foreach collection=\u0026#34;shoppingCartList\u0026#34; item=\u0026#34;sc\u0026#34; separator=\u0026#34;,\u0026#34;\u0026gt; (#{sc.name},#{sc.image},#{sc.userId},#{sc.dishId},#{sc.setmealId},#{sc.dishFlavor},#{sc.number},#{sc.amount},#{sc.createTime}) \u0026lt;/foreach\u0026gt; \u0026lt;/insert\u0026gt; 4.3 功能测试 略\n商家端订单管理模块 1. 订单搜索 1.1 需求分析和设计 产品原型：\n业务规则：\n输入订单号/手机号进行搜索，支持模糊搜索 根据订单状态进行筛选 下单时间进行时间筛选 搜索内容为空，提示未找到相关订单 搜索结果页，展示包含搜索关键词的内容 分页展示搜索到的订单数据 接口设计：参见接口文档\n1.2 代码实现 1.2.1 admin/OrderController 在admin包下创建OrderController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /** * 订单管理 */ @RestController(\u0026#34;adminOrderController\u0026#34;) @RequestMapping(\u0026#34;/admin/order\u0026#34;) @Slf4j @Api(tags = \u0026#34;订单管理接口\u0026#34;) public class OrderController { @Autowired private OrderService orderService; /** * 订单搜索 * * @param ordersPageQueryDTO * @return */ @GetMapping(\u0026#34;/conditionSearch\u0026#34;) @ApiOperation(\u0026#34;订单搜索\u0026#34;) public Result\u0026lt;PageResult\u0026gt; conditionSearch(OrdersPageQueryDTO ordersPageQueryDTO) { PageResult pageResult = orderService.conditionSearch(ordersPageQueryDTO); return Result.success(pageResult); } } 1.2.2 OrderService 1 2 3 4 5 6 /** * 条件搜索订单 * @param ordersPageQueryDTO * @return */ PageResult conditionSearch(OrdersPageQueryDTO ordersPageQueryDTO); 1.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 /** * 订单搜索 * * @param ordersPageQueryDTO * @return */ public PageResult conditionSearch(OrdersPageQueryDTO ordersPageQueryDTO) { PageHelper.startPage(ordersPageQueryDTO.getPage(), ordersPageQueryDTO.getPageSize()); Page\u0026lt;Orders\u0026gt; page = orderMapper.pageQuery(ordersPageQueryDTO); // 部分订单状态，需要额外返回订单菜品信息，将Orders转化为OrderVO List\u0026lt;OrderVO\u0026gt; orderVOList = getOrderVOList(page); return new PageResult(page.getTotal(), orderVOList); } private List\u0026lt;OrderVO\u0026gt; getOrderVOList(Page\u0026lt;Orders\u0026gt; page) { // 需要返回订单菜品信息，自定义OrderVO响应结果 List\u0026lt;OrderVO\u0026gt; orderVOList = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Orders\u0026gt; ordersList = page.getResult(); if (!CollectionUtils.isEmpty(ordersList)) { for (Orders orders : ordersList) { // 将共同字段复制到OrderVO OrderVO orderVO = new OrderVO(); BeanUtils.copyProperties(orders, orderVO); String orderDishes = getOrderDishesStr(orders); // 将订单菜品信息封装到orderVO中，并添加到orderVOList orderVO.setOrderDishes(orderDishes); orderVOList.add(orderVO); } } return orderVOList; } /** * 根据订单id获取菜品信息字符串 * * @param orders * @return */ private String getOrderDishesStr(Orders orders) { // 查询订单菜品详情信息（订单中的菜品和数量） List\u0026lt;OrderDetail\u0026gt; orderDetailList = orderDetailMapper.getByOrderId(orders.getId()); // 将每一条订单菜品信息拼接为字符串（格式：宫保鸡丁*3；） List\u0026lt;String\u0026gt; orderDishList = orderDetailList.stream().map(x -\u0026gt; { String orderDish = x.getName() + \u0026#34;*\u0026#34; + x.getNumber() + \u0026#34;;\u0026#34;; return orderDish; }).collect(Collectors.toList()); // 将该订单对应的所有菜品信息拼接在一起 return String.join(\u0026#34;\u0026#34;, orderDishList); } 1.3 功能测试 略\n2. 各个状态的订单数量统计 2.1 需求分析和设计 产品原型：\n接口设计：参见接口文档\n2.2 代码实现 2.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 各个状态的订单数量统计 * * @return */ @GetMapping(\u0026#34;/statistics\u0026#34;) @ApiOperation(\u0026#34;各个状态的订单数量统计\u0026#34;) public Result\u0026lt;OrderStatisticsVO\u0026gt; statistics() { OrderStatisticsVO orderStatisticsVO = orderService.statistics(); return Result.success(orderStatisticsVO); } 2.2.2 OrderService 1 2 3 4 5 /** * 各个状态的订单数量统计 * @return */ OrderStatisticsVO statistics(); 2.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 /** * 各个状态的订单数量统计 * * @return */ public OrderStatisticsVO statistics() { // 根据状态，分别查询出待接单、待派送、派送中的订单数量 Integer toBeConfirmed = orderMapper.countStatus(Orders.TO_BE_CONFIRMED); Integer confirmed = orderMapper.countStatus(Orders.CONFIRMED); Integer deliveryInProgress = orderMapper.countStatus(Orders.DELIVERY_IN_PROGRESS); // 将查询出的数据封装到orderStatisticsVO中响应 OrderStatisticsVO orderStatisticsVO = new OrderStatisticsVO(); orderStatisticsVO.setToBeConfirmed(toBeConfirmed); orderStatisticsVO.setConfirmed(confirmed); orderStatisticsVO.setDeliveryInProgress(deliveryInProgress); return orderStatisticsVO; } 2.2.4 OrderMapper 1 2 3 4 5 6 /** * 根据状态统计订单数量 * @param status */ @Select(\u0026#34;select count(id) from orders where status = #{status}\u0026#34;) Integer countStatus(Integer status); 2.3 功能测试 略\n3. 查询订单详情 3.1 需求分析和设计 产品原型：\n业务规则：\n订单详情页面需要展示订单基本信息（状态、订单号、下单时间、收货人、电话、收货地址、金额等） 订单详情页面需要展示订单明细数据（商品名称、数量、单价） 接口设计：参见接口文档\n3.2 代码实现 3.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 12 /** * 订单详情 * * @param id * @return */ @GetMapping(\u0026#34;/details/{id}\u0026#34;) @ApiOperation(\u0026#34;查询订单详情\u0026#34;) public Result\u0026lt;OrderVO\u0026gt; details(@PathVariable(\u0026#34;id\u0026#34;) Long id) { OrderVO orderVO = orderService.details(id); return Result.success(orderVO); } 3.3 功能测试 略\n4. 接单 4.1 需求分析和设计 产品原型：\n业务规则：\n商家接单其实就是将订单的状态修改为“已接单” 接口设计：参见接口文档\n4.2 代码实现 4.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 接单 * * @return */ @PutMapping(\u0026#34;/confirm\u0026#34;) @ApiOperation(\u0026#34;接单\u0026#34;) public Result confirm(@RequestBody OrdersConfirmDTO ordersConfirmDTO) { orderService.confirm(ordersConfirmDTO); return Result.success(); } 4.2.2 OrderService 1 2 3 4 5 6 /** * 接单 * * @param ordersConfirmDTO */ void confirm(OrdersConfirmDTO ordersConfirmDTO); 4.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 /** * 接单 * * @param ordersConfirmDTO */ public void confirm(OrdersConfirmDTO ordersConfirmDTO) { Orders orders = Orders.builder() .id(ordersConfirmDTO.getId()) .status(Orders.CONFIRMED) .build(); orderMapper.update(orders); } 4.3 功能测试 略\n5. 拒单 5.1 需求分析和设计 产品原型：\n业务规则：\n商家拒单其实就是将订单状态修改为“已取消” 只有订单处于“待接单”状态时可以执行拒单操作 商家拒单时需要指定拒单原因 商家拒单时，如果用户已经完成了支付，需要为用户退款 接口设计：参见接口文档\n5.2 代码实现 5.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 拒单 * * @return */ @PutMapping(\u0026#34;/rejection\u0026#34;) @ApiOperation(\u0026#34;拒单\u0026#34;) public Result rejection(@RequestBody OrdersRejectionDTO ordersRejectionDTO) throws Exception { orderService.rejection(ordersRejectionDTO); return Result.success(); } 5.2.2 OrderService 1 2 3 4 5 6 /** * 拒单 * * @param ordersRejectionDTO */ void rejection(OrdersRejectionDTO ordersRejectionDTO) throws Exception; 5.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 /** * 拒单 * * @param ordersRejectionDTO */ public void rejection(OrdersRejectionDTO ordersRejectionDTO) throws Exception { // 根据id查询订单 Orders ordersDB = orderMapper.getById(ordersRejectionDTO.getId()); // 订单只有存在且状态为2（待接单）才可以拒单 if (ordersDB == null || !ordersDB.getStatus().equals(Orders.TO_BE_CONFIRMED)) { throw new OrderBusinessException(MessageConstant.ORDER_STATUS_ERROR); } //支付状态 Integer payStatus = ordersDB.getPayStatus(); if (payStatus == Orders.PAID) { //用户已支付，需要退款 String refund = weChatPayUtil.refund( ordersDB.getNumber(), ordersDB.getNumber(), new BigDecimal(0.01), new BigDecimal(0.01)); log.info(\u0026#34;申请退款：{}\u0026#34;, refund); } // 拒单需要退款，根据订单id更新订单状态、拒单原因、取消时间 Orders orders = new Orders(); orders.setId(ordersDB.getId()); orders.setStatus(Orders.CANCELLED); orders.setRejectionReason(ordersRejectionDTO.getRejectionReason()); orders.setCancelTime(LocalDateTime.now()); orderMapper.update(orders); } 5.3 功能测试 略\n6. 取消订单 6.1 需求分析和设计 产品原型：\n业务规则：\n取消订单其实就是将订单状态修改为“已取消” 商家取消订单时需要指定取消原因 商家取消订单时，如果用户已经完成了支付，需要为用户退款 接口设计：参见接口文档\n6.2 代码实现 6.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 取消订单 * * @return */ @PutMapping(\u0026#34;/cancel\u0026#34;) @ApiOperation(\u0026#34;取消订单\u0026#34;) public Result cancel(@RequestBody OrdersCancelDTO ordersCancelDTO) throws Exception { orderService.cancel(ordersCancelDTO); return Result.success(); } 6.2.2 OrderService 1 2 3 4 5 6 /** * 商家取消订单 * * @param ordersCancelDTO */ void cancel(OrdersCancelDTO ordersCancelDTO) throws Exception; 6.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 /** * 取消订单 * * @param ordersCancelDTO */ public void cancel(OrdersCancelDTO ordersCancelDTO) throws Exception { // 根据id查询订单 Orders ordersDB = orderMapper.getById(ordersCancelDTO.getId()); //支付状态 Integer payStatus = ordersDB.getPayStatus(); if (payStatus == 1) { //用户已支付，需要退款 String refund = weChatPayUtil.refund( ordersDB.getNumber(), ordersDB.getNumber(), new BigDecimal(0.01), new BigDecimal(0.01)); log.info(\u0026#34;申请退款：{}\u0026#34;, refund); } // 管理端取消订单需要退款，根据订单id更新订单状态、取消原因、取消时间 Orders orders = new Orders(); orders.setId(ordersCancelDTO.getId()); orders.setStatus(Orders.CANCELLED); orders.setCancelReason(ordersCancelDTO.getCancelReason()); orders.setCancelTime(LocalDateTime.now()); orderMapper.update(orders); } 6.3 功能测试 略\n7. 派送订单 7.1 需求分析和设计 产品原型：\n业务规则：\n派送订单其实就是将订单状态修改为“派送中” 只有状态为“待派送”的订单可以执行派送订单操作 接口设计：参见接口文档\n7.2 代码实现 7.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 派送订单 * * @return */ @PutMapping(\u0026#34;/delivery/{id}\u0026#34;) @ApiOperation(\u0026#34;派送订单\u0026#34;) public Result delivery(@PathVariable(\u0026#34;id\u0026#34;) Long id) { orderService.delivery(id); return Result.success(); } 7.2.2 OrderService 1 2 3 4 5 6 /** * 派送订单 * * @param id */ void delivery(Long id); 7.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /** * 派送订单 * * @param id */ public void delivery(Long id) { // 根据id查询订单 Orders ordersDB = orderMapper.getById(id); // 校验订单是否存在，并且状态为3 if (ordersDB == null || !ordersDB.getStatus().equals(Orders.CONFIRMED)) { throw new OrderBusinessException(MessageConstant.ORDER_STATUS_ERROR); } Orders orders = new Orders(); orders.setId(ordersDB.getId()); // 更新订单状态,状态转为派送中 orders.setStatus(Orders.DELIVERY_IN_PROGRESS); orderMapper.update(orders); } 7.3 功能测试 略\n8. 完成订单 8.1 需求分析和设计 产品原型：\n业务规则：\n完成订单其实就是将订单状态修改为“已完成” 只有状态为“派送中”的订单可以执行订单完成操作 接口设计：参见接口文档\n8.2 代码实现 8.2.1 admin/OrderController 1 2 3 4 5 6 7 8 9 10 11 /** * 完成订单 * * @return */ @PutMapping(\u0026#34;/complete/{id}\u0026#34;) @ApiOperation(\u0026#34;完成订单\u0026#34;) public Result complete(@PathVariable(\u0026#34;id\u0026#34;) Long id) { orderService.complete(id); return Result.success(); } 8.2.2 OrderService 1 2 3 4 5 6 /** * 完成订单 * * @param id */ void complete(Long id); 8.2.3 OrderServiceImpl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /** * 完成订单 * * @param id */ public void complete(Long id) { // 根据id查询订单 Orders ordersDB = orderMapper.getById(id); // 校验订单是否存在，并且状态为4 if (ordersDB == null || !ordersDB.getStatus().equals(Orders.DELIVERY_IN_PROGRESS)) { throw new OrderBusinessException(MessageConstant.ORDER_STATUS_ERROR); } Orders orders = new Orders(); orders.setId(ordersDB.getId()); // 更新订单状态,状态转为完成 orders.setStatus(Orders.COMPLETED); orders.setDeliveryTime(LocalDateTime.now()); orderMapper.update(orders); } 8.3 功能测试 略\n校验收货地址是否超出配送范围 1. 环境准备 注册账号：https://passport.baidu.com/v2/?reg\u0026amp;tt=1671699340600\u0026amp;overseas=\u0026amp;gid=CF954C2-A3D2-417F-9FE6-B0F249ED7E33\u0026amp;tpl=pp\u0026amp;u=https%3A%2F%2Flbsyun.baidu.com%2Findex.php%3Ftitle%3D首页\n登录百度地图开放平台：https://lbsyun.baidu.com/\n进入控制台，创建应用，获取AK：\n相关接口:\nhttps://lbsyun.baidu.com/index.php?title=webapi/guide/webservice-geocoding\nhttps://lbsyun.baidu.com/index.php?title=webapi/directionlite-v1\n2. 代码开发 2.1 application.yml 配置外卖商家店铺地址和百度地图的AK：\n2.2 OrderServiceImpl 改造OrderServiceImpl，注入上面的配置项：\n1 2 3 4 5 @Value(\u0026#34;${sky.shop.address}\u0026#34;) private String shopAddress; @Value(\u0026#34;${sky.baidu.ak}\u0026#34;) private String ak; 在OrderServiceImpl中提供校验方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 /** * 检查客户的收货地址是否超出配送范围 * @param address */ private void checkOutOfRange(String address) { Map map = new HashMap(); map.put(\u0026#34;address\u0026#34;,shopAddress); map.put(\u0026#34;output\u0026#34;,\u0026#34;json\u0026#34;); map.put(\u0026#34;ak\u0026#34;,ak); //获取店铺的经纬度坐标 String shopCoordinate = HttpClientUtil.doGet(\u0026#34;https://api.map.baidu.com/geocoding/v3\u0026#34;, map); JSONObject jsonObject = JSON.parseObject(shopCoordinate); if(!jsonObject.getString(\u0026#34;status\u0026#34;).equals(\u0026#34;0\u0026#34;)){ throw new OrderBusinessException(\u0026#34;店铺地址解析失败\u0026#34;); } //数据解析 JSONObject location = jsonObject.getJSONObject(\u0026#34;result\u0026#34;).getJSONObject(\u0026#34;location\u0026#34;); String lat = location.getString(\u0026#34;lat\u0026#34;); String lng = location.getString(\u0026#34;lng\u0026#34;); //店铺经纬度坐标 String shopLngLat = lat + \u0026#34;,\u0026#34; + lng; map.put(\u0026#34;address\u0026#34;,address); //获取用户收货地址的经纬度坐标 String userCoordinate = HttpClientUtil.doGet(\u0026#34;https://api.map.baidu.com/geocoding/v3\u0026#34;, map); jsonObject = JSON.parseObject(userCoordinate); if(!jsonObject.getString(\u0026#34;status\u0026#34;).equals(\u0026#34;0\u0026#34;)){ throw new OrderBusinessException(\u0026#34;收货地址解析失败\u0026#34;); } //数据解析 location = jsonObject.getJSONObject(\u0026#34;result\u0026#34;).getJSONObject(\u0026#34;location\u0026#34;); lat = location.getString(\u0026#34;lat\u0026#34;); lng = location.getString(\u0026#34;lng\u0026#34;); //用户收货地址经纬度坐标 String userLngLat = lat + \u0026#34;,\u0026#34; + lng; map.put(\u0026#34;origin\u0026#34;,shopLngLat); map.put(\u0026#34;destination\u0026#34;,userLngLat); map.put(\u0026#34;steps_info\u0026#34;,\u0026#34;0\u0026#34;); //路线规划 String json = HttpClientUtil.doGet(\u0026#34;https://api.map.baidu.com/directionlite/v1/driving\u0026#34;, map); jsonObject = JSON.parseObject(json); if(!jsonObject.getString(\u0026#34;status\u0026#34;).equals(\u0026#34;0\u0026#34;)){ throw new OrderBusinessException(\u0026#34;配送路线规划失败\u0026#34;); } //数据解析 JSONObject result = jsonObject.getJSONObject(\u0026#34;result\u0026#34;); JSONArray jsonArray = (JSONArray) result.get(\u0026#34;routes\u0026#34;); Integer distance = (Integer) ((JSONObject) jsonArray.get(0)).get(\u0026#34;distance\u0026#34;); if(distance \u0026gt; 5000){ //配送距离超过5000米 throw new OrderBusinessException(\u0026#34;超出配送范围\u0026#34;); } } 在OrderServiceImpl的submitOrder方法中调用上面的校验方法：\n订单状态定时处理、来单提醒和客户催单 Spring Task Spring Task 是Spring框架提供的任务调度工具，可以按照约定的时间自动执行某个代码逻辑。\n定位：定时任务框架\n作用：定时自动执行某段Java代码\n介绍 应用场景：\n信用卡每月还卡提醒 银行贷款每月还款提醒 火车票售票系统处理未支付订单 入职纪念日为用户发送通知 cron表达式 cron表达式其实就是一个字符串，通过cron表达式可以定义任务触发的时间\n构成规则：分为6或7个域，由空格分隔开，每个域代表一个含义\n每个域的含义分别为：秒、分钟、小时、日、月、周、年(可选) 周 和 日互斥，只能选择其一，另外一个选择？\n2022年10月12日上午9点整 对应的cron表达式： 0 0 9 12 10 ？2022\n入门案例 Spring Task使用步骤：\n导入Maven坐标 spring-context 启动类添加注解@EnableScheduling开启任务调度 自定义定时任务类 订单状态定时处理 需求分析和设计 用户下单后可能存在的情况：\n下单未支付，订单一直处于“未支付”状态 用户收货后管理端未点击完成按钮，订单一直处于“派送中”状态 对于上面两种情况需要通过定时任务来修改订单状态，具体逻辑为：\n通过定时任务每分钟检查一次是否存在支付超时订单（下单后超过15分钟仍未支付则判定为支付超时订单），如果存在则修改订单状态为“已取消” 通过定时任务每天凌晨1点检查一次是否存在“派送中”的订单，如果存在则修改订单状态为“已完成” 代码开发 功能测试 WebSocket 介绍 WebSocket 是基于 TCP 的一种新的网络协议。它实现了浏览器与服务器全双工通信——浏览器和服务器只需要完成一次握手，两者之间就可以创建持久性的连接，并进行双向数据传输。\nHTTP协议和WebSocket协议对比：\nHTTP是短连接 WebSocket是长连接 HTTP通信是单向的，基于请求响应模式 WebSocket支持双向通信 HTTP和WebSocket底层都是TCP连接 应用场景\n视频弹幕 网页聊天 体育实况更新 股票基金报价实时更新 效果展示：\n入门案例 实现步骤：\n直接使用websocket.html页面作为WebSocket客户端 导入WebSocket的maven坐标 导入WebSocket服务端组建WebSocketServer，用于和客户端通信 导入配置类WebSocketConfiguration，注册WebSocket的服务端组件 导入定时任务类WebSocketTask，定时向客户端推送数据 来单提醒 需求分析和设计 用户下单并且支付成功后需要第一时间通知外卖商家。通知的形式有如下两种：\n语音播报 弹出提示框 设计：\n通过WebSocket实现管理端页面和服务端页面保持长连接状态 当客户支付后，调用WebSocket的相关API实现服务端向管理端推送消息 管理端浏览器解析服务端推送的消息，判断是来单提醒还是客户催单，进行相应的消息提示和语音播报 约定服务端发送给客户端浏览器的数据格式为JSON，字段包括：type、orderid、content type 为消息类型，1为来单提醒，2为客户催单 orderid 为订单id content 为消息内容 代码开发 功能测试 客户催单 需求分析和设计 用户在小程序中点击催单按钮后，需要第一时间通知外卖商家。通知的形式有如下两种：\n语音播报 弹出提示框 设计：\n通过WebSocket实现管理端页面和服务端页面保持长连接状态 当用户点击催单按钮后，调用WebSocket的相关API实现服务端向管理端推送消息 管理端浏览器解析服务端推送的消息，判断是来单提醒还是客户催单，进行相应的消息提示和语音播报 约定服务端发送给客户端浏览器的数据格式为JSON，字段包括：type、orderid、content type 为消息类型，1为来单提醒，2为客户催单 orderid 为订单id content 为消息内容 接口设计：\n代码开发 功能测试 数据统计-图形报表 Apache Echarts 介绍 Apache Echarts是一款基于Javascript 的数据可视化图表库，提供直观，生动，可交互，可个性化定制的数据可视化图表。\n官网地址：https://echarts.apache.org/zh/index.html\n效果展示：\n入门案例 总结:使用Echarts，重点在于研究当前图表所需的数据格式。通常是需要后端提供符合格式要求的动态数据，然后响应给前端来展示图表。\n营业额统计 需求分析和设计 产品原型：\n业务规则：\n营业额指订单状态为已完成的订单金额合计 基于可视化报表的折线图展示营业额数据，X轴为日期，Y轴为营业额 根据时间选择区间，展示每天的营业额数据 接口设计：\n代码开发 根据接口定义设计对应的VO：\n功能测试 用户统计 需求分析和设计 产品原型：\n业务规则：\n基于可视化报表的折线图展示用户数据，x轴为日期，y轴为用户数 根据时间选择区间，展示每天的用户总量和新增用户量数据 接口设计：\n代码开发 根据用户统计接口的返回结果设计VO：\n功能测试 订单统计 需求分析和设计 产品原型：\n业务规则：\n有效订单指状态为“已完成”的订单 基于可视化报表的折线图展示订单数据，X轴为日期，Y轴为订单数量 根据时间选择区间，展示每天的订单总数和有效订单数 展示所选时间区间内的有效订单数、总订单数、订单完成率，订单完成率=有效订单数/总订单数 * 100% 接口设计：\n代码开发 根据订单统计接口的返回结果设计VO：\n功能测试 销量排行Top10 需求分析和设计 产品原型：\n业务规则：\n根据时间选择区间，展示销量前10的商品（包括菜品和套餐） 基于可视化报表的柱状图降序展示商品销量 此处的销量为商品销售的份数 接口设计：\n代码开发 根据销量排名接口的返回结果设计VO：\n功能测试 数据统计-Excel报表 工作台 需求分析和设计 工作台是系统运营的数据看板，并提供快捷操作入口，可以有效提高商家的工作效率。\n工作台展示的数据：\n今日数据 订单管理 菜品总览 套餐总览 订单信息 名词解释：\n营业额：已完成订单的总金额 有效订单：已完成订单的数量 订单完成率：有效订单数、总订单数 * 100 平均客单价：营业额 / 有效订单数 新增用户：新增用户的数量 接口设计：\n今日数据接口 订单管理接口 菜品总览接口 套餐总览接口 订单搜索（已完成） 各个状态的订单数量统计（已完成） 代码导入 功能测试 Apache POI 介绍 Apache POl是一个处理Miscrosoft Office各种文件格式的开源项目。简单来说就是，我们可以使用 POl 在 Java 程序中对Miscrosoft Office各种文件进行读写操作。\n一般情况下，POI都是用于操作Excel文件。\nApache POI 的应用场景：\n银行网银系统导出交易明细 各种业务系统导出Excel报表 批量导入业务数据 入门案例 Apache POI的maven坐标：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public class POITest { /** * 通过POI创建Excel文件并且写入文件内容 */ public static void write() throws Exception{ //在内存中创建一个Excel文件 XSSFWorkbook excel = new XSSFWorkbook(); //在Excel文件中创建一个sheet页 XSSFSheet sheet = excel.createSheet(\u0026#34;info\u0026#34;); //在sheet页中创建行对象，rownum编号从0开始 XSSFRow row = sheet.createRow(1); //创建单元格，并且写入文件内容 row.createCell(1).setCellValue(\u0026#34;姓名\u0026#34;); row.createCell(2).setCellValue(\u0026#34;城市\u0026#34;); //创建一个新行 row = sheet.createRow(2); row.createCell(1).setCellValue(\u0026#34;张三\u0026#34;); row.createCell(2).setCellValue(\u0026#34;北京\u0026#34;); row = sheet.createRow(3); row.createCell(1).setCellValue(\u0026#34;李四\u0026#34;); row.createCell(2).setCellValue(\u0026#34;南京\u0026#34;); //通过输出流将内存中的Excel文件写入磁盘 FileOutputStream out = new FileOutputStream(new File(\u0026#34;D:\\\\Projects\\\\info.xlsx\u0026#34;)); excel.write(out); //关闭资源 out.close(); excel.close(); } public static void main(String[] args) throws Exception { write(); } } 导出运营数据Excel报表 需求分析和设计 产品原型：\n导出的Excel报表格式：\n业务规则：\n导出Excel形式的报表文件 导出最近30天的运营数据 接口设计：\n注意：当前接口没有返回数据，因为报表导出功能本质上是文件下载，服务端会通过输出流将Excel文件下载到客户端浏览器\n代码开发 实现步骤：\n设计Excel模板文件\n查询近30天的运营数据\n将查询到的运营数据写入模板文件\n通过输出流将Excel文件下载到客户端浏览器\n","date":"2024-04-13T18:46:11+08:00","image":"https://nova-bryan.github.io/p/%E8%8B%8D%E7%A9%B9%E5%A4%96%E5%8D%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/%E8%8B%8D%E7%A9%B9%E5%A4%96%E5%8D%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"苍穹外卖学习笔记"},{"content":"SpringBoot-配置优先级 在我们前面的课程当中，我们已经讲解了SpringBoot项目当中支持的三类配置文件：\napplication.properties application.yml application.yaml 在SpringBoot项目当中，我们要想配置一个属性，可以通过这三种方式当中的任意一种来配置都可以，那么如果项目中同时存在这三种配置文件，且都配置了同一个属性，如：Tomcat端口号，到底哪一份配置文件生效呢？\napplication.properties 1 server.port=8081 application.yml 1 2 server: port: 8082 application.yaml 1 2 server: port: 8082 我们启动SpringBoot程序，测试下三个配置文件中哪个Tomcat端口号生效：\nproperties、yaml、yml三种配置文件同时存在 properties、yaml、yml三种配置文件，优先级最高的是properties\nyaml、yml两种配置文件同时存在 配置文件优先级排名（从高到低）：\nproperties配置文件 yml配置文件 yaml配置文件 注意事项：虽然springboot支持多种格式配置文件，但是在项目开发时，推荐统一使用一种格式的配置。（yml是主流）\n在SpringBoot项目当中除了以上3种配置文件外，SpringBoot为了增强程序的扩展性，除了支持配置文件的配置方式以外，还支持另外两种常见的配置方式：\nJava系统属性配置 （格式： -Dkey=value）\n1 -Dserver.port=9000 命令行参数 （格式：\u0026ndash;key=value）\n1 --server.port=10010 那在idea当中运行程序时，如何来指定Java系统属性和命令行参数呢？\n编辑启动程序的配置信息 重启服务，同时配置Tomcat端口(三种配置文件、系统属性、命令行参数)，测试哪个Tomcat端口号生效：\n删除命令行参数配置，重启SpringBoot服务：\n优先级： 命令行参数 \u0026gt; 系统属性参数 \u0026gt; properties参数 \u0026gt; yml参数 \u0026gt; yaml参数\n思考：如果项目已经打包上线了，这个时候我们又如何来设置Java系统属性和命令行参数呢？\n1 java -Dserver.port=9000 -jar XXXXX.jar --server.port=10010 下面我们来演示下打包程序运行时指定Java系统属性和命令行参数：\n执行maven打包指令package，把项目打成jar文件 使用命令：java -jar 方式运行jar文件程序 项目打包：\n运行jar程序：\n同时设置Java系统属性和命令行参数 仅设置Java系统属性 注意事项：\nSpringboot项目进行打包时，需要引入插件 spring-boot-maven-plugin (基于官网骨架创建项目，会自动添加该插件) 在SpringBoot项目当中，常见的属性配置方式有5种， 3种配置文件，加上2种外部属性的配置(Java系统属性、命令行参数)。通过以上的测试，我们也得出了优先级(从低到高)：\napplication.yaml（忽略） application.yml application.properties java系统属性（-Dxxx=xxx） 命令行参数（\u0026ndash;xxx=xxx） bean的管理-bean的获取 在前面的课程当中，我们已经讲过了我们可以通过Spring当中提供的注解@Component以及它的三个衍生注解（@Controller、@Service、@Repository）来声明IOC容器中的bean对象，同时我们也学习了如何为应用程序注入运行时所需要依赖的bean对象，也就是依赖注入DI。\n我们今天主要学习IOC容器中Bean的其他使用细节，主要学习以下三方面：\n如何从IOC容器中手动的获取到bean对象 bean的作用域配置 管理第三方的bean对象 默认情况下，SpringBoot项目在启动的时候会自动的创建IOC容器(也称为Spring容器)，并且在启动的过程当中会自动的将bean对象都创建好，存放在IOC容器当中。应用程序在运行时需要依赖什么bean对象，就直接进行依赖注入就可以了。\n而在Spring容器中提供了一些方法，可以主动从IOC容器中获取到bean对象，下面介绍3种常用方式：\n根据name获取bean\n1 Object getBean(String name) 根据类型获取bean\n1 \u0026lt;T\u0026gt; T getBean(Class\u0026lt;T\u0026gt; requiredType) 根据name获取bean（带类型转换）\n1 \u0026lt;T\u0026gt; T getBean(String name, Class\u0026lt;T\u0026gt; requiredType) 思考：要从IOC容器当中来获取到bean对象，需要先拿到IOC容器对象，怎么样才能拿到IOC容器呢？\n想获取到IOC容器，直接将IOC容器对象注入进来就可以了 控制器：DeptController\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @RestController @RequestMapping(\u0026#34;/depts\u0026#34;) public class DeptController { @Autowired private DeptService deptService; public DeptController(){ System.out.println(\u0026#34;DeptController constructor ....\u0026#34;); } @GetMapping public Result list(){ List\u0026lt;Dept\u0026gt; deptList = deptService.list(); return Result.success(deptList); } @DeleteMapping(\u0026#34;/{id}\u0026#34;) public Result delete(@PathVariable Integer id) { deptService.delete(id); return Result.success(); } @PostMapping public Result save(@RequestBody Dept dept){ deptService.save(dept); return Result.success(); } } 业务实现类：DeptServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Slf4j @Service public class DeptServiceImpl implements DeptService { @Autowired private DeptMapper deptMapper; @Override public List\u0026lt;Dept\u0026gt; list() { List\u0026lt;Dept\u0026gt; deptList = deptMapper.list(); return deptList; } @Override public void delete(Integer id) { deptMapper.delete(id); } @Override public void save(Dept dept) { dept.setCreateTime(LocalDateTime.now()); dept.setUpdateTime(LocalDateTime.now()); deptMapper.save(dept); } } Mapper接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Mapper public interface DeptMapper { //查询全部部门数据 @Select(\u0026#34;select * from dept\u0026#34;) List\u0026lt;Dept\u0026gt; list(); //删除部门 @Delete(\u0026#34;delete from dept where id = #{id}\u0026#34;) void delete(Integer id); //新增部门 @Insert(\u0026#34;insert into dept(name, create_time, update_time) values (#{name},#{createTime},#{updateTime})\u0026#34;) void save(Dept dept); } 测试类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @SpringBootTest class SpringbootWebConfig2ApplicationTests { @Autowired private ApplicationContext applicationContext; //IOC容器对象 //获取bean对象 @Test public void testGetBean(){ //根据bean的名称获取 DeptController bean1 = (DeptController) applicationContext.getBean(\u0026#34;deptController\u0026#34;); System.out.println(bean1); //根据bean的类型获取 DeptController bean2 = applicationContext.getBean(DeptController.class); System.out.println(bean2); //根据bean的名称 及 类型获取 DeptController bean3 = applicationContext.getBean(\u0026#34;deptController\u0026#34;, DeptController.class); System.out.println(bean3); } } 程序运行后控制台日志：\n问题：输出的bean对象地址值是一样的，说明IOC容器当中的bean对象有几个？\n答案：只有一个。 （默认情况下，IOC中的bean对象是单例）\n那么能不能将bean对象设置为非单例的(每次获取的bean都是一个新对象)？\n可以，在下一个知识点(bean作用域)中讲解。\n注意事项：\n上述所说的 【Spring项目启动时，会把其中的bean都创建好】还会受到作用域及延迟初始化影响，这里主要针对于默认的单例非延迟加载的bean而言。 bean的管理-bean的作用域 在前面我们提到的IOC容器当中，默认bean对象是单例模式(只有一个实例对象)。那么如何设置bean对象为非单例呢？需要设置bean的作用域。\n在Spring中支持五种作用域，后三种在web环境才生效：\n作用域 说明 singleton 容器内同名称的bean只有一个实例（单例）（默认） prototype 每次使用该bean时会创建新的实例（非单例） request 每个请求范围内会创建新的实例（web环境中，了解） session 每个会话范围内会创建新的实例（web环境中，了解） application 每个应用范围内会创建新的实例（web环境中，了解） 知道了bean的5种作用域了，我们要怎么去设置一个bean的作用域呢？\n可以借助Spring中的@Scope注解来进行配置作用域 1). 测试一\n控制器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //默认bean的作用域为：singleton (单例) @Lazy //延迟加载（第一次使用bean对象时，才会创建bean对象并交给ioc容器管理） @RestController @RequestMapping(\u0026#34;/depts\u0026#34;) public class DeptController { @Autowired private DeptService deptService; public DeptController(){ System.out.println(\u0026#34;DeptController constructor ....\u0026#34;); } //省略其他代码... } 测试类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @SpringBootTest class SpringbootWebConfig2ApplicationTests { @Autowired private ApplicationContext applicationContext; //IOC容器对象 //bean的作用域 @Test public void testScope(){ for (int i = 0; i \u0026lt; 10; i++) { DeptController deptController = applicationContext.getBean(DeptController.class); System.out.println(deptController); } } } 重启SpringBoot服务，运行测试方法，查看控制台打印的日志：\n注意事项：\nIOC容器中的bean默认使用的作用域：singleton (单例) 默认singleton的bean，在容器启动时被创建，可以使用@Lazy注解来延迟初始化(延迟到第一次使用时) 2). 测试二\n修改控制器DeptController代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Scope(\u0026#34;prototype\u0026#34;) //bean作用域为非单例 @Lazy //延迟加载 @RestController @RequestMapping(\u0026#34;/depts\u0026#34;) public class DeptController { @Autowired private DeptService deptService; public DeptController(){ System.out.println(\u0026#34;DeptController constructor ....\u0026#34;); } //省略其他代码... } 重启SpringBoot服务，再次执行测试方法，查看控制吧打印的日志：\n注意事项：\nprototype的bean，每一次使用该bean的时候都会创建一个新的实例 实际开发当中，绝大部分的Bean是单例的，也就是说绝大部分Bean不需要配置scope属性 bean的管理-第三方bean 学习完bean的获取、bean的作用域之后，接下来我们再来学习第三方bean的配置。\n之前我们所配置的bean，像controller、service，dao三层体系下编写的类，这些类都是我们在项目当中自己定义的类(自定义类)。当我们要声明这些bean，也非常简单，我们只需要在类上加上@Component以及它的这三个衍生注解（@Controller、@Service、@Repository），就可以来声明这个bean对象了。 但是在我们项目开发当中，还有一种情况就是这个类它不是我们自己编写的，而是我们引入的第三方依赖当中提供的。\n在pom.xml文件中，引入dom4j：\n1 2 3 4 5 6 \u0026lt;!--Dom4j--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.dom4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dom4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; dom4j就是第三方组织提供的。 dom4j中的SAXReader类就是第三方编写的。\n当我们需要使用到SAXReader对象时，直接进行依赖注入是不是就可以了呢？\n按照我们之前的做法，需要在SAXReader类上添加一个注解@Component（将当前类交给IOC容器管理） 结论：第三方提供的类是只读的。无法在第三方类上添加@Component注解或衍生注解。\n那么我们应该怎样使用并定义第三方的bean呢？\n如果要管理的bean对象来自于第三方（不是自定义的），是无法用@Component 及衍生注解声明bean的，就需要用到**@Bean**注解。 解决方案1：在启动类上添加@Bean标识的方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 @SpringBootApplication public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } //声明第三方bean @Bean //将当前方法的返回值对象交给IOC容器管理, 成为IOC容器bean public SAXReader saxReader(){ return new SAXReader(); } } xml文件：\n1 2 3 4 5 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;emp\u0026gt; \u0026lt;name\u0026gt;Tom\u0026lt;/name\u0026gt; \u0026lt;age\u0026gt;18\u0026lt;/age\u0026gt; \u0026lt;/emp\u0026gt; 测试类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @SpringBootTest class SpringbootWebConfig2ApplicationTests { @Autowired private SAXReader saxReader; //第三方bean的管理 @Test public void testThirdBean() throws Exception { Document document = saxReader.read(this.getClass().getClassLoader().getResource(\u0026#34;1.xml\u0026#34;)); Element rootElement = document.getRootElement(); String name = rootElement.element(\u0026#34;name\u0026#34;).getText(); String age = rootElement.element(\u0026#34;age\u0026#34;).getText(); System.out.println(name + \u0026#34; : \u0026#34; + age); } //省略其他代码... } 重启SpringBoot服务，执行测试方法后，控制台输出日志：\n1 Tom : 18 说明：以上在启动类中声明第三方Bean的作法，不建议使用（项目中要保证启动类的纯粹性）\n解决方案2：在配置类中定义@Bean标识的方法\n如果需要定义第三方Bean时， 通常会单独定义一个配置类 1 2 3 4 5 6 7 8 9 10 11 12 @Configuration //配置类 (在配置类当中对第三方bean进行集中的配置管理) public class CommonConfig { //声明第三方bean @Bean //将当前方法的返回值对象交给IOC容器管理, 成为IOC容器bean //通过@Bean注解的name/value属性指定bean名称, 如果未指定, 默认是方法名 public SAXReader reader(DeptService deptService){ System.out.println(deptService); return new SAXReader(); } } 注释掉SpringBoot启动类中创建第三方bean对象的代码，重启服务，执行测试方法，查看控制台日志：\n1 Tom : 18 在方法上加上一个@Bean注解，Spring 容器在启动的时候，它会自动的调用这个方法，并将方法的返回值声明为Spring容器当中的Bean对象。\n注意事项 ：\n通过@Bean注解的name或value属性可以声明bean的名称，如果不指定，默认bean的名称就是方法名。 如果第三方bean需要依赖其它bean对象，直接在bean定义方法中设置形参即可，容器会根据类型自动装配。 关于Bean大家只需要保持一个原则：\n如果是在项目当中我们自己定义的类，想将这些类交给IOC容器管理，我们直接使用@Component以及它的衍生注解来声明就可以。 如果这个类它不是我们自己定义的，而是引入的第三方依赖当中提供的类，而且我们还想将这个类交给IOC容器管理。此时我们就需要在配置类中定义一个方法，在方法上加上一个@Bean注解，通过这种方式来声明第三方的bean对象。 SpringBoot原理-起步依赖 经过前面10多天课程的学习，大家也会发现基于SpringBoot进行web程序的开发是非常简单、非常高效的。\nSpringBoot使我们能够集中精力地去关注业务功能的开发，而不用过多地关注框架本身的配置使用。而我们前面所讲解的都是面向应用层面的技术，接下来我们开始学习SpringBoot的原理，这部分内容偏向于底层的原理分析。\n在剖析SpringBoot的原理之前，我们先来快速回顾一下我们前面所讲解的Spring家族的框架。\nSpring是目前世界上最流行的Java框架，它可以帮助我们更加快速、更加容易的来构建Java项目。而在Spring家族当中提供了很多优秀的框架，而所有的框架都是基于一个基础框架的SpringFramework(也就是Spring框架)。而前面我们也提到，如果我们直接基于Spring框架进行项目的开发，会比较繁琐。\n这个繁琐主要体现在两个地方：\n在pom.xml中依赖配置比较繁琐，在项目开发时，需要自己去找到对应的依赖，还需要找到依赖它所配套的依赖以及对应版本，否则就会出现版本冲突问题。 在使用Spring框架进行项目开发时，需要在Spring的配置文件中做大量的配置，这就造成Spring框架入门难度较大，学习成本较高。 基于Spring存在的问题，官方在Spring框架4.0版本之后，又推出了一个全新的框架：SpringBoot。\n通过 SpringBoot来简化Spring框架的开发(是简化不是替代)。我们直接基于SpringBoot来构建Java项目，会让我们的项目开发更加简单，更加快捷。\nSpringBoot框架之所以使用起来更简单更快捷，是因为SpringBoot框架底层提供了两个非常重要的功能：一个是起步依赖，一个是自动配置。\n通过SpringBoot所提供的起步依赖，就可以大大的简化pom文件当中依赖的配置，从而解决了Spring框架当中依赖配置繁琐的问题。\n通过自动配置的功能就可以大大的简化框架在使用时bean的声明以及bean的配置。我们只需要引入程序开发时所需要的起步依赖，项目开发时所用到常见的配置都已经有了，我们直接使用就可以了。\n简单回顾之后，接下来我们来学习下SpringBoot的原理。其实学习SpringBoot的原理就是来解析SpringBoot当中的起步依赖与自动配置的原理。我们首先来学习SpringBoot当中起步依赖的原理。\n起步依赖\n假如我们没有使用SpringBoot，用的是Spring框架进行web程序的开发，此时我们就需要引入web程序开发所需要的一些依赖。\nspring-webmvc依赖：这是Spring框架进行web程序开发所需要的依赖\nservlet-api依赖：Servlet基础依赖\njackson-databind依赖：JSON处理工具包\n如果要使用AOP，还需要引入aop依赖、aspect依赖\n项目中所引入的这些依赖，还需要保证版本匹配，否则就可能会出现版本冲突问题。\n如果我们使用了SpringBoot，就不需要像上面这么繁琐的引入依赖了。我们只需要引入一个依赖就可以了，那就是web开发的起步依赖：springboot-starter-web。\n为什么我们只需要引入一个web开发的起步依赖，web开发所需要的所有的依赖都有了呢？\n因为Maven的依赖传递。 在SpringBoot给我们提供的这些起步依赖当中，已提供了当前程序开发所需要的所有的常见依赖(官网地址：https://docs.spring.io/spring-boot/docs/2.7.7/reference/htmlsingle/#using.build-systems.starters)。 比如：springboot-starter-web，这是web开发的起步依赖，在web开发的起步依赖当中，就集成了web开发中常见的依赖：json、web、webmvc、tomcat等。我们只需要引入这一个起步依赖，其他的依赖都会自动的通过Maven的依赖传递进来。 结论：起步依赖的原理就是Maven的依赖传递。\nSpringBoot原理-自动配置-概述 自动配置\n我们讲解了SpringBoot当中起步依赖的原理，就是Maven的依赖传递。接下来我们解析下自动配置的原理，我们要分析自动配置的原理，首先要知道什么是自动配置。\nSpringBoot的自动配置就是当Spring容器启动后，一些配置类、bean对象就自动存入到了IOC容器中，不需要我们手动去声明，从而简化了开发，省去了繁琐的配置操作。\n比如：我们要进行事务管理、要进行AOP程序的开发，此时就不需要我们再去手动的声明这些bean对象了，我们直接使用就可以从而大大的简化程序的开发，省去了繁琐的配置操作。\n下面我们打开idea，一起来看下自动配置的效果：\n运行SpringBoot启动类 大家会看到有两个CommonConfig，在第一个CommonConfig类中定义了一个bean对象，bean对象的名字叫reader。\n在第二个CommonConfig中它的bean名字叫commonConfig，为什么还会有这样一个bean对象呢？原因是在CommonConfig配置类上添加了一个注解@Configuration，而@Configuration底层就是@Component\n所以配置类最终也是SpringIOC容器当中的一个bean对象\n在IOC容器中除了我们自己定义的bean以外，还有很多配置类，这些配置类都是SpringBoot在启动的时候加载进来的配置类。这些配置类加载进来之后，它也会生成很多的bean对象。\n比如：配置类GsonAutoConfiguration里面有一个bean，bean的名字叫gson，它的类型是Gson。\ncom.google.gson.Gson是谷歌包中提供的用来处理JSON格式数据的。\n当我们想要使用这些配置类中生成的bean对象时，可以使用@Autowired就自动注入了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import com.google.gson.Gson; import com.itheima.pojo.Result; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; @SpringBootTest public class AutoConfigurationTests { @Autowired private Gson gson; @Test public void testJson(){ String json = gson.toJson(Result.success()); System.out.println(json); } } 添加断点，使用debug模式运行测试类程序：\n问题：在当前项目中我们并没有声明谷歌提供的Gson这么一个bean对象，然后我们却可以通过@Autowired从Spring容器中注入bean对象，那么这个bean对象怎么来的？\n答案：SpringBoot项目在启动时通过自动配置完成了bean对象的创建。\n体验了SpringBoot的自动配置了，下面我们就来分析自动配置的原理。其实分析自动配置原理就是来解析在SpringBoot项目中，在引入依赖之后是如何将依赖jar包当中所定义的配置类以及bean加载到SpringIOC容器中的。\nSpringBoot原理-自动配置-方案 我们知道了什么是自动配置之后，接下来我们就要来剖析自动配置的原理。解析自动配置的原理就是分析在 SpringBoot项目当中，我们引入对应的依赖之后，是如何将依赖jar包当中所提供的bean以及配置类直接加载到当前项目的SpringIOC容器当中的。\n接下来，我们就直接通过代码来分析自动配置原理。\n准备工作：在Idea中导入\u0026quot;资料\\03. 自动配置原理\u0026quot;下的itheima-utils工程\n1、在SpringBoot项目 spring-boot-web-config2 工程中，通过坐标引入itheima-utils依赖\n1 2 3 4 5 6 @Component public class TokenParser { public void parse(){ System.out.println(\u0026#34;TokenParser ... parse ...\u0026#34;); } } 2、在测试类中，添加测试方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @SpringBootTest public class AutoConfigurationTests { @Autowired private ApplicationContext applicationContext; @Test public void testTokenParse(){ System.out.println(applicationContext.getBean(TokenParser.class)); } //省略其他代码... } 3、执行测试方法\n异常信息描述： 没有com.example.TokenParse类型的bean\n说明：在Spring容器中没有找到com.example.TokenParse类型的bean对象\n思考：引入进来的第三方依赖当中的bean以及配置类为什么没有生效？\n原因在我们之前讲解IOC的时候有提到过，在类上添加@Component注解来声明bean对象时，还需要保证@Component注解能被Spring的组件扫描到。 SpringBoot项目中的@SpringBootApplication注解，具有包扫描的作用，但是它只会扫描启动类所在的当前包以及子包。 当前包：com.itheima， 第三方依赖中提供的包：com.example（扫描不到） 那么如何解决以上问题的呢？\n方案1：@ComponentScan 组件扫描 方案2：@Import 导入（使用@Import导入的类会被Spring加载到IOC容器中） 方案一\n@ComponentScan组件扫描\n1 2 3 4 5 6 7 @SpringBootApplication @ComponentScan({\u0026#34;com.itheima\u0026#34;,\u0026#34;com.example\u0026#34;}) //指定要扫描的包 public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } } 重新执行测试方法，控制台日志输出：\n大家可以想象一下，如果采用以上这种方式来完成自动配置，那我们进行项目开发时，当需要引入大量的第三方的依赖，就需要在启动类上配置N多要扫描的包，这种方式会很繁琐。而且这种大面积的扫描性能也比较低。\n缺点：\n使用繁琐 性能低 结论：SpringBoot中并没有采用以上这种方案。\n方案二\n@Import导入\n导入形式主要有以下几种： 导入普通类 导入配置类 导入ImportSelector接口实现类 1). 使用@Import导入普通类：\n1 2 3 4 5 6 7 @Import(TokenParser.class) //导入的类会被Spring加载到IOC容器中 @SpringBootApplication public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } } 重新执行测试方法，控制台日志输出：\n2). 使用@Import导入配置类：\n配置类 1 2 3 4 5 6 7 8 9 10 11 12 @Configuration public class HeaderConfig { @Bean public HeaderParser headerParser(){ return new HeaderParser(); } @Bean public HeaderGenerator headerGenerator(){ return new HeaderGenerator(); } } 启动类 1 2 3 4 5 6 7 @Import(HeaderConfig.class) //导入配置类 @SpringBootApplication public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } } 测试类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @SpringBootTest public class AutoConfigurationTests { @Autowired private ApplicationContext applicationContext; @Test public void testHeaderParser(){ System.out.println(applicationContext.getBean(HeaderParser.class)); } @Test public void testHeaderGenerator(){ System.out.println(applicationContext.getBean(HeaderGenerator.class)); } //省略其他代码... } 执行测试方法：\n3). 使用@Import导入ImportSelector接口实现类：\nImportSelector接口实现类 1 2 3 4 5 6 public class MyImportSelector implements ImportSelector { public String[] selectImports(AnnotationMetadata importingClassMetadata) { //返回值字符串数组（数组中封装了全限定名称的类） return new String[]{\u0026#34;com.example.HeaderConfig\u0026#34;}; } } 启动类 1 2 3 4 5 6 7 8 @Import(MyImportSelector.class) //导入ImportSelector接口实现类 @SpringBootApplication public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } } 执行测试方法：\n我们使用@Import注解通过这三种方式都可以导入第三方依赖中所提供的bean或者是配置类。\n思考：如果基于以上方式完成自动配置，当要引入一个第三方依赖时，是不是还要知道第三方依赖中有哪些配置类和哪些Bean对象？\n答案：是的。 （对程序员来讲，很不友好，而且比较繁琐） 思考：当我们要使用第三方依赖，依赖中到底有哪些bean和配置类，谁最清楚？\n答案：第三方依赖自身最清楚。 结论：我们不用自己指定要导入哪些bean对象和配置类了，让第三方依赖它自己来指定。\n怎么让第三方依赖自己指定bean对象和配置类？\n比较常见的方案就是第三方依赖给我们提供一个注解，这个注解一般都以@EnableXxxx开头的注解，注解中封装的就是@Import注解 4). 使用第三方依赖提供的 @EnableXxxxx注解\n第三方依赖中提供的注解 1 2 3 4 5 @Retention(RetentionPolicy.RUNTIME) @Target(ElementType.TYPE) @Import(MyImportSelector.class)//指定要导入哪些bean对象或配置类 public @interface EnableHeaderConfig { } 在使用时只需在启动类上加上@EnableXxxxx注解即可 1 2 3 4 5 6 7 @EnableHeaderConfig //使用第三方依赖提供的Enable开头的注解 @SpringBootApplication public class SpringbootWebConfig2Application { public static void main(String[] args) { SpringApplication.run(SpringbootWebConfig2Application.class, args); } } 执行测试方法：\n以上四种方式都可以完成导入操作，但是第4种方式会更方便更优雅，而这种方式也是SpringBoot当中所采用的方式。\nSpringBoot原理-自动配置-原理分析-源码跟踪 前面我们讲解了在项目当中引入第三方依赖之后，如何加载第三方依赖中定义好的bean对象以及配置类，从而完成自动配置操作。那下面我们通过源码跟踪的形式来剖析下SpringBoot底层到底是如何完成自动配置的。\n源码跟踪技巧：\n在跟踪框架源码的时候，一定要抓住关键点，找到核心流程。一定不要从头到尾一行代码去看，一个方法的去研究，一定要找到关键流程，抓住关键点，先在宏观上对整个流程或者整个原理有一个认识，有精力再去研究其中的细节。\n要搞清楚SpringBoot的自动配置原理，要从SpringBoot启动类上使用的核心注解@SpringBootApplication开始分析：\n在@SpringBootApplication注解中包含了：\n元注解（不再解释） @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 我们先来看第一个注解：@SpringBootConfiguration\n@SpringBootConfiguration注解上使用了@Configuration，表明SpringBoot启动类就是一个配置类。\n@Indexed注解，是用来加速应用启动的（不用关心）。\n接下来再先看@ComponentScan注解：\n@ComponentScan注解是用来进行组件扫描的，扫描启动类所在的包及其子包下所有被@Component及其衍生注解声明的类。\nSpringBoot启动类，之所以具备扫描包功能，就是因为包含了@ComponentScan注解。\n最后我们来看看@EnableAutoConfiguration注解（自动配置核心注解）：\n使用@Import注解，导入了实现ImportSelector接口的实现类。\nAutoConfigurationImportSelector类是ImportSelector接口的实现类。\nAutoConfigurationImportSelector类中重写了ImportSelector接口的selectImports()方法：\nselectImports()方法底层调用getAutoConfigurationEntry()方法，获取可自动配置的配置类信息集合\ngetAutoConfigurationEntry()方法通过调用getCandidateConfigurations(annotationMetadata, attributes)方法获取在配置文件中配置的所有自动配置类的集合\ngetCandidateConfigurations方法的功能：\n获取所有基于META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports文件、META-INF/spring.factories文件中配置类的集合\nMETA-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports文件和META-INF/spring.factories文件这两个文件在哪里呢？\n通常在引入的起步依赖中，都有包含以上两个文件 在前面在给大家演示自动配置的时候，我们直接在测试类当中注入了一个叫gson的bean对象，进行JSON格式转换。虽然我们没有配置bean对象，但是我们是可以直接注入使用的。原因就是因为在自动配置类当中做了自动配置。到底是在哪个自动配置类当中做的自动配置呢？我们通过搜索来查询一下。\n在META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports配置文件中指定了第三方依赖Gson的配置类：GsonAutoConfiguration\n第三方依赖中提供的GsonAutoConfiguration类：\n在GsonAutoConfiguration类上，添加了注解@AutoConfiguration，通过查看源码，可以明确：GsonAutoConfiguration类是一个配置。\n看到这里，大家就应该明白为什么可以完成自动配置了，原理就是在配置类中定义一个@Bean标识的方法，而Spring会自动调用配置类中使用@Bean标识的方法，并把方法的返回值注册到IOC容器中。\n自动配置源码小结\n自动配置原理源码入口就是@SpringBootApplication注解，在这个注解中封装了3个注解，分别是：\n@SpringBootConfiguration 声明当前类是一个配置类 @ComponentScan 进行组件扫描（SpringBoot中默认扫描的是启动类所在的当前包及其子包） @EnableAutoConfiguration 封装了@Import注解（Import注解中指定了一个ImportSelector接口的实现类） 在实现类重写的selectImports()方法，读取当前项目下所有依赖jar包中META-INF/spring.factories、META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports两个文件里面定义的配置类（配置类中定义了@Bean注解标识的方法）。 当SpringBoot程序启动时，就会加载配置文件当中所定义的配置类，并将这些配置类信息(类的全限定名)封装到String类型的数组中，最终通过@Import注解将这些配置类全部加载到Spring的IOC容器中，交给IOC容器管理。\n最后呢给大家抛出一个问题：在META-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports文件中定义的配置类非常多，而且每个配置类中又可以定义很多的bean，那这些bean都会注册到Spring的IOC容器中吗？\n答案：并不是。 在声明bean对象时，上面有加一个以@Conditional开头的注解，这种注解的作用就是按照条件进行装配，只有满足条件之后，才会将bean注册到Spring的IOC容器中（下面会详细来讲解）\nSpringBoot原理-自动配置-原理分析-@Conditional 我们在跟踪SpringBoot自动配置的源码的时候，在自动配置类声明bean的时候，除了在方法上加了一个@Bean注解以外，还会经常用到一个注解，就是以Conditional开头的这一类的注解。以Conditional开头的这些注解都是条件装配的注解。下面我们就来介绍下条件装配注解。\n@Conditional注解：\n作用：按照一定的条件进行判断，在满足给定条件后才会注册对应的bean对象到Spring的IOC容器中。 位置：方法、类 @Conditional本身是一个父注解，派生出大量的子注解： @ConditionalOnClass：判断环境中有对应字节码文件，才注册bean到IOC容器。 @ConditionalOnMissingBean：判断环境中没有对应的bean(类型或名称)，才注册bean到IOC容器。 @ConditionalOnProperty：判断配置文件中有对应属性和值，才注册bean到IOC容器。 下面我们通过代码来演示下Conditional注解的使用：\n@ConditionalOnClass注解 1 2 3 4 5 6 7 8 9 10 11 @Configuration public class HeaderConfig { @Bean @ConditionalOnClass(name=\u0026#34;io.jsonwebtoken.Jwts\u0026#34;)//环境中存在指定的这个类，才会将该bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } //省略其他代码... } pom.xml 1 2 3 4 5 6 \u0026lt;!--JWT令牌--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.jsonwebtoken\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jjwt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 测试类 1 2 3 4 5 6 7 8 9 10 11 12 @SpringBootTest public class AutoConfigurationTests { @Autowired private ApplicationContext applicationContext; @Test public void testHeaderParser(){ System.out.println(applicationContext.getBean(HeaderParser.class)); } //省略其他代码... } 执行testHeaderParser()测试方法：\n因为io.jsonwebtoken.Jwts字节码文件在启动SpringBoot程序时已存在，所以创建HeaderParser对象并注册到IOC容器中。\n@ConditionalOnMissingBean注解 1 2 3 4 5 6 7 8 9 10 11 @Configuration public class HeaderConfig { @Bean @ConditionalOnMissingBean //不存在该类型的bean，才会将该bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } //省略其他代码... } 执行testHeaderParser()测试方法：\nSpringBoot在调用@Bean标识的headerParser()前，IOC容器中是没有HeaderParser类型的bean，所以HeaderParser对象正常创建，并注册到IOC容器中。\n再次修改@ConditionalOnMissingBean注解：\n1 2 3 4 5 6 7 8 9 10 11 @Configuration public class HeaderConfig { @Bean @ConditionalOnMissingBean(name=\u0026#34;deptController2\u0026#34;)//不存在指定名称的bean，才会将该bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } //省略其他代码... } 执行testHeaderParser()测试方法：\n因为在SpringBoot环境中不存在名字叫deptController2的bean对象，所以创建HeaderParser对象并注册到IOC容器中。\n再次修改@ConditionalOnMissingBean注解：\n1 2 3 4 5 6 7 8 9 10 11 @Configuration public class HeaderConfig { @Bean @ConditionalOnMissingBean(HeaderConfig.class)//不存在指定类型的bean，才会将bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } //省略其他代码... } 1 2 3 4 5 6 7 8 9 10 11 12 @SpringBootTest public class AutoConfigurationTests { @Autowired private ApplicationContext applicationContext; @Test public void testHeaderParser(){ System.out.println(applicationContext.getBean(HeaderParser.class)); } //省略其他代码... } 执行testHeaderParser()测试方法：\n因为HeaderConfig类中添加@Configuration注解，而@Configuration注解中包含了@Component，所以SpringBoot启动时会创建HeaderConfig类对象，并注册到IOC容器中。\n当IOC容器中有HeaderConfig类型的bean存在时，不会把创建HeaderParser对象注册到IOC容器中。而IOC容器中没有HeaderParser类型的对象时，通过getBean(HeaderParser.class)方法获取bean对象时，引发异常：NoSuchBeanDefinitionException\n@ConditionalOnProperty注解（这个注解和配置文件当中配置的属性有关系） 先在application.yml配置文件中添加如下的键值对：\n1 name: itheima 在声明bean的时候就可以指定一个条件@ConditionalOnProperty\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Configuration public class HeaderConfig { @Bean @ConditionalOnProperty(name =\u0026#34;name\u0026#34;,havingValue = \u0026#34;itheima\u0026#34;)//配置文件中存在指定属性名与值，才会将bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } @Bean public HeaderGenerator headerGenerator(){ return new HeaderGenerator(); } } 执行testHeaderParser()测试方法：\n修改@ConditionalOnProperty注解： havingValue的值修改为\u0026quot;itheima2\u0026quot;\n1 2 3 4 5 @Bean @ConditionalOnProperty(name =\u0026#34;name\u0026#34;,havingValue = \u0026#34;itheima2\u0026#34;)//配置文件中存在指定属性名与值，才会将bean加入IOC容器 public HeaderParser headerParser(){ return new HeaderParser(); } 再次执行testHeaderParser()测试方法：\n因为application.yml配置文件中，不存在： name: itheima2，所以HeaderParser对象在IOC容器中不存在\n我们再回头看看之前讲解SpringBoot源码时提到的一个配置类：GsonAutoConfiguration\n最后再给大家梳理一下自动配置原理：\n自动配置的核心就在@SpringBootApplication注解上，SpringBootApplication这个注解底层包含了3个注解，分别是：\n@SpringBootConfiguration\n@ComponentScan\n@EnableAutoConfiguration\n@EnableAutoConfiguration这个注解才是自动配置的核心。\n它封装了一个@Import注解，Import注解里面指定了一个ImportSelector接口的实现类。 在这个实现类中，重写了ImportSelector接口中的selectImports()方法。 而selectImports()方法中会去读取两份配置文件，并将配置文件中定义的配置类做为selectImports()方法的返回值返回，返回值代表的就是需要将哪些类交给Spring的IOC容器进行管理。 那么所有自动配置类的中声明的bean都会加载到Spring的IOC容器中吗? 其实并不会，因为这些配置类中在声明bean时，通常都会添加@Conditional开头的注解，这个注解就是进行条件装配。而Spring会根据Conditional注解有选择性的进行bean的创建。 @Enable 开头的注解底层，它就封装了一个注解 import 注解，它里面指定了一个类，是 ImportSelector 接口的实现类。在实现类当中，我们需要去实现 ImportSelector 接口当中的一个方法 selectImports 这个方法。这个方法的返回值代表的就是我需要将哪些类交给 spring 的 IOC容器进行管理。 此时它会去读取两份配置文件，一份儿是 spring.factories，另外一份儿是 autoConfiguration.imports。而在 autoConfiguration.imports 这份儿文件当中，它就会去配置大量的自动配置的类。 而前面我们也提到过这些所有的自动配置类当中，所有的 bean都会加载到 spring 的 IOC 容器当中吗？其实并不会，因为这些配置类当中，在声明 bean 的时候，通常会加上这么一类@Conditional 开头的注解。这个注解就是进行条件装配。所以SpringBoot非常的智能，它会根据 @Conditional 注解来进行条件装配。只有条件成立，它才会声明这个bean，才会将这个 bean 交给 IOC 容器管理。 SpringBoot原理-自动配置-案例(自定义starter分析) 前面我们解析了SpringBoot中自动配置的原理，下面我们就通过一个自定义starter案例来加深大家对于自动配置原理的理解。首先介绍一下自定义starter的业务场景，再来分析一下具体的操作步骤。\n所谓starter指的就是SpringBoot当中的起步依赖。在SpringBoot当中已经给我们提供了很多的起步依赖了，我们为什么还需要自定义 starter 起步依赖？这是因为在实际的项目开发当中，我们可能会用到很多第三方的技术，并不是所有的第三方的技术官方都给我们提供了与SpringBoot整合的starter起步依赖，但是这些技术又非常的通用，在很多项目组当中都在使用。\n业务场景：\n我们前面案例当中所使用的阿里云OSS对象存储服务，现在阿里云的官方是没有给我们提供对应的起步依赖的，这个时候使用起来就会比较繁琐，我们需要引入对应的依赖。我们还需要在配置文件当中进行配置，还需要基于官方SDK示例来改造对应的工具类，我们在项目当中才可以进行使用。 大家想在我们当前项目当中使用了阿里云OSS，我们需要进行这么多步的操作。在别的项目组当中要想使用阿里云OSS，是不是也需要进行这么多步的操作，所以这个时候我们就可以自定义一些公共组件，在这些公共组件当中，我就可以提前把需要配置的bean都提前配置好。将来在项目当中，我要想使用这个技术，我直接将组件对应的坐标直接引入进来，就已经自动配置好了，就可以直接使用了。我们也可以把公共组件提供给别的项目组进行使用，这样就可以大大的简化我们的开发。 在SpringBoot项目中，一般都会将这些公共组件封装为SpringBoot当中的starter，也就是我们所说的起步依赖。\nSpringBoot官方starter命名： spring-boot-starter-xxxx\n第三组织提供的starter命名： xxxx-spring-boot-starter\nMybatis提供了配置类，并且也提供了springboot会自动读取的配置文件。当SpringBoot项目启动时，会读取到spring.factories配置文件中的配置类并加载配置类，生成相关bean对象注册到IOC容器中。\n结果：我们可以直接在SpringBoot程序中使用Mybatis自动配置的bean对象。\n在自定义一个起步依赖starter的时候，按照规范需要定义两个模块：\nstarter模块（进行依赖管理[把程序开发所需要的依赖都定义在starter起步依赖中]） autoconfigure模块（自动配置） 将来在项目当中进行相关功能开发时，只需要引入一个起步依赖就可以了，因为它会将autoconfigure自动配置的依赖给传递下来。\n上面我们简单介绍了自定义starter的场景，以及自定义starter时涉及到的模块之后，接下来我们就来完成一个自定义starter的案例。\n需求：自定义aliyun-oss-spring-boot-starter，完成阿里云OSS操作工具类AliyunOSSUtils的自动配置。\n目标：引入起步依赖引入之后，要想使用阿里云OSS，注入AliyunOSSUtils直接使用即可。\n之前阿里云OSS的使用：\n配置文件 1 2 3 4 5 6 7 #配置阿里云OSS参数 aliyun: oss: endpoint: https://oss-cn-shanghai.aliyuncs.com accessKeyId: accessKeySecret: bucketName: web-framework01 AliOSSProperties类 1 2 3 4 5 6 7 8 9 10 11 12 13 @Data @Component @ConfigurationProperties(prefix = \u0026#34;aliyun.oss\u0026#34;) public class AliOSSProperties { //区域 private String endpoint; //身份ID private String accessKeyId ; //身份密钥 private String accessKeySecret ; //存储空间 private String bucketName; } AliOSSUtils工具类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @Component //当前类对象由Spring创建和管理 public class AliOSSUtils { @Autowired private AliOSSProperties aliOSSProperties; /** * 实现上传图片到OSS */ public String upload(MultipartFile multipartFile) throws IOException { // 获取上传的文件的输入流 InputStream inputStream = multipartFile.getInputStream(); // 避免文件覆盖 String originalFilename = multipartFile.getOriginalFilename(); String fileName = UUID.randomUUID().toString() + originalFilename.substring(originalFilename.lastIndexOf(\u0026#34;.\u0026#34;)); //上传文件到 OSS OSS ossClient = new OSSClientBuilder().build(aliOSSProperties.getEndpoint(), aliOSSProperties.getAccessKeyId(), aliOSSProperties.getAccessKeySecret()); ossClient.putObject(aliOSSProperties.getBucketName(), fileName, inputStream); //文件访问路径 String url =aliOSSProperties.getEndpoint().split(\u0026#34;//\u0026#34;)[0] + \u0026#34;//\u0026#34; + aliOSSProperties.getBucketName() + \u0026#34;.\u0026#34; + aliOSSProperties.getEndpoint().split(\u0026#34;//\u0026#34;)[1] + \u0026#34;/\u0026#34; + fileName; // 关闭ossClient ossClient.shutdown(); return url;// 把上传到oss的路径返回 } } 当我们在项目当中要使用阿里云OSS，就可以注入AliOSSUtils工具类来进行文件上传。但这种方式其实是比较繁琐的。\n大家再思考，现在我们使用阿里云OSS，需要做这么几步，将来大家在开发其他的项目的时候，你使用阿里云OSS，这几步你要不要做？当团队中其他小伙伴也在使用阿里云OSS的时候，步骤 不也是一样的。\n所以这个时候我们就可以制作一个公共组件(自定义starter)。starter定义好之后，将来要使用阿里云OSS进行文件上传，只需要将起步依赖引入进来之后，就可以直接注入AliOSSUtils使用了。\n需求明确了，接下来我们再来分析一下具体的实现步骤：\n第1步：创建自定义starter模块（进行依赖管理） 把阿里云OSS所有的依赖统一管理起来 第2步：创建autoconfigure模块 在starter中引入autoconfigure （我们使用时只需要引入starter起步依赖即可） 第3步：在autoconfigure中完成自动配置 定义一个自动配置类，在自动配置类中将所要配置的bean都提前配置好 定义配置文件，把自动配置类的全类名定义在配置文件中 我们分析完自定义阿里云OSS自动配置的操作步骤了，下面我们就按照分析的步骤来实现自定义starter。\nSpringBoot原理-自动配置-案例(自定义starter实现) 自定义starter的步骤我们刚才已经分析了，接下来我们就按照分析的步骤来完成自定义starter的开发。\n首先我们先来创建两个Maven模块：\n1). aliyun-oss-spring-boot-starter模块\n创建完starter模块后，删除多余的文件，最终保留内容如下：\n删除pom.xml文件中多余的内容后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;11\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 2). aliyun-oss-spring-boot-autoconfigure模块\n创建完starter模块后，删除多余的文件，最终保留内容如下：\n删除pom.xml文件中多余的内容后：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;11\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 按照我们之前的分析，是需要在starter模块中来引入autoconfigure这个模块的。打开starter模块中的pom文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;11\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--引入autoconfigure模块--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 前两步已经完成了，接下来是最关键的就是第三步：\n在autoconfigure模块当中来完成自动配置操作。\n我们将之前案例中所使用的阿里云OSS部分的代码直接拷贝到autoconfigure模块下，然后进行改造就行了。\n拷贝过来后，还缺失一些相关的依赖，需要把相关依赖也拷贝过来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;11\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--引入web起步依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--Lombok--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--阿里云OSS--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-sdk-oss\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.15.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.xml.bind\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jaxb-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.activation\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;activation\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- no more than 2.3.3--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.glassfish.jaxb\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jaxb-runtime\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 现在大家思考下，在类上添加的@Component注解还有用吗？\n答案：没用了。 在SpringBoot项目中，并不会去扫描com.aliyun.oss这个包，不扫描这个包那类上的注解也就失去了作用。\n@Component注解不需要使用了，可以从类上删除了。\n删除后报红色错误，暂时不理会，后面再来处理。\n删除AliOSSUtils类中的@Component注解、@Autowired注解\n下面我们就要定义一个自动配置类了，在自动配置类当中来声明AliOSSUtils的bean对象。\nAliOSSAutoConfiguration类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Configuration//当前类为Spring配置类 @EnableConfigurationProperties(AliOSSProperties.class)//导入AliOSSProperties类，并交给SpringIOC管理 public class AliOSSAutoConfiguration { //创建AliOSSUtils对象，并交给SpringIOC容器 @Bean public AliOSSUtils aliOSSUtils(AliOSSProperties aliOSSProperties){ AliOSSUtils aliOSSUtils = new AliOSSUtils(); aliOSSUtils.setAliOSSProperties(aliOSSProperties); return aliOSSUtils; } } AliOSSProperties类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 /*阿里云OSS相关配置*/ @Data @ConfigurationProperties(prefix = \u0026#34;aliyun.oss\u0026#34;) public class AliOSSProperties { //区域 private String endpoint; //身份ID private String accessKeyId ; //身份密钥 private String accessKeySecret ; //存储空间 private String bucketName; } AliOSSUtils类：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Data public class AliOSSUtils { private AliOSSProperties aliOSSProperties; /** * 实现上传图片到OSS */ public String upload(MultipartFile multipartFile) throws IOException { // 获取上传的文件的输入流 InputStream inputStream = multipartFile.getInputStream(); // 避免文件覆盖 String originalFilename = multipartFile.getOriginalFilename(); String fileName = UUID.randomUUID().toString() + originalFilename.substring(originalFilename.lastIndexOf(\u0026#34;.\u0026#34;)); //上传文件到 OSS OSS ossClient = new OSSClientBuilder().build(aliOSSProperties.getEndpoint(), aliOSSProperties.getAccessKeyId(), aliOSSProperties.getAccessKeySecret()); ossClient.putObject(aliOSSProperties.getBucketName(), fileName, inputStream); //文件访问路径 String url =aliOSSProperties.getEndpoint().split(\u0026#34;//\u0026#34;)[0] + \u0026#34;//\u0026#34; + aliOSSProperties.getBucketName() + \u0026#34;.\u0026#34; + aliOSSProperties.getEndpoint().split(\u0026#34;//\u0026#34;)[1] + \u0026#34;/\u0026#34; + fileName; // 关闭ossClient ossClient.shutdown(); return url;// 把上传到oss的路径返回 } } 在aliyun-oss-spring-boot-autoconfigure模块中的resources下，新建自动配置文件：\nMETA-INF/spring/org.springframework.boot.autoconfigure.AutoConfiguration.imports 1 com.aliyun.oss.AliOSSAutoConfiguration 自定义starter测试\n阿里云OSS的starter我们刚才已经定义好了，接下来我们就来做一个测试。\n今天的课程资料当中，提供了一个自定义starter的测试工程。我们直接打开文件夹，里面有一个测试工程。测试工程就是springboot-autoconfiguration-test，我们只需要将测试工程直接导入到Idea当中即可。\n测试前准备：\n在test工程中引入阿里云starter依赖 通过依赖传递，会把autoconfigure依赖也引入了 1 2 3 4 5 6 \u0026lt;!--引入阿里云OSS起步依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.aliyun.oss\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aliyun-oss-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. 在test工程中的application.yml文件中，配置阿里云OSS配置参数信息（从以前的工程中拷贝即可）\n1 2 3 4 5 6 7 #配置阿里云OSS参数 aliyun: oss: endpoint: https://oss-cn-shanghai.aliyuncs.com accessKeyId: accessKeySecret: bucketName: web-framework01 3. 在test工程中的UploadController类编写代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RestController public class UploadController { @Autowired private AliOSSUtils aliOSSUtils; @PostMapping(\u0026#34;/upload\u0026#34;) public String upload(MultipartFile image) throws Exception { //上传文件到阿里云 OSS String url = aliOSSUtils.upload(image); return url; } } 编写完代码后，我们启动当前的SpringBoot测试工程：\n随着SpringBoot项目启动，自动配置会把AliOSSUtils的bean对象装配到IOC容器中 用postman工具进行文件上传：\n通过断点可以看到自动注入AliOSSUtils的bean对象：\nWeb后端开发-总结 到此基于SpringBoot进行web后端开发的相关知识我们已经学习完毕了。下面我们一起针对这段web课程做一个总结。\n我们来回顾一下关于web后端开发，我们都学习了哪些内容，以及每一块知识，具体是属于哪个框架的。\nweb后端开发现在基本上都是基于标准的三层架构进行开发的，在三层架构当中，Controller控制器层负责接收请求响应数据，Service业务层负责具体的业务逻辑处理，而Dao数据访问层也叫持久层，就是用来处理数据访问操作的，来完成数据库当中数据的增删改查操作。\n在三层架构当中，前端发起请求首先会到达Controller(不进行逻辑处理)，然后Controller会直接调用Service 进行逻辑处理， Service再调用Dao完成数据访问操作。\n如果我们在执行具体的业务处理之前，需要去做一些通用的业务处理，比如：我们要进行统一的登录校验，我们要进行统一的字符编码等这些操作时，我们就可以借助于Javaweb当中三大组件之一的过滤器Filter或者是Spring当中提供的拦截器Interceptor来实现。\n而为了实现三层架构层与层之间的解耦，我们学习了Spring框架当中的第一大核心：IOC控制反转与DI依赖注入。\n所谓控制反转，指的是将对象创建的控制权由应用程序自身交给外部容器，这个容器就是我们常说的IOC容器或Spring容器。\n而DI依赖注入指的是容器为程序提供运行时所需要的资源。\n除了IOC与DI我们还讲到了AOP面向切面编程，还有Spring中的事务管理、全局异常处理器，以及传递会话技术Cookie、Session以及新的会话跟踪解决方案JWT令牌，阿里云OSS对象存储服务，以及通过Mybatis持久层架构操作数据库等技术。\n我们在学习这些web后端开发技术的时候，我们都是基于主流的SpringBoot进行整合使用的。而SpringBoot又是用来简化开发，提高开发效率的。像过滤器、拦截器、IOC、DI、AOP、事务管理等这些技术到底是哪个框架提供的核心功能？\nFilter过滤器、Cookie、 Session这些都是传统的JavaWeb提供的技术。\nJWT令牌、阿里云OSS对象存储服务，是现在企业项目中常见的一些解决方案。\nIOC控制反转、DI依赖注入、AOP面向切面编程、事务管理、全局异常处理、拦截器等，这些技术都是 Spring Framework框架当中提供的核心功能。\nMybatis就是一个持久层的框架，是用来操作数据库的。\n在Spring框架的生态中，对web程序开发提供了很好的支持，如：全局异常处理器、拦截器这些都是Spring框架中web开发模块所提供的功能，而Spring框架的web开发模块，我们也称为：SpringMVC\nSpringMVC不是一个单独的框架，它是Spring框架的一部分，是Spring框架中的web开发模块，是用来简化原始的Servlet程序开发的。\n外界俗称的SSM，就是由：SpringMVC、Spring Framework、Mybatis三块组成。\n基于传统的SSM框架进行整合开发项目会比较繁琐，而且效率也比较低，所以在现在的企业项目开发当中，基本上都是直接基于SpringBoot整合SSM进行项目开发的。\n","date":"2024-04-13T16:01:26+08:00","image":"https://nova-bryan.github.io/p/springboot%E5%8E%9F%E7%90%86%E5%92%8Cbean%E7%9A%84%E7%AE%A1%E7%90%86/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/springboot%E5%8E%9F%E7%90%86%E5%92%8Cbean%E7%9A%84%E7%AE%A1%E7%90%86/","title":"SpringBoot原理和Bean的管理"},{"content":"登录基础功能 在前面的课程中，我们已经实现了部门管理、员工管理的基本功能，但是大家会发现，我们并没有登录，就直接访问到了Tlias智能学习辅助系统的后台。 这是不安全的，所以我们今天的主题就是登录认证。 最终我们要实现的效果就是用户必须登录之后，才可以访问后台系统中的功能。\n需求 在登录界面中，我们可以输入用户的用户名以及密码，然后点击 \u0026ldquo;登录\u0026rdquo; 按钮就要请求服务器，服务端判断用户输入的用户名或者密码是否正确。如果正确，则返回成功结果，前端跳转至系统首页面。\n接口文档 我们参照接口文档来开发登录功能\n基本信息\n1 2 3 请求路径：/login 请求方式：POST 接口描述：该接口用于员工登录Tlias智能学习辅助系统，登录完毕后，系统下发JWT令牌。 请求参数\n参数格式：application/json\n参数说明：\n名称 类型 是否必须 备注 username string 必须 用户名 password string 必须 密码 请求数据样例：\n1 2 3 4 { \u0026#34;username\u0026#34;: \u0026#34;jinyong\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;123456\u0026#34; } 响应数据 参数格式：application/json\n参数说明：\n名称 类型 是否必须 默认值 备注 其他信息 code number 必须 响应码, 1 成功 ; 0 失败 msg string 非必须 提示信息 data string 必须 返回的数据 , jwt令牌 响应数据样例：\n1 2 3 4 5 { \u0026#34;code\u0026#34;: 1, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiJ9.eyJuYW1lIjoi6YeR5bq4IiwiaWQiOjEsInVzZXJuYW1lIjoiamlueW9uZyIsImV4cCI6MTY2MjIwNzA0OH0.KkUc_CXJZJ8Dd063eImx4H9Ojfrr6XMJ-yVzaWCVZCo\u0026#34; } 思路分析 登录服务端的核心逻辑就是：接收前端请求传递的用户名和密码 ，然后再根据用户名和密码查询用户信息，如果用户信息存在，则说明用户输入的用户名和密码正确。如果查询到的用户不存在，则说明用户输入的用户名和密码错误。\n功能开发 LoginController\n1 2 3 4 5 6 7 8 9 10 11 12 @RestController public class LoginController { @Autowired private EmpService empService; @PostMapping(\u0026#34;/login\u0026#34;) public Result login(@RequestBody Emp emp){ Emp e = empService.login(emp); return e != null ? Result.success():Result.error(\u0026#34;用户名或密码错误\u0026#34;); } } EmpService\n1 2 3 4 5 6 7 8 9 10 11 public interface EmpService { /** * 用户登录 * @param emp * @return */ public Emp login(Emp emp); //省略其他代码... } EmpServiceImpl\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Slf4j @Service public class EmpServiceImpl implements EmpService { @Autowired private EmpMapper empMapper; @Override public Emp login(Emp emp) { //调用dao层功能：登录 Emp loginEmp = empMapper.getByUsernameAndPassword(emp); //返回查询结果给Controller return loginEmp; } //省略其他代码... } EmpMapper\n1 2 3 4 5 6 7 8 9 10 @Mapper public interface EmpMapper { @Select(\u0026#34;select id, username, password, name, gender, image, job, entrydate, dept_id, create_time, update_time \u0026#34; + \u0026#34;from emp \u0026#34; + \u0026#34;where username=#{username} and password =#{password}\u0026#34;) public Emp getByUsernameAndPassword(Emp emp); //省略其他代码... } 测试 功能开发完毕后，我们就可以启动服务，打开postman进行测试了。\n发起POST请求，访问：http://localhost:8080/login\npostman测试通过了，那接下来，我们就可以结合着前端工程进行联调测试。\n先退出系统，进入到登录页面：\n在登录页面输入账户密码：\n登录成功之后进入到后台管理系统页面：\n登录校验-概述 问题分析 我们已经完成了基础登录功能的开发与测试，在我们登录成功后就可以进入到后台管理系统中进行数据的操作。\n但是当我们在浏览器中新的页面上输入地址：http://localhost:9528/#/system/dept，发现没有登录仍然可以进入到后端管理系统页面。\n而真正的登录功能应该是：登陆后才能访问后端系统页面，不登陆则跳转登陆页面进行登陆。\n为什么会出现这个问题？其实原因很简单，就是因为针对于我们当前所开发的部门管理、员工管理以及文件上传等相关接口来说，我们在服务器端并没有做任何的判断，没有去判断用户是否登录了。所以无论用户是否登录，都可以访问部门管理以及员工管理的相关数据。所以我们目前所开发的登录功能，它只是徒有其表。而我们要想解决这个问题，我们就需要完成一步非常重要的操作：登录校验。\n什么是登录校验？\n所谓登录校验，指的是我们在服务器端接收到浏览器发送过来的请求之后，首先我们要对请求进行校验。先要校验一下用户登录了没有，如果用户已经登录了，就直接执行对应的业务操作就可以了；如果用户没有登录，此时就不允许他执行相关的业务操作，直接给前端响应一个错误的结果，最终跳转到登录页面，要求他登录成功之后，再来访问对应的数据。 了解完什么是登录校验之后，接下来我们分析一下登录校验大概的实现思路。\n首先我们在宏观上先有一个认知：\n前面在讲解HTTP协议的时候，我们提到HTTP协议是无状态协议。什么又是无状态的协议？\n所谓无状态，指的是每一次请求都是独立的，下一次请求并不会携带上一次请求的数据。而浏览器与服务器之间进行交互，基于HTTP协议也就意味着现在我们通过浏览器来访问了登陆这个接口，实现了登陆的操作，接下来我们在执行其他业务操作时，服务器也并不知道这个员工到底登陆了没有。因为HTTP协议是无状态的，两次请求之间是独立的，所以是无法判断这个员工到底登陆了没有。\n那应该怎么来实现登录校验的操作呢？具体的实现思路可以分为两部分：\n在员工登录成功后，需要将用户登录成功的信息存起来，记录用户已经登录成功的标记。 在浏览器发起请求时，需要在服务端进行统一拦截，拦截后进行登录校验。 想要判断员工是否已经登录，我们需要在员工登录成功之后，存储一个登录成功的标记，接下来在每一个接口方法执行之前，先做一个条件判断，判断一下这个员工到底登录了没有。如果是登录了，就可以执行正常的业务操作，如果没有登录，会直接给前端返回一个错误的信息，前端拿到这个错误信息之后会自动的跳转到登录页面。\n我们程序中所开发的查询功能、删除功能、添加功能、修改功能，都需要使用以上套路进行登录校验。此时就会出现：相同代码逻辑，每个功能都需要编写，就会造成代码非常繁琐。\n为了简化这块操作，我们可以使用一种技术：统一拦截技术。\n通过统一拦截的技术，我们可以来拦截浏览器发送过来的所有的请求，拦截到这个请求之后，就可以通过请求来获取之前所存入的登录标记，在获取到登录标记且标记为登录成功，就说明员工已经登录了。如果已经登录，我们就直接放行(意思就是可以访问正常的业务接口了)。\n我们要完成以上操作，会涉及到web开发中的两个技术：\n会话技术 统一拦截技术 而统一拦截技术现实方案也有两种：\nServlet规范中的Filter过滤器 Spring提供的interceptor拦截器 下面我们先学习会话技术，然后再学习统一拦截技术。\n登录认证-登录校验-会话技术 会话技术介绍 什么是会话？\n在我们日常生活当中，会话指的就是谈话、交谈。\n在web开发当中，会话指的就是浏览器与服务器之间的一次连接，我们就称为一次会话。\n在用户打开浏览器第一次访问服务器的时候，这个会话就建立了，直到有任何一方断开连接，此时会话就结束了。在一次会话当中，是可以包含多次请求和响应的。\n比如：打开了浏览器来访问web服务器上的资源（浏览器不能关闭、服务器不能断开）\n第1次：访问的是登录的接口，完成登录操作 第2次：访问的是部门管理接口，查询所有部门数据 第3次：访问的是员工管理接口，查询员工数据 只要浏览器和服务器都没有关闭，以上3次请求都属于一次会话当中完成的。\n需要注意的是：会话是和浏览器关联的，当有三个浏览器客户端和服务器建立了连接时，就会有三个会话。同一个浏览器在未关闭之前请求了多次服务器，这多次请求是属于同一个会话。比如：1、2、3这三个请求都是属于同一个会话。当我们关闭浏览器之后，这次会话就结束了。而如果我们是直接把web服务器关了，那么所有的会话就都结束了。\n知道了会话的概念了，接下来我们再来了解下会话跟踪。\n会话跟踪：一种维护浏览器状态的方法，服务器需要识别多次请求是否来自于同一浏览器，以便在同一次会话的多次请求间共享数据。\n服务器会接收很多的请求，但是服务器是需要识别出这些请求是不是同一个浏览器发出来的。比如：1和2这两个请求是不是同一个浏览器发出来的，3和5这两个请求不是同一个浏览器发出来的。如果是同一个浏览器发出来的，就说明是同一个会话。如果是不同的浏览器发出来的，就说明是不同的会话。而识别多次请求是否来自于同一浏览器的过程，我们就称为会话跟踪。\n我们使用会话跟踪技术就是要完成在同一个会话中，多个请求之间进行共享数据。\n为什么要共享数据呢？\n由于HTTP是无状态协议，在后面请求中怎么拿到前一次请求生成的数据呢？此时就需要在一次会话的多次请求之间进行数据共享\n会话跟踪技术有两种：\nCookie（客户端会话跟踪技术） 数据存储在客户端浏览器当中 Session（服务端会话跟踪技术） 数据存储在储在服务端 令牌技术 登录认证-登录校验-会话跟踪方案一 上面我们介绍了什么是会话，什么是会话跟踪，并且也提到了会话跟踪 3 种常见的技术方案。接下来，我们就来对比一下这 3 种会话跟踪的技术方案，来看一下具体的实现思路，以及它们之间的优缺点。\n方案一-Cookie cookie 是客户端会话跟踪技术，它是存储在客户端浏览器的，我们使用 cookie 来跟踪会话，我们就可以在浏览器第一次发起请求来请求服务器的时候，我们在服务器端来设置一个cookie。\n比如第一次请求了登录接口，登录接口执行完成之后，我们就可以设置一个cookie，在 cookie 当中我们就可以来存储用户相关的一些数据信息。比如我可以在 cookie 当中来存储当前登录用户的用户名，用户的ID。\n服务器端在给客户端在响应数据的时候，会自动的将 cookie 响应给浏览器，浏览器接收到响应回来的 cookie 之后，会自动的将 cookie 的值存储在浏览器本地。接下来在后续的每一次请求当中，都会将浏览器本地所存储的 cookie 自动地携带到服务端。\n接下来在服务端我们就可以获取到 cookie 的值。我们可以去判断一下这个 cookie 的值是否存在，如果不存在这个cookie，就说明客户端之前是没有访问登录接口的；如果存在 cookie 的值，就说明客户端之前已经登录完成了。这样我们就可以基于 cookie 在同一次会话的不同请求之间来共享数据。\n我刚才在介绍流程的时候，用了 3 个自动：\n服务器会 自动 的将 cookie 响应给浏览器。 浏览器接收到响应回来的数据之后，会 自动 的将 cookie 存储在浏览器本地。 在后续的请求当中，浏览器会 自动 的将 cookie 携带到服务器端。 为什么这一切都是自动化进行的？\n是因为 cookie 它是 HTP 协议当中所支持的技术，而各大浏览器厂商都支持了这一标准。在 HTTP 协议官方给我们提供了一个响应头和请求头：\n响应头 Set-Cookie ：设置Cookie数据的 请求头 Cookie：携带Cookie数据的 代码测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @Slf4j @RestController public class SessionController { //设置Cookie @GetMapping(\u0026#34;/c1\u0026#34;) public Result cookie1(HttpServletResponse response){ response.addCookie(new Cookie(\u0026#34;login_username\u0026#34;,\u0026#34;itheima\u0026#34;)); //设置Cookie/响应Cookie return Result.success(); } //获取Cookie @GetMapping(\u0026#34;/c2\u0026#34;) public Result cookie2(HttpServletRequest request){ Cookie[] cookies = request.getCookies(); for (Cookie cookie : cookies) { if(cookie.getName().equals(\u0026#34;login_username\u0026#34;)){ System.out.println(\u0026#34;login_username: \u0026#34;+cookie.getValue()); //输出name为login_username的cookie } } return Result.success(); } } A. 访问c1接口，设置Cookie，http://localhost:8080/c1\n我们可以看到，设置的cookie，通过响应头Set-Cookie响应给浏览器，并且浏览器会将Cookie，存储在浏览器端。\nB. 访问c2接口 http://localhost:8080/c2，此时浏览器会自动的将Cookie携带到服务端，是通过请求头Cookie，携带的。\n优缺点\n优点：HTTP协议中支持的技术（像Set-Cookie 响应头的解析以及 Cookie 请求头数据的携带，都是浏览器自动进行的，是无需我们手动操作的） 缺点： 移动端APP(Android、IOS)中无法使用Cookie 不安全，用户可以自己禁用Cookie Cookie不能跨域 跨域介绍：\n现在的项目，大部分都是前后端分离的，前后端最终也会分开部署，前端部署在服务器 192.168.150.200 上，端口 80，后端部署在 192.168.150.100上，端口 8080 我们打开浏览器直接访问前端工程，访问url：http://192.168.150.200/login.html 然后在该页面发起请求到服务端，而服务端所在地址不再是localhost，而是服务器的IP地址192.168.150.100，假设访问接口地址为：http://192.168.150.100:8080/login 那此时就存在跨域操作了，因为我们是在 http://192.168.150.200/login.html 这个页面上访问了http://192.168.150.100:8080/login 接口 此时如果服务器设置了一个Cookie，这个Cookie是不能使用的，因为Cookie无法跨域 区分跨域的维度：\n协议 IP/协议 端口 只要上述的三个维度有任何一个维度不同，那就是跨域操作\n举例：\nhttp://192.168.150.200/login.html \u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt; https://192.168.150.200/login [协议不同，跨域]\nhttp://192.168.150.200/login.html \u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt; http://192.168.150.100/login [IP不同，跨域]\nhttp://192.168.150.200/login.html \u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt; http://192.168.150.200:8080/login [端口不同，跨域]\nhttp://192.168.150.200/login.html \u0026mdash;\u0026mdash;\u0026mdash;-\u0026gt; http://192.168.150.200/login [不跨域]\n登录认证-登录校验-会话跟踪方案二、三 方案二-Session 前面介绍的时候，我们提到Session，它是服务器端会话跟踪技术，所以它是存储在服务器端的。而 Session 的底层其实就是基于我们刚才所介绍的 Cookie 来实现的。\n获取Session\n如果我们现在要基于 Session 来进行会话跟踪，浏览器在第一次请求服务器的时候，我们就可以直接在服务器当中来获取到会话对象Session。如果是第一次请求Session ，会话对象是不存在的，这个时候服务器会自动的创建一个会话对象Session 。而每一个会话对象Session ，它都有一个ID（示意图中Session后面括号中的1，就表示ID），我们称之为 Session 的ID。\n响应Cookie (JSESSIONID)\n接下来，服务器端在给浏览器响应数据的时候，它会将 Session 的 ID 通过 Cookie 响应给浏览器。其实在响应头当中增加了一个 Set-Cookie 响应头。这个 Set-Cookie 响应头对应的值是不是cookie？ cookie 的名字是固定的 JSESSIONID 代表的服务器端会话对象 Session 的 ID。浏览器会自动识别这个响应头，然后自动将Cookie存储在浏览器本地。\n查找Session\n接下来，在后续的每一次请求当中，都会将 Cookie 的数据获取出来，并且携带到服务端。接下来服务器拿到JSESSIONID这个 Cookie 的值，也就是 Session 的ID。拿到 ID 之后，就会从众多的 Session 当中来找到当前请求对应的会话对象Session。\n这样我们是不是就可以通过 Session 会话对象在同一次会话的多次请求之间来共享数据了？好，这就是基于 Session 进行会话跟踪的流程。\n代码测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Slf4j @RestController public class SessionController { @GetMapping(\u0026#34;/s1\u0026#34;) public Result session1(HttpSession session){ log.info(\u0026#34;HttpSession-s1: {}\u0026#34;, session.hashCode()); session.setAttribute(\u0026#34;loginUser\u0026#34;, \u0026#34;tom\u0026#34;); //往session中存储数据 return Result.success(); } @GetMapping(\u0026#34;/s2\u0026#34;) public Result session2(HttpServletRequest request){ HttpSession session = request.getSession(); log.info(\u0026#34;HttpSession-s2: {}\u0026#34;, session.hashCode()); Object loginUser = session.getAttribute(\u0026#34;loginUser\u0026#34;); //从session中获取数据 log.info(\u0026#34;loginUser: {}\u0026#34;, loginUser); return Result.success(loginUser); } } A. 访问 s1 接口，http://localhost:8080/s1\n请求完成之后，在响应头中，就会看到有一个Set-Cookie的响应头，里面响应回来了一个Cookie，就是JSESSIONID，这个就是服务端会话对象 Session 的ID。\nB. 访问 s2 接口，http://localhost:8080/s2\n接下来，在后续的每次请求时，都会将Cookie的值，携带到服务端，那服务端呢，接收到Cookie之后，会自动的根据JSESSIONID的值，找到对应的会话对象Session。\n那经过这两步测试，大家也会看到，在控制台中输出如下日志：\n两次请求，获取到的Session会话对象的hashcode是一样的，就说明是同一个会话对象。而且，第一次请求时，往Session会话对象中存储的值，第二次请求时，也获取到了。 那这样，我们就可以通过Session会话对象，在同一个会话的多次请求之间来进行数据共享了。\n优缺点\n优点：Session是存储在服务端的，安全 缺点： 服务器集群环境下无法直接使用Session 移动端APP(Android、IOS)中无法使用Cookie 用户可以自己禁用Cookie Cookie不能跨域 PS：Session 底层是基于Cookie实现的会话跟踪，如果Cookie不可用，则该方案，也就失效了。\n服务器集群环境为何无法使用Session？\n首先第一点，我们现在所开发的项目，一般都不会只部署在一台服务器上，因为一台服务器会存在一个很大的问题，就是单点故障。所谓单点故障，指的就是一旦这台服务器挂了，整个应用都没法访问了。 所以在现在的企业项目开发当中，最终部署的时候都是以集群的形式来进行部署，也就是同一个项目它会部署多份。比如这个项目我们现在就部署了 3 份。\n而用户在访问的时候，到底访问这三台其中的哪一台？其实用户在访问的时候，他会访问一台前置的服务器，我们叫负载均衡服务器，我们在后面项目当中会详细讲解。目前大家先有一个印象负载均衡服务器，它的作用就是将前端发起的请求均匀的分发给后面的这三台服务器。\n此时假如我们通过 session 来进行会话跟踪，可能就会存在这样一个问题。用户打开浏览器要进行登录操作，此时会发起登录请求。登录请求到达负载均衡服务器，将这个请求转给了第一台 Tomcat 服务器。\nTomcat 服务器接收到请求之后，要获取到会话对象session。获取到会话对象 session 之后，要给浏览器响应数据，最终在给浏览器响应数据的时候，就会携带这么一个 cookie 的名字，就是 JSESSIONID ，下一次再请求的时候，是不是又会将 Cookie 携带到服务端？\n好。此时假如又执行了一次查询操作，要查询部门的数据。这次请求到达负载均衡服务器之后，负载均衡服务器将这次请求转给了第二台 Tomcat 服务器，此时他就要到第二台 Tomcat 服务器当中。根据JSESSIONID 也就是对应的 session 的 ID 值，要找对应的 session 会话对象。\n我想请问在第二台服务器当中有没有这个ID的会话对象 Session， 是没有的。此时是不是就出现问题了？我同一个浏览器发起了 2 次请求，结果获取到的不是同一个会话对象，这就是Session这种会话跟踪方案它的缺点，在服务器集群环境下无法直接使用Session。\n大家会看到上面这两种传统的会话技术，在现在的企业开发当中是不是会存在很多的问题。 为了解决这些问题，在现在的企业开发当中，基本上都会采用第三种方案，通过令牌技术来进行会话跟踪。接下来我们就来介绍一下令牌技术，来看一下令牌技术又是如何跟踪会话的。\n登录校验-JWT令牌-介绍 前面我们介绍了基于令牌技术来实现会话追踪。这里所提到的令牌就是用户身份的标识，其本质就是一个字符串。令牌的形式有很多，我们使用的是功能强大的 JWT令牌。\n介绍 JWT全称：JSON Web Token （官网：https://jwt.io/）\n定义了一种简洁的、自包含的格式，用于在通信双方以json数据格式安全的传输信息。由于数字签名的存在，这些信息是可靠的。\n简洁：是指jwt就是一个简单的字符串。可以在请求参数或者是请求头当中直接传递。\n自包含：指的是jwt令牌，看似是一个随机的字符串，但是我们是可以根据自身的需求在jwt令牌中存储自定义的数据内容。如：可以直接在jwt令牌中存储用户的相关信息。\n简单来讲，jwt就是将原始的json数据格式进行了安全的封装，这样就可以直接基于jwt在通信双方安全的进行信息传输了。\nJWT的组成： （JWT令牌由三个部分组成，三个部分之间使用英文的点来分割）\n第一部分：Header(头）， 记录令牌类型、签名算法等。 例如：{\u0026ldquo;alg\u0026rdquo;:\u0026ldquo;HS256\u0026rdquo;,\u0026ldquo;type\u0026rdquo;:\u0026ldquo;JWT\u0026rdquo;}\n第二部分：Payload(有效载荷），携带一些自定义信息、默认信息等。 例如：{\u0026ldquo;id\u0026rdquo;:\u0026ldquo;1\u0026rdquo;,\u0026ldquo;username\u0026rdquo;:\u0026ldquo;Tom\u0026rdquo;}\n第三部分：Signature(签名），防止Token被篡改、确保安全性。将header、payload，并加入指定秘钥，通过指定签名算法计算而来。\n签名的目的就是为了防jwt令牌被篡改，而正是因为jwt令牌最后一个部分数字签名的存在，所以整个jwt 令牌是非常安全可靠的。一旦jwt令牌当中任何一个部分、任何一个字符被篡改了，整个令牌在校验的时候都会失败，所以它是非常安全可靠的。\nJWT是如何将原始的JSON格式数据，转变为字符串的呢？\n其实在生成JWT令牌时，会对JSON格式的数据进行一次编码：进行base64编码\nBase64：是一种基于64个可打印的字符来表示二进制数据的编码方式。既然能编码，那也就意味着也能解码。所使用的64个字符分别是A到Z、a到z、 0- 9，一个加号，一个斜杠，加起来就是64个字符。任何数据经过base64编码之后，最终就会通过这64个字符来表示。当然还有一个符号，那就是等号。等号它是一个补位的符号\n需要注意的是Base64是编码方式，而不是加密方式。\nJWT令牌最典型的应用场景就是登录认证：\n在浏览器发起请求来执行登录操作，此时会访问登录的接口，如果登录成功之后，我们需要生成一个jwt令牌，将生成的 jwt令牌返回给前端。 前端拿到jwt令牌之后，会将jwt令牌存储起来。在后续的每一次请求中都会将jwt令牌携带到服务端。 服务端统一拦截请求之后，先来判断一下这次请求有没有把令牌带过来，如果没有带过来，直接拒绝访问，如果带过来了，还要校验一下令牌是否是有效。如果有效，就直接放行进行请求的处理。 在JWT登录认证的场景中我们发现，整个流程当中涉及到两步操作：\n在登录成功之后，要生成令牌。 每一次请求当中，要接收令牌并对令牌进行校验。 稍后我们再来学习如何来生成jwt令牌，以及如何来校验jwt令牌。\n登录校验-JWT令牌-生成和校验 生成和校验 简单介绍了JWT令牌以及JWT令牌的组成之后，接下来我们就来学习基于Java代码如何生成和校验JWT令牌。\n首先我们先来实现JWT令牌的生成。要想使用JWT令牌，需要先引入JWT的依赖：\n1 2 3 4 5 6 \u0026lt;!-- JWT依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.jsonwebtoken\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jjwt\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在引入完JWT来赖后，就可以调用工具包中提供的API来完成JWT令牌的生成和校验\n工具类：Jwts\n生成JWT代码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Test public void genJwt(){ Map\u0026lt;String,Object\u0026gt; claims = new HashMap\u0026lt;\u0026gt;(); claims.put(\u0026#34;id\u0026#34;,1); claims.put(\u0026#34;username\u0026#34;,\u0026#34;Tom\u0026#34;); String jwt = Jwts.builder() .setClaims(claims) //自定义内容(载荷) .signWith(SignatureAlgorithm.HS256, \u0026#34;itheima\u0026#34;) //签名算法 .setExpiration(new Date(System.currentTimeMillis() + 24*3600*1000)) //有效期 .compact(); System.out.println(jwt); } 运行测试方法：\n1 eyJhbGciOiJIUzI1NiJ9.eyJpZCI6MSwiZXhwIjoxNjcyNzI5NzMwfQ.fHi0Ub8npbyt71UqLXDdLyipptLgxBUg_mSuGJtXtBk 输出的结果就是生成的JWT令牌,，通过英文的点分割对三个部分进行分割，我们可以将生成的令牌复制一下，然后打开JWT的官网，将生成的令牌直接放在Encoded位置，此时就会自动的将令牌解析出来。\n第一部分解析出来，看到JSON格式的原始数据，所使用的签名算法为HS256。\n第二个部分是我们自定义的数据，之前我们自定义的数据就是id，还有一个exp代表的是我们所设置的过期时间。\n由于前两个部分是base64编码，所以是可以直接解码出来。但最后一个部分并不是base64编码，是经过签名算法计算出来的，所以最后一个部分是不会解析的。\n实现了JWT令牌的生成，下面我们接着使用Java代码来校验JWT令牌(解析生成的令牌)：\n1 2 3 4 5 6 7 8 9 @Test public void parseJwt(){ Claims claims = Jwts.parser() .setSigningKey(\u0026#34;itheima\u0026#34;)//指定签名密钥（必须保证和生成令牌时使用相同的签名密钥） .parseClaimsJws(\u0026#34;eyJhbGciOiJIUzI1NiJ9.eyJpZCI6MSwiZXhwIjoxNjcyNzI5NzMwfQ.fHi0Ub8npbyt71UqLXDdLyipptLgxBUg_mSuGJtXtBk\u0026#34;) .getBody(); System.out.println(claims); } 运行测试方法：\n1 {id=1, exp=1672729730} 令牌解析后，我们可以看到id和过期时间，如果在解析的过程当中没有报错，就说明解析成功了。\n下面我们做一个测试：把令牌header中的数字9变为8，运行测试方法后发现报错：\n原header： eyJhbGciOiJIUzI1NiJ9\n修改为： eyJhbGciOiJIUzI1NiJ8\n结论：篡改令牌中的任何一个字符，在对令牌进行解析时都会报错，所以JWT令牌是非常安全可靠的。\n我们继续测试：修改生成令牌的时指定的过期时间，修改为1分钟\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 @Test public void genJwt(){ Map\u0026lt;String,Object\u0026gt; claims = new HashMap\u0026lt;\u0026gt;(); claims.put(“id”,1); claims.put(“username”,“Tom”); String jwt = Jwts.builder() .setClaims(claims) //自定义内容(载荷) .signWith(SignatureAlgorithm.HS256, “itheima”) //签名算法 .setExpiration(new Date(System.currentTimeMillis() + 60*1000)) //有效期60秒 .compact(); System.out.println(jwt); //输出结果：eyJhbGciOiJIUzI1NiJ9.eyJpZCI6MSwiZXhwIjoxNjczMDA5NzU0fQ.RcVIR65AkGiax-ID6FjW60eLFH3tPTKdoK7UtE4A1ro } @Test public void parseJwt(){ Claims claims = Jwts.parser() .setSigningKey(\u0026#34;itheima\u0026#34;)//指定签名密钥 .parseClaimsJws(\u0026#34;eyJhbGciOiJIUzI1NiJ9.eyJpZCI6MSwiZXhwIjoxNjczMDA5NzU0fQ.RcVIR65AkGiax-ID6FjW60eLFH3tPTKdoK7UtE4A1ro\u0026#34;) .getBody(); System.out.println(claims); } 等待1分钟之后运行测试方法发现也报错了，说明：JWT令牌过期后，令牌就失效了，解析的为非法令牌。\n通过以上测试，我们在使用JWT令牌时需要注意：\nJWT校验时使用的签名秘钥，必须和生成JWT令牌时使用的秘钥是配套的。 如果JWT令牌解析校验时报错，则说明 JWT令牌被篡改 或 失效了，令牌非法。 登录校验-JWT令牌-登录后下发令牌 登录下发令牌 JWT令牌的生成和校验的基本操作我们已经学习完了，接下来我们就需要在案例当中通过JWT令牌技术来跟踪会话。具体的思路我们前面已经分析过了，主要就是两步操作：\n生成令牌 在登录成功之后来生成一个JWT令牌，并且把这个令牌直接返回给前端 校验令牌 拦截前端请求，从请求中获取到令牌，对令牌进行解析校验 那我们首先来完成：登录成功之后生成JWT令牌，并且把令牌返回给前端。\nJWT令牌怎么返回给前端呢？此时我们就需要再来看一下接口文档当中关于登录接口的描述（主要看响应数据）：\n响应数据\n参数格式：application/json\n参数说明：\n名称 类型 是否必须 默认值 备注 其他信息 code number 必须 响应码, 1 成功 ; 0 失败 msg string 非必须 提示信息 data string 必须 返回的数据 , jwt令牌 响应数据样例：\n1 2 3 4 5 { \u0026#34;code\u0026#34;: 1, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiJ9.eyJuYW1lIjoi6YeR5bq4IiwiaWQiOjEsInVzZXJuYW1lIjoiamlueW9uZyIsImV4cCI6MTY2MjIwNzA0OH0.KkUc_CXJZJ8Dd063eImx4H9Ojfrr6XMJ-yVzaWCVZCo\u0026#34; } 备注说明 用户登录成功后，系统会自动下发JWT令牌，然后在后续的每次请求中，都需要在请求头header中携带到服务端，请求头的名称为 token ，值为 登录时下发的JWT令牌。\n如果检测到用户未登录，则会返回如下固定错误信息：\n1 2 3 4 5 { \u0026#34;code\u0026#34;: 0, \u0026#34;msg\u0026#34;: \u0026#34;NOT_LOGIN\u0026#34;, \u0026#34;data\u0026#34;: null } 解读完接口文档中的描述了，目前我们先来完成令牌的生成和令牌的下发，我们只需要生成一个令牌返回给前端就可以了。\n实现步骤：\n引入JWT工具类 在项目工程下创建com.itheima.utils包，并把提供JWT工具类复制到该包下 登录完成后，调用工具类生成JWT令牌并返回 JWT工具类\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class JwtUtils { private static String signKey = \u0026#34;itheima\u0026#34;;//签名密钥 private static Long expire = 43200000L; //有效时间 /** * 生成JWT令牌 * @param claims JWT第二部分负载 payload 中存储的内容 * @return */ public static String generateJwt(Map\u0026lt;String, Object\u0026gt; claims){ String jwt = Jwts.builder() .addClaims(claims)//自定义信息（有效载荷） .signWith(SignatureAlgorithm.HS256, signKey)//签名算法（头部） .setExpiration(new Date(System.currentTimeMillis() + expire))//过期时间 .compact(); return jwt; } /** * 解析JWT令牌 * @param jwt JWT令牌 * @return JWT第二部分负载 payload 中存储的内容 */ public static Claims parseJWT(String jwt){ Claims claims = Jwts.parser() .setSigningKey(signKey)//指定签名密钥 .parseClaimsJws(jwt)//指定令牌Token .getBody(); return claims; } } 登录成功，生成JWT令牌并返回\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 @RestController @Slf4j public class LoginController { //依赖业务层对象 @Autowired private EmpService empService; @PostMapping(\u0026#34;/login\u0026#34;) public Result login(@RequestBody Emp emp) { //调用业务层：登录功能 Emp loginEmp = empService.login(emp); //判断：登录用户是否存在 if(loginEmp !=null ){ //自定义信息 Map\u0026lt;String , Object\u0026gt; claims = new HashMap\u0026lt;\u0026gt;(); claims.put(\u0026#34;id\u0026#34;, loginEmp.getId()); claims.put(\u0026#34;username\u0026#34;,loginEmp.getUsername()); claims.put(\u0026#34;name\u0026#34;,loginEmp.getName()); //使用JWT工具类，生成身份令牌 String token = JwtUtils.generateJwt(claims); return Result.success(token); } return Result.error(\u0026#34;用户名或密码错误\u0026#34;); } } 重启服务，打开postman测试登录接口：\n打开浏览器完成前后端联调操作：利用开发者工具，抓取一下网络请求\n登录请求完成后，可以看到JWT令牌已经响应给了前端，此时前端就会将JWT令牌存储在浏览器本地。\n服务器响应的JWT令牌存储在本地浏览器哪里了呢？\n在当前案例中，JWT令牌存储在浏览器的本地存储空间local storage中了。 local storage是浏览器的本地存储，在移动端也是支持的。 我们在发起一个查询部门数据的请求，此时我们可以看到在请求头中包含一个token(JWT令牌)，后续的每一次请求当中，都会将这个令牌携带到服务端。\n登录校验-Filter-入门 刚才通过浏览器的开发者工具，我们可以看到在后续的请求当中，都会在请求头中携带JWT令牌到服务端，而服务端需要统一拦截所有的请求，从而判断是否携带的有合法的JWT令牌。 那怎么样来统一拦截到所有的请求校验令牌的有效性呢？这里我们会学习两种解决方案：\nFilter过滤器 Interceptor拦截器 我们首先来学习过滤器Filter。\n快速入门 什么是Filter？\nFilter表示过滤器，是 JavaWeb三大组件(Servlet、Filter、Listener)之一。 过滤器可以把对资源的请求拦截下来，从而实现一些特殊的功能 使用了过滤器之后，要想访问web服务器上的资源，必须先经过滤器，过滤器处理完毕之后，才可以访问对应的资源。 过滤器一般完成一些通用的操作，比如：登录校验、统一编码处理、敏感字符处理等。 下面我们通过Filter快速入门程序掌握过滤器的基本使用操作：\n第1步，定义过滤器 ：1.定义一个类，实现 Filter 接口，并重写其所有方法。 第2步，配置过滤器：Filter类上加 @WebFilter 注解，配置拦截资源的路径。引导类上加 @ServletComponentScan 开启Servlet组件支持。 *定义过滤器*\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //定义一个类，实现一个标准的Filter过滤器的接口 public class DemoFilter implements Filter { @Override //初始化方法, 只调用一次 public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\u0026#34;init 初始化方法执行了\u0026#34;); } @Override //拦截到请求之后调用, 调用多次 public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\u0026#34;Demo 拦截到了请求...放行前逻辑\u0026#34;); //放行 chain.doFilter(request,response); } @Override //销毁方法, 只调用一次 public void destroy() { System.out.println(\u0026#34;destroy 销毁方法执行了\u0026#34;); } } init方法：过滤器的初始化方法。在web服务器启动的时候会自动的创建Filter过滤器对象，在创建过滤器对象的时候会自动调用init初始化方法，这个方法只会被调用一次。 doFilter方法：这个方法是在每一次拦截到请求之后都会被调用，所以这个方法是会被调用多次的，每拦截到一次请求就会调用一次doFilter()方法。 destroy方法： 是销毁的方法。当我们关闭服务器的时候，它会自动的调用销毁方法destroy，而这个销毁方法也只会被调用一次。 在定义完Filter之后，Filter其实并不会生效，还需要完成Filter的配置，Filter的配置非常简单，只需要在Filter类上添加一个注解：@WebFilter，并指定属性urlPatterns，通过这个属性指定过滤器要拦截哪些请求\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) //配置过滤器要拦截的请求路径（ /* 表示拦截浏览器的所有请求 ） public class DemoFilter implements Filter { @Override //初始化方法, 只调用一次 public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\u0026#34;init 初始化方法执行了\u0026#34;); } @Override //拦截到请求之后调用, 调用多次 public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\u0026#34;Demo 拦截到了请求...放行前逻辑\u0026#34;); //放行 chain.doFilter(request,response); } @Override //销毁方法, 只调用一次 public void destroy() { System.out.println(\u0026#34;destroy 销毁方法执行了\u0026#34;); } } 当我们在Filter类上面加了@WebFilter注解之后，接下来我们还需要在启动类上面加上一个注解@ServletComponentScan，通过这个@ServletComponentScan注解来开启SpringBoot项目对于Servlet组件的支持。\n1 2 3 4 5 6 7 8 9 @ServletComponentScan @SpringBootApplication public class TliasWebManagementApplication { public static void main(String[] args) { SpringApplication.run(TliasWebManagementApplication.class, args); } } 重新启动服务，打开浏览器，执行部门管理的请求，可以看到控制台输出了过滤器中的内容：\n注意事项：\n在过滤器Filter中，如果不执行放行操作，将无法访问后面的资源。 放行操作：chain.doFilter(request, response);\n现在我们已完成了Filter过滤器的基本使用，下面我们将学习Filter过滤器在使用过程中的一些细节。\n登录校验-Filter-详解（执行流程-拦截路径） Filter过滤器的快速入门程序我们已经完成了，接下来我们就要详细的介绍一下过滤器Filter在使用中的一些细节。主要介绍以下3个方面的细节：\n过滤器的执行流程 过滤器的拦截路径配置 过滤器链 执行流程 首先我们先来看下过滤器的执行流程：\n过滤器当中我们拦截到了请求之后，如果希望继续访问后面的web资源，就要执行放行操作，放行就是调用 FilterChain对象当中的doFilter()方法，在调用doFilter()这个方法之前所编写的代码属于放行之前的逻辑。\n在放行后访问完 web 资源之后还会回到过滤器当中，回到过滤器之后如有需求还可以执行放行之后的逻辑，放行之后的逻辑我们写在doFilter()这行代码之后。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) public class DemoFilter implements Filter { @Override //初始化方法, 只调用一次 public void init(FilterConfig filterConfig) throws ServletException { System.out.println(\u0026#34;init 初始化方法执行了\u0026#34;); } @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;DemoFilter 放行前逻辑.....\u0026#34;); //放行请求 filterChain.doFilter(servletRequest,servletResponse); System.out.println(\u0026#34;DemoFilter 放行后逻辑.....\u0026#34;); } @Override //销毁方法, 只调用一次 public void destroy() { System.out.println(\u0026#34;destroy 销毁方法执行了\u0026#34;); } } 拦截路径 执行流程我们搞清楚之后，接下来再来介绍一下过滤器的拦截路径，Filter可以根据需求，配置不同的拦截资源路径：\n拦截路径 urlPatterns值 含义 拦截具体路径 /login 只有访问 /login 路径时，才会被拦截 目录拦截 /emps/* 访问/emps下的所有资源，都会被拦截 拦截所有 /* 访问所有资源，都会被拦截 下面我们来测试\u0026quot;拦截具体路径\u0026quot;：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @WebFilter(urlPatterns = \u0026#34;/login\u0026#34;) //拦截/login具体路径 public class DemoFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;DemoFilter 放行前逻辑.....\u0026#34;); //放行请求 filterChain.doFilter(servletRequest,servletResponse); System.out.println(\u0026#34;DemoFilter 放行后逻辑.....\u0026#34;); } @Override public void init(FilterConfig filterConfig) throws ServletException { Filter.super.init(filterConfig); } @Override public void destroy() { Filter.super.destroy(); } } 测试1：访问部门管理请求，发现过滤器没有拦截请求\n测试2：访问登录请求/login，发现过滤器拦截请求\n下面我们来测试\u0026quot;目录拦截\u0026quot;：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @WebFilter(urlPatterns = \u0026#34;/depts/*\u0026#34;) //拦截所有以/depts开头，后面是什么无所谓 public class DemoFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;DemoFilter 放行前逻辑.....\u0026#34;); //放行请求 filterChain.doFilter(servletRequest,servletResponse); System.out.println(\u0026#34;DemoFilter 放行后逻辑.....\u0026#34;); } @Override public void init(FilterConfig filterConfig) throws ServletException { Filter.super.init(filterConfig); } @Override public void destroy() { Filter.super.destroy(); } } 测试1：访问部门管理请求，发现过滤器拦截了请求\n测试2：访问登录请求/login，发现过滤器没有拦截请求\n登录校验-Filter-详解（过滤器链） 过滤器链 最后我们在来介绍下过滤器链，什么是过滤器链呢？所谓过滤器链指的是在一个web应用程序当中，可以配置多个过滤器，多个过滤器就形成了一个过滤器链。\n比如：在我们web服务器当中，定义了两个过滤器，这两个过滤器就形成了一个过滤器链。\n而这个链上的过滤器在执行的时候会一个一个的执行，会先执行第一个Filter，放行之后再来执行第二个Filter，如果执行到了最后一个过滤器放行之后，才会访问对应的web资源。\n访问完web资源之后，按照我们刚才所介绍的过滤器的执行流程，还会回到过滤器当中来执行过滤器放行后的逻辑，而在执行放行后的逻辑的时候，顺序是反着的。\n先要执行过滤器2放行之后的逻辑，再来执行过滤器1放行之后的逻辑，最后在给浏览器响应数据。\n以上就是当我们在web应用当中配置了多个过滤器，形成了这样一个过滤器链以及过滤器链的执行顺序。下面我们通过idea来验证下过滤器链。\n验证步骤：\n在filter包下再来新建一个Filter过滤器类：AbcFilter 在AbcFilter过滤器中编写放行前和放行后逻辑 配置AbcFilter过滤器拦截请求路径为：/* 重启SpringBoot服务，查看DemoFilter、AbcFilter的执行日志 AbcFilter过滤器\n1 2 3 4 5 6 7 8 9 10 11 12 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) public class AbcFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\u0026#34;Abc 拦截到了请求... 放行前逻辑\u0026#34;); //放行 chain.doFilter(request,response); System.out.println(\u0026#34;Abc 拦截到了请求... 放行后逻辑\u0026#34;); } } DemoFilter过滤器\n1 2 3 4 5 6 7 8 9 10 11 12 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) public class DemoFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;DemoFilter 放行前逻辑.....\u0026#34;); //放行请求 filterChain.doFilter(servletRequest,servletResponse); System.out.println(\u0026#34;DemoFilter 放行后逻辑.....\u0026#34;); } } 打开浏览器访问登录接口：\n通过控制台日志的输出，大家发现AbcFilter先执行DemoFilter后执行，这是为什么呢？\n其实是和过滤器的类名有关系。以注解方式配置的Filter过滤器，它的执行优先级是按时过滤器类名的自动排序确定的，类名排名越靠前，优先级越高。\n假如我们想让DemoFilter先执行，怎么办呢？答案就是修改类名。\n测试：修改AbcFilter类名为XbcFilter，运行程序查看控制台日志\n1 2 3 4 5 6 7 8 9 10 11 12 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) public class XbcFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\u0026#34;Xbc 拦截到了请求...放行前逻辑\u0026#34;); //放行 chain.doFilter(request,response); System.out.println(\u0026#34;Xbc 拦截到了请求...放行后逻辑\u0026#34;); } } 到此，关于过滤器的使用细节，我们已经全部介绍完毕了。\n登录校验-Filter-登录校验过滤器 12.1 分析 过滤器Filter的快速入门以及使用细节我们已经介绍完了，接下来最后一步，我们需要使用过滤器Filter来完成案例当中的登录校验功能。\n我们先来回顾下前面分析过的登录校验的基本流程：\n要进入到后台管理系统，我们必须先完成登录操作，此时就需要访问登录接口login。 登录成功之后，我们会在服务端生成一个JWT令牌，并且把JWT令牌返回给前端，前端会将JWT令牌存储下来。 在后续的每一次请求当中，都会将JWT令牌携带到服务端，请求到达服务端之后，要想去访问对应的业务功能，此时我们必须先要校验令牌的有效性。 对于校验令牌的这一块操作，我们使用登录校验的过滤器，在过滤器当中来校验令牌的有效性。如果令牌是无效的，就响应一个错误的信息，也不会再去放行访问对应的资源了。如果令牌存在，并且它是有效的，此时就会放行去访问对应的web资源，执行相应的业务操作。 大概清楚了在Filter过滤器的实现步骤了，那在正式开发登录校验过滤器之前，我们思考两个问题：\n所有的请求，拦截到了之后，都需要校验令牌吗？ 答案：登录请求例外 拦截到请求后，什么情况下才可以放行，执行业务操作？ 答案：有令牌，且令牌校验通过(合法)；否则都返回未登录错误结果 具体流程 我们要完成登录校验，主要是利用Filter过滤器实现，而Filter过滤器的流程步骤：\n基于上面的业务流程，我们分析出具体的操作步骤：\n获取请求url 判断请求url中是否包含login，如果包含，说明是登录操作，放行 获取请求头中的令牌（token） 判断令牌是否存在，如果不存在，返回错误结果（未登录） 解析token，如果解析失败，返回错误结果（未登录） 放行 代码实现 分析清楚了以上的问题后，我们就参照接口文档来开发登录功能了，登录接口描述如下：\n基本信息\n1 2 3 4 5 请求路径：/login 请求方式：POST 接口描述：该接口用于员工登录Tlias智能学习辅助系统，登录完毕后，系统下发JWT令牌。 请求参数\n参数格式：application/json\n参数说明：\n名称 类型 是否必须 备注 username string 必须 用户名 password string 必须 密码 请求数据样例：\n1 2 3 4 { \u0026#34;username\u0026#34;: \u0026#34;jinyong\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;123456\u0026#34; } 响应数据 参数格式：application/json\n参数说明：\n名称 类型 是否必须 默认值 备注 其他信息 code number 必须 响应码, 1 成功 ; 0 失败 msg string 非必须 提示信息 data string 必须 返回的数据 , jwt令牌 响应数据样例：\n1 2 3 4 5 { \u0026#34;code\u0026#34;: 1, \u0026#34;msg\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;data\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiJ9.eyJuYW1lIjoi6YeR5bq4IiwiaWQiOjEsInVzZXJuYW1lIjoiamlueW9uZyIsImV4cCI6MTY2MjIwNzA0OH0.KkUc_CXJZJ8Dd063eImx4H9Ojfrr6XMJ-yVzaWCVZCo\u0026#34; } 备注说明 用户登录成功后，系统会自动下发JWT令牌，然后在后续的每次请求中，都需要在请求头header中携带到服务端，请求头的名称为 token ，值为 登录时下发的JWT令牌。\n如果检测到用户未登录，则会返回如下固定错误信息：\n1 2 3 4 5 { \u0026#34;code\u0026#34;: 0, \u0026#34;msg\u0026#34;: \u0026#34;NOT_LOGIN\u0026#34;, \u0026#34;data\u0026#34;: null } 登录校验过滤器：LoginCheckFilter\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @Slf4j @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) //拦截所有请求 public class LoginCheckFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain chain) throws IOException, ServletException { //前置：强制转换为http协议的请求对象、响应对象 （转换原因：要使用子类中特有方法） HttpServletRequest request = (HttpServletRequest) servletRequest; HttpServletResponse response = (HttpServletResponse) servletResponse; //1.获取请求url String url = request.getRequestURL().toString(); log.info(\u0026#34;请求路径：{}\u0026#34;, url); //请求路径：http://localhost:8080/login //2.判断请求url中是否包含login，如果包含，说明是登录操作，放行 if(url.contains(\u0026#34;/login\u0026#34;)){ chain.doFilter(request, response);//放行请求 return;//结束当前方法的执行 } //3.获取请求头中的令牌（token） String token = request.getHeader(\u0026#34;token\u0026#34;); log.info(\u0026#34;从请求头中获取的令牌：{}\u0026#34;,token); //4.判断令牌是否存在，如果不存在，返回错误结果（未登录） if(!StringUtils.hasLength(token)){ log.info(\u0026#34;Token不存在\u0026#34;); Result responseResult = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); //把Result对象转换为JSON格式字符串 (fastjson是阿里巴巴提供的用于实现对象和json的转换工具类) String json = JSONObject.toJSONString(responseResult); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return; } //5.解析token，如果解析失败，返回错误结果（未登录） try { JwtUtils.parseJWT(token); }catch (Exception e){ log.info(\u0026#34;令牌解析失败!\u0026#34;); Result responseResult = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); //把Result对象转换为JSON格式字符串 (fastjson是阿里巴巴提供的用于实现对象和json的转换工具类) String json = JSONObject.toJSONString(responseResult); response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return; } //6.放行 chain.doFilter(request, response); } } 在上述过滤器的功能实现中，我们使用到了一个第三方json处理的工具包fastjson。我们要想使用，需要引入如下依赖：\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;fastjson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.76\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 登录校验的过滤器我们编写完成了，接下来我们就可以重新启动服务来做一个测试：\n测试前先把之前所编写的测试使用的过滤器，暂时注释掉。直接将@WebFilter注解给注释掉即可。\n测试1：未登录是否可以访问部门管理页面\n首先关闭浏览器，重新打开浏览器，在地址栏中输入：http://localhost:9528/#/system/dept\n由于用户没有登录，登录校验过滤器返回错误信息，前端页面根据返回的错误信息结果，自动跳转到登录页面了\n测试2：先进行登录操作，再访问部门管理页面 登录校验成功之后，可以正常访问相关业务操作页面\n登录校验-Interceptor-入门 学习完了过滤器Filter之后，接下来我们继续学习拦截器Interseptor。\n拦截器我们主要分为三个方面进行讲解：\n介绍下什么是拦截器，并通过快速入门程序上手拦截器 拦截器的使用细节 通过拦截器Interceptor完成登录校验功能 我们先学习第一块内容：拦截器快速入门\n快速入门 什么是拦截器？\n是一种动态拦截方法调用的机制，类似于过滤器。 拦截器是Spring框架中提供的，用来动态拦截控制器方法的执行。 拦截器的作用：\n拦截请求，在指定方法调用前后，根据业务需要执行预先设定的代码。 在拦截器当中，我们通常也是做一些通用性的操作，比如：我们可以通过拦截器来拦截前端发起的请求，将登录校验的逻辑全部编写在拦截器当中。在校验的过程当中，如发现用户登录了(携带JWT令牌且是合法令牌)，就可以直接放行，去访问spring当中的资源。如果校验时发现并没有登录或是非法令牌，就可以直接给前端响应未登录的错误信息。\n下面我们通过快速入门程序，来学习下拦截器的基本使用。拦截器的使用步骤和过滤器类似，也分为两步：\n定义拦截器 注册配置拦截器 **自定义拦截器：**实现HandlerInterceptor接口，并重写其所有方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //自定义拦截器 @Component public class LoginCheckInterceptor implements HandlerInterceptor { //目标资源方法执行前执行。 返回true：放行 返回false：不放行 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\u0026#34;preHandle .... \u0026#34;); return true; //true表示放行 } //目标资源方法执行后执行 @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\u0026#34;postHandle ... \u0026#34;); } //视图渲染完毕后执行，最后执行 @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\u0026#34;afterCompletion .... \u0026#34;); } } 注意：\npreHandle方法：目标资源方法执行前执行。 返回true：放行 返回false：不放行\npostHandle方法：目标资源方法执行后执行\nafterCompletion方法：视图渲染完毕后执行，最后执行\n注册配置拦截器：实现WebMvcConfigurer接口，并重写addInterceptors方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Configuration public class WebConfig implements WebMvcConfigurer { //自定义的拦截器对象 @Autowired private LoginCheckInterceptor loginCheckInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //注册自定义拦截器对象 registry.addInterceptor(loginCheckInterceptor).addPathPatterns(\u0026#34;/**\u0026#34;);//设置拦截器拦截的请求路径（ /** 表示拦截所有请求） } } 重新启动SpringBoot服务，打开postman测试：\n接下来我们再来做一个测试：将拦截器中返回值改为false\n使用postman，再次点击send发送请求后，没有响应数据，说明请求被拦截了没有放行\n登录校验-Interceptor-详解 拦截器的入门程序完成之后，接下来我们来介绍拦截器的使用细节。拦截器的使用细节我们主要介绍两个部分：\n拦截器的拦截路径配置 拦截器的执行流程 拦截路径 首先我们先来看拦截器的拦截路径的配置，在注册配置拦截器的时候，我们要指定拦截器的拦截路径，通过addPathPatterns(\u0026quot;要拦截路径\u0026quot;)方法，就可以指定要拦截哪些资源。\n在入门程序中我们配置的是/**，表示拦截所有资源，而在配置拦截器时，不仅可以指定要拦截哪些资源，还可以指定不拦截哪些资源，只需要调用excludePathPatterns(\u0026quot;不拦截路径\u0026quot;)方法，指定哪些资源不需要拦截。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Configuration public class WebConfig implements WebMvcConfigurer { //拦截器对象 @Autowired private LoginCheckInterceptor loginCheckInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //注册自定义拦截器对象 registry.addInterceptor(loginCheckInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;)//设置拦截器拦截的请求路径（ /** 表示拦截所有请求） .excludePathPatterns(\u0026#34;/login\u0026#34;);//设置不拦截的请求路径 } } 在拦截器中除了可以设置/**拦截所有资源外，还有一些常见拦截路径设置：\n拦截路径 含义 举例 /* 一级路径 能匹配/depts，/emps，/login，不能匹配 /depts/1 /** 任意级路径 能匹配/depts，/depts/1，/depts/1/2 /depts/* /depts下的一级路径 能匹配/depts/1，不能匹配/depts/1/2，/depts /depts/** /depts下的任意级路径 能匹配/depts，/depts/1，/depts/1/2，不能匹配/emps/1 下面主要来演示下/**与/*的区别：\n修改拦截器配置，把拦截路径设置为/* 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Configuration public class WebConfig implements WebMvcConfigurer { //拦截器对象 @Autowired private LoginCheckInterceptor loginCheckInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //注册自定义拦截器对象 registry.addInterceptor(loginCheckInterceptor) .addPathPatterns(\u0026#34;/*\u0026#34;) .excludePathPatterns(\u0026#34;/login\u0026#34;);//设置不拦截的请求路径 } } 使用postman测试：http://localhost:8080/emps/1\n控制台没有输出拦截器中的日志信息，说明/*没有匹配到拦截路径/emp/1 。\n执行流程 介绍完拦截路径的配置之后，接下来我们再来介绍拦截器的执行流程。通过执行流程，大家就能够清晰的知道过滤器与拦截器的执行时机。\n当我们打开浏览器来访问部署在web服务器当中的web应用时，此时我们所定义的过滤器会拦截到这次请求。拦截到这次请求之后，它会先执行放行前的逻辑，然后再执行放行操作。而由于我们当前是基于springboot开发的，所以放行之后是进入到了spring的环境当中，也就是要来访问我们所定义的controller当中的接口方法。 Tomcat并不识别所编写的Controller程序，但是它识别Servlet程序，所以在Spring的Web环境中提供了一个非常核心的Servlet：DispatcherServlet（前端控制器），所有请求都会先进行到DispatcherServlet，再将请求转给Controller。 当我们定义了拦截器后，会在执行Controller的方法之前，请求被拦截器拦截住。执行preHandle()方法，这个方法执行完成后需要返回一个布尔类型的值，如果返回true，就表示放行本次操作，才会继续访问controller中的方法；如果返回false，则不会放行（controller中的方法也不会执行）。 在controller当中的方法执行完毕之后，再回过来执行postHandle()这个方法以及afterCompletion() 方法，然后再返回给DispatcherServlet，最终再来执行过滤器当中放行后的这一部分逻辑的逻辑。执行完毕之后，最终给浏览器响应数据。 接下来我们就来演示下过滤器和拦截器同时存在的执行流程：\n开启LoginCheckInterceptor拦截器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 @Component public class LoginCheckInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\u0026#34;preHandle .... \u0026#34;); return true; //true表示放行 } @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception { System.out.println(\u0026#34;postHandle ... \u0026#34;); } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { System.out.println(\u0026#34;afterCompletion .... \u0026#34;); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @Configuration public class WebConfig implements WebMvcConfigurer { //拦截器对象 @Autowired private LoginCheckInterceptor loginCheckInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //注册自定义拦截器对象 registry.addInterceptor(loginCheckInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;)//拦截所有请求 .excludePathPatterns(\u0026#34;/login\u0026#34;);//不拦截登录请求 } } 开启DemoFilter过滤器 1 2 3 4 5 6 7 8 9 10 11 12 @WebFilter(urlPatterns = \u0026#34;/*\u0026#34;) public class DemoFilter implements Filter { @Override public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException { System.out.println(\u0026#34;DemoFilter 放行前逻辑.....\u0026#34;); //放行请求 filterChain.doFilter(servletRequest,servletResponse); System.out.println(\u0026#34;DemoFilter 放行后逻辑.....\u0026#34;); } } 重启SpringBoot服务后，清空日志，打开Postman，测试查询部门：\n以上就是拦截器的执行流程。通过执行流程分析，大家应该已经清楚了过滤器和拦截器之间的区别，其实它们之间的区别主要是两点：\n接口规范不同：过滤器需要实现Filter接口，而拦截器需要实现HandlerInterceptor接口。 拦截范围不同：过滤器Filter会拦截所有的资源，而Interceptor只会拦截Spring环境中的资源。 登录校验-Interceptor-登录校验拦截器 讲解完了拦截器的基本操作之后，接下来我们需要完成最后一步操作：通过拦截器来完成案例当中的登录校验功能。\n登录校验的业务逻辑以及操作步骤我们前面已经分析过了，和登录校验Filter过滤器当中的逻辑是完全一致的。现在我们只需要把这个技术方案由原来的过滤器换成拦截器interceptor就可以了。\n登录校验拦截器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 //自定义拦截器 @Component //当前拦截器对象由Spring创建和管理 @Slf4j public class LoginCheckInterceptor implements HandlerInterceptor { //前置方式 @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { System.out.println(\u0026#34;preHandle .... \u0026#34;); //1.获取请求url //2.判断请求url中是否包含login，如果包含，说明是登录操作，放行 //3.获取请求头中的令牌（token） String token = request.getHeader(\u0026#34;token\u0026#34;); log.info(\u0026#34;从请求头中获取的令牌：{}\u0026#34;,token); //4.判断令牌是否存在，如果不存在，返回错误结果（未登录） if(!StringUtils.hasLength(token)){ log.info(\u0026#34;Token不存在\u0026#34;); //创建响应结果对象 Result responseResult = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); //把Result对象转换为JSON格式字符串 (fastjson是阿里巴巴提供的用于实现对象和json的转换工具类) String json = JSONObject.toJSONString(responseResult); //设置响应头（告知浏览器：响应的数据类型为json、响应的数据编码表为utf-8） response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return false;//不放行 } //5.解析token，如果解析失败，返回错误结果（未登录） try { JwtUtils.parseJWT(token); }catch (Exception e){ log.info(\u0026#34;令牌解析失败!\u0026#34;); //创建响应结果对象 Result responseResult = Result.error(\u0026#34;NOT_LOGIN\u0026#34;); //把Result对象转换为JSON格式字符串 (fastjson是阿里巴巴提供的用于实现对象和json的转换工具类) String json = JSONObject.toJSONString(responseResult); //设置响应头 response.setContentType(\u0026#34;application/json;charset=utf-8\u0026#34;); //响应 response.getWriter().write(json); return false; } //6.放行 return true; } 注册配置拦截器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Configuration public class WebConfig implements WebMvcConfigurer { //拦截器对象 @Autowired private LoginCheckInterceptor loginCheckInterceptor; @Override public void addInterceptors(InterceptorRegistry registry) { //注册自定义拦截器对象 registry.addInterceptor(loginCheckInterceptor) .addPathPatterns(\u0026#34;/**\u0026#34;) .excludePathPatterns(\u0026#34;/login\u0026#34;); } } 登录校验的拦截器编写完成后，接下来我们就可以重新启动服务来做一个测试： （关闭登录校验Filter过滤器）\n测试1：未登录是否可以访问部门管理页面\n首先关闭浏览器，重新打开浏览器，在地址栏中输入：http://localhost:9528/#/system/dept\n由于用户没有登录，校验机制返回错误信息，前端页面根据返回的错误信息结果，自动跳转到登录页面了\n测试2：先进行登录操作，再访问部门管理页面 登录校验成功之后，可以正常访问相关业务操作页面\n到此我们也就验证了所开发的登录校验的拦截器也是没问题的。登录校验的过滤器和拦截器，我们只需要使用其中的一种就可以了。\n异常处理 当前问题 登录功能和登录校验功能我们都实现了，下面我们学习下今天最后一块技术点：异常处理。首先我们先来看一下系统出现异常之后会发生什么现象，再来介绍异常处理的方案。\n我们打开浏览器，访问系统中的新增部门操作，系统中已经有了 \u0026ldquo;就业部\u0026rdquo; 这个部门，我们再来增加一个就业部，看看会发生什么现象。\n点击确定之后，窗口关闭了，页面没有任何反应，就业部也没有添加上。 而此时，大家会发现，网络请求报错了。\n状态码为500，表示服务器端异常，我们打开idea，来看一下，服务器端出了什么问题。\n上述错误信息的含义是，dept部门表的name字段的值 就业部 重复了，因为在数据库表dept中已经有了就业部，我们之前设计这张表时，为name字段建议了唯一约束，所以该字段的值是不能重复的。\n而当我们再添加就业部，这个部门时，就违反了唯一约束，此时就会报错。\n我们来看一下出现异常之后，最终服务端给前端响应回来的数据长什么样。\n响应回来的数据是一个JSON格式的数据。但这种JSON格式的数据还是我们开发规范当中所提到的统一响应结果Result吗？显然并不是。由于返回的数据不符合开发规范，所以前端并不能解析出响应的JSON数据。\n接下来我们需要思考的是出现异常之后，当前案例项目的异常是怎么处理的？\n答案：没有做任何的异常处理 当我们没有做任何的异常处理时，我们三层架构处理异常的方案：\nMapper接口在操作数据库的时候出错了，此时异常会往上抛(谁调用Mapper就抛给谁)，会抛给service。 service 中也存在异常了，会抛给controller。 而在controller当中，我们也没有做任何的异常处理，所以最终异常会再往上抛。最终抛给框架之后，框架就会返回一个JSON格式的数据，里面封装的就是错误的信息，但是框架返回的JSON格式的数据并不符合我们的开发规范。 16.2 解决方案 那么在三层构架项目中，出现了异常，该如何处理?\n方案一：在所有Controller的所有方法中进行try…catch处理 缺点：代码臃肿（不推荐） 方案二：全局异常处理器 好处：简单、优雅（推荐） 全局异常处理器 我们该怎么样定义全局异常处理器？\n定义全局异常处理器非常简单，就是定义一个类，在类上加上一个注解@RestControllerAdvice，加上这个注解就代表我们定义了一个全局异常处理器。 在全局异常处理器当中，需要定义一个方法来捕获异常，在这个方法上需要加上注解@ExceptionHandler。通过@ExceptionHandler注解当中的value属性来指定我们要捕获的是哪一类型的异常。 1 2 3 4 5 6 7 8 9 10 11 12 @RestControllerAdvice public class GlobalExceptionHandler { //处理异常 @ExceptionHandler(Exception.class) //指定能够处理的异常类型 public Result ex(Exception e){ e.printStackTrace();//打印堆栈中的异常信息 //捕获到异常之后，响应一个标准的Result return Result.error(\u0026#34;对不起,操作失败,请联系管理员\u0026#34;); } } @RestControllerAdvice = @ControllerAdvice + @ResponseBody\n处理异常的方法返回值会转换为json后再响应给前端\n重新启动SpringBoot服务，打开浏览器，再来测试一下添加部门这个操作，我们依然添加已存在的 \u0026ldquo;就业部\u0026rdquo; 这个部门：\n此时，我们可以看到，出现异常之后，异常已经被全局异常处理器捕获了。然后返回的错误信息，被前端程序正常解析，然后提示出了对应的错误提示信息。\n以上就是全局异常处理器的使用，主要涉及到两个注解：\n@RestControllerAdvice //表示当前类为全局异常处理器 @ExceptionHandler //指定可以捕获哪种类型的异常进行处理 ","date":"2024-04-13T16:01:23+08:00","image":"https://nova-bryan.github.io/p/%E7%99%BB%E5%BD%95%E5%8A%9F%E8%83%BD/image_hu17089168107649188491.png","permalink":"https://nova-bryan.github.io/p/%E7%99%BB%E5%BD%95%E5%8A%9F%E8%83%BD/","title":"登录功能"}]